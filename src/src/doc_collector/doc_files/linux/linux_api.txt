
====================


void INIT_LIST_HEAD(struct list_head *list)
Initialize a list_head structure

Parameters


struct list_head *list
list_head structure to be initialized.

Description

Initializes the list_head to point to itself.  If it is a list header,
the result is an empty list.


====================


void list_add(struct list_head *new, struct list_head *head)
add a new entry

Parameters


struct list_head *new
new entry to be added

struct list_head *head
list head to add it after

Description

Insert a new entry after the specified head.
This is good for implementing stacks.


====================


void list_add_tail(struct list_head *new, struct list_head *head)
add a new entry

Parameters


struct list_head *new
new entry to be added

struct list_head *head
list head to add it before

Description

Insert a new entry before the specified head.
This is useful for implementing queues.


====================


void list_del(struct list_head *entry)
deletes entry from list.

Parameters


struct list_head *entry
the element to delete from the list.

Note

list_empty() on entry does not return true after this, the entry is
in an undefined state.


====================


void list_replace(struct list_head *old, struct list_head *new)
replace old entry by new one

Parameters


struct list_head *old
the element to be replaced

struct list_head *new
the new element to insert

Description

If old was empty, it will be overwritten.


====================


void list_replace_init(struct list_head *old, struct list_head *new)
replace old entry by new one and initialize the old one

Parameters


struct list_head *old
the element to be replaced

struct list_head *new
the new element to insert

Description

If old was empty, it will be overwritten.


====================


void list_swap(struct list_head *entry1, struct list_head *entry2)
replace entry1 with entry2 and re-add entry1 at entry2's position

Parameters


struct list_head *entry1
the location to place entry2

struct list_head *entry2
the location to place entry1


====================


void list_del_init(struct list_head *entry)
deletes entry from list and reinitialize it.

Parameters


struct list_head *entry
the element to delete from the list.


====================


void list_move(struct list_head *list, struct list_head *head)
delete from one list and add as another's head

Parameters


struct list_head *list
the entry to move

struct list_head *head
the head that will precede our entry


====================


void list_move_tail(struct list_head *list, struct list_head *head)
delete from one list and add as another's tail

Parameters


struct list_head *list
the entry to move

struct list_head *head
the head that will follow our entry


====================


void list_bulk_move_tail(struct list_head *head, struct list_head *first, struct list_head *last)
move a subsection of a list to its tail

Parameters


struct list_head *head
the head that will follow our entry

struct list_head *first
first entry to move

struct list_head *last
last entry to move, can be the same as first

Description

Move all entries between first and including last before head.
All three entries must belong to the same linked list.


====================


int list_is_first(const struct list_head *list, const struct list_head *head)

tests whether list is the first entry in list head


Parameters


const struct list_head *list
the entry to test

const struct list_head *head
the head of the list


====================


int list_is_last(const struct list_head *list, const struct list_head *head)
tests whether list is the last entry in list head

Parameters


const struct list_head *list
the entry to test

const struct list_head *head
the head of the list


====================


int list_is_head(const struct list_head *list, const struct list_head *head)
tests whether list is the list head

Parameters


const struct list_head *list
the entry to test

const struct list_head *head
the head of the list


====================


int list_empty(const struct list_head *head)
tests whether a list is empty

Parameters


const struct list_head *head
the list to test.


====================


void list_del_init_careful(struct list_head *entry)
deletes entry from list and reinitialize it.

Parameters


struct list_head *entry
the element to delete from the list.

Description

This is the same as list_del_init(), except designed to be used
together with list_empty_careful() in a way to guarantee ordering
of other memory operations.

Any memory operations done before a list_del_init_careful() are
guaranteed to be visible after a list_empty_careful() test.


====================


int list_empty_careful(const struct list_head *head)
tests whether a list is empty and not being modified

Parameters


const struct list_head *head
the list to test

Description

tests whether a list is empty _and_ checks that no other CPU might be
in the process of modifying either member (next or prev)

NOTE

using list_empty_careful() without synchronization
can only be safe if the only activity that can happen
to the list entry is list_del_init(). Eg. it cannot be used
if another CPU could re-list_add() it.


====================


void list_rotate_left(struct list_head *head)
rotate the list to the left

Parameters


struct list_head *head
the head of the list


====================


void list_rotate_to_front(struct list_head *list, struct list_head *head)
Rotate list to specific item.

Parameters


struct list_head *list
The desired new front of the list.

struct list_head *head
The head of the list.

Description

Rotates list so that list becomes the new front of the list.


====================


int list_is_singular(const struct list_head *head)
tests whether a list has just one entry.

Parameters


const struct list_head *head
the list to test.


====================


void list_cut_position(struct list_head *list, struct list_head *head, struct list_head *entry)
cut a list into two

Parameters


struct list_head *list
a new list to add all removed entries

struct list_head *head
a list with entries

struct list_head *entry
an entry within head, could be the head itself
and if so we won't cut the list

Description

This helper moves the initial part of head, up to and
including entry, from head to list. You should
pass on entry an element you know is on head. list
should be an empty list or a list you do not care about
losing its data.


====================


void list_cut_before(struct list_head *list, struct list_head *head, struct list_head *entry)
cut a list into two, before given entry

Parameters


struct list_head *list
a new list to add all removed entries

struct list_head *head
a list with entries

struct list_head *entry
an entry within head, could be the head itself

Description

This helper moves the initial part of head, up to but
excluding entry, from head to list.  You should pass
in entry an element you know is on head.  list should
be an empty list or a list you do not care about losing
its data.
If entry == head, all entries on head are moved to
list.


====================


void list_splice(const struct list_head *list, struct list_head *head)
join two lists, this is designed for stacks

Parameters


const struct list_head *list
the new list to add.

struct list_head *head
the place to add it in the first list.


====================


void list_splice_tail(struct list_head *list, struct list_head *head)
join two lists, each list being a queue

Parameters


struct list_head *list
the new list to add.

struct list_head *head
the place to add it in the first list.


====================


void list_splice_init(struct list_head *list, struct list_head *head)
join two lists and reinitialise the emptied list.

Parameters


struct list_head *list
the new list to add.

struct list_head *head
the place to add it in the first list.

Description

The list at list is reinitialised


====================


void list_splice_tail_init(struct list_head *list, struct list_head *head)
join two lists and reinitialise the emptied list

Parameters


struct list_head *list
the new list to add.

struct list_head *head
the place to add it in the first list.

Description

Each of the lists is a queue.
The list at list is reinitialised


====================


list_entry(ptr, type, member)
get the struct for this entry

Parameters


ptr
the struct list_head pointer.

type
the type of the struct this is embedded in.

member
the name of the list_head within the struct.


====================


list_first_entry(ptr, type, member)
get the first element from a list

Parameters


ptr
the list head to take the element from.

type
the type of the struct this is embedded in.

member
the name of the list_head within the struct.

Description

Note, that list is expected to be not empty.


====================


list_last_entry(ptr, type, member)
get the last element from a list

Parameters


ptr
the list head to take the element from.

type
the type of the struct this is embedded in.

member
the name of the list_head within the struct.

Description

Note, that list is expected to be not empty.


====================


list_first_entry_or_null(ptr, type, member)
get the first element from a list

Parameters


ptr
the list head to take the element from.

type
the type of the struct this is embedded in.

member
the name of the list_head within the struct.

Description

Note that if the list is empty, it returns NULL.


====================


list_next_entry(pos, member)
get the next element in list

Parameters


pos
the type * to cursor

member
the name of the list_head within the struct.


====================


list_prev_entry(pos, member)
get the prev element in list

Parameters


pos
the type * to cursor

member
the name of the list_head within the struct.


====================


list_for_each(pos, head)
iterate over a list

Parameters


pos
the struct list_head to use as a loop cursor.

head
the head for your list.


====================


list_for_each_continue(pos, head)
continue iteration over a list

Parameters


pos
the struct list_head to use as a loop cursor.

head
the head for your list.

Description

Continue to iterate over a list, continuing after the current position.


====================


list_for_each_prev(pos, head)
iterate over a list backwards

Parameters


pos
the struct list_head to use as a loop cursor.

head
the head for your list.


====================


list_for_each_safe(pos, n, head)
iterate over a list safe against removal of list entry

Parameters


pos
the struct list_head to use as a loop cursor.

n
another struct list_head to use as temporary storage

head
the head for your list.


====================


list_for_each_prev_safe(pos, n, head)
iterate over a list backwards safe against removal of list entry

Parameters


pos
the struct list_head to use as a loop cursor.

n
another struct list_head to use as temporary storage

head
the head for your list.


====================


list_entry_is_head(pos, head, member)
test if the entry points to the head of the list

Parameters


pos
the type * to cursor

head
the head for your list.

member
the name of the list_head within the struct.


====================


list_for_each_entry(pos, head, member)
iterate over list of given type

Parameters


pos
the type * to use as a loop cursor.

head
the head for your list.

member
the name of the list_head within the struct.


====================


list_for_each_entry_reverse(pos, head, member)
iterate backwards over list of given type.

Parameters


pos
the type * to use as a loop cursor.

head
the head for your list.

member
the name of the list_head within the struct.


====================


list_prepare_entry(pos, head, member)
prepare a pos entry for use in list_for_each_entry_continue()

Parameters


pos
the type * to use as a start point

head
the head of the list

member
the name of the list_head within the struct.

Description

Prepares a pos entry for use as a start point in list_for_each_entry_continue().


====================


list_for_each_entry_continue(pos, head, member)
continue iteration over list of given type

Parameters


pos
the type * to use as a loop cursor.

head
the head for your list.

member
the name of the list_head within the struct.

Description

Continue to iterate over list of given type, continuing after
the current position.


====================


list_for_each_entry_continue_reverse(pos, head, member)
iterate backwards from the given point

Parameters


pos
the type * to use as a loop cursor.

head
the head for your list.

member
the name of the list_head within the struct.

Description

Start to iterate over list of given type backwards, continuing after
the current position.


====================


list_for_each_entry_from(pos, head, member)
iterate over list of given type from the current point

Parameters


pos
the type * to use as a loop cursor.

head
the head for your list.

member
the name of the list_head within the struct.

Description

Iterate over list of given type, continuing from current position.


====================


list_for_each_entry_from_reverse(pos, head, member)
iterate backwards over list of given type from the current point

Parameters


pos
the type * to use as a loop cursor.

head
the head for your list.

member
the name of the list_head within the struct.

Description

Iterate backwards over list of given type, continuing from current position.


====================


list_for_each_entry_safe(pos, n, head, member)
iterate over list of given type safe against removal of list entry

Parameters


pos
the type * to use as a loop cursor.

n
another type * to use as temporary storage

head
the head for your list.

member
the name of the list_head within the struct.


====================


list_for_each_entry_safe_continue(pos, n, head, member)
continue list iteration safe against removal

Parameters


pos
the type * to use as a loop cursor.

n
another type * to use as temporary storage

head
the head for your list.

member
the name of the list_head within the struct.

Description

Iterate over list of given type, continuing after current point,
safe against removal of list entry.


====================


list_for_each_entry_safe_from(pos, n, head, member)
iterate over list from current point safe against removal

Parameters


pos
the type * to use as a loop cursor.

n
another type * to use as temporary storage

head
the head for your list.

member
the name of the list_head within the struct.

Description

Iterate over list of given type from current point, safe against
removal of list entry.


====================


list_for_each_entry_safe_reverse(pos, n, head, member)
iterate backwards over list safe against removal

Parameters


pos
the type * to use as a loop cursor.

n
another type * to use as temporary storage

head
the head for your list.

member
the name of the list_head within the struct.

Description

Iterate backwards over list of given type, safe against removal
of list entry.


====================


list_safe_reset_next(pos, n, member)
reset a stale list_for_each_entry_safe loop

Parameters


pos
the loop cursor used in the list_for_each_entry_safe loop

n
temporary storage used in list_for_each_entry_safe

member
the name of the list_head within the struct.

Description

list_safe_reset_next is not safe to use in general if the list may be
modified concurrently (eg. the lock is dropped in the loop body). An
exception to this is if the cursor element (pos) is pinned in the list,
and list_safe_reset_next is called after re-taking the lock and before
completing the current iteration of the loop body.


====================


int hlist_unhashed(const struct hlist_node *h)
Has node been removed from list and reinitialized?

Parameters


const struct hlist_node *h
Node to be checked

Description

Not that not all removal functions will leave a node in unhashed
state.  For example, hlist_nulls_del_init_rcu() does leave the
node in unhashed state, but hlist_nulls_del() does not.


====================


int hlist_unhashed_lockless(const struct hlist_node *h)
Version of hlist_unhashed for lockless use

Parameters


const struct hlist_node *h
Node to be checked

Description

This variant of hlist_unhashed() must be used in lockless contexts
to avoid potential load-tearing.  The READ_ONCE() is paired with the
various WRITE_ONCE() in hlist helpers that are defined below.


====================


int hlist_empty(const struct hlist_head *h)
Is the specified hlist_head structure an empty hlist?

Parameters


const struct hlist_head *h
Structure to check.


====================


void hlist_del(struct hlist_node *n)
Delete the specified hlist_node from its list

Parameters


struct hlist_node *n
Node to delete.

Description

Note that this function leaves the node in hashed state.  Use
hlist_del_init() or similar instead to unhash n.


====================


void hlist_del_init(struct hlist_node *n)
Delete the specified hlist_node from its list and initialize

Parameters


struct hlist_node *n
Node to delete.

Description

Note that this function leaves the node in unhashed state.


====================


void hlist_add_head(struct hlist_node *n, struct hlist_head *h)
add a new entry at the beginning of the hlist

Parameters


struct hlist_node *n
new entry to be added

struct hlist_head *h
hlist head to add it after

Description

Insert a new entry after the specified head.
This is good for implementing stacks.


====================


void hlist_add_before(struct hlist_node *n, struct hlist_node *next)
add a new entry before the one specified

Parameters


struct hlist_node *n
new entry to be added

struct hlist_node *next
hlist node to add it before, which must be non-NULL


====================


void hlist_add_behind(struct hlist_node *n, struct hlist_node *prev)
add a new entry after the one specified

Parameters


struct hlist_node *n
new entry to be added

struct hlist_node *prev
hlist node to add it after, which must be non-NULL


====================


void hlist_add_fake(struct hlist_node *n)
create a fake hlist consisting of a single headless node

Parameters


struct hlist_node *n
Node to make a fake list out of

Description

This makes n appear to be its own predecessor on a headless hlist.
The point of this is to allow things like hlist_del() to work correctly
in cases where there is no list.


====================


bool hlist_fake(struct hlist_node *h)
Is this node a fake hlist?

Parameters


struct hlist_node *h
Node to check for being a self-referential fake hlist.


====================


bool hlist_is_singular_node(struct hlist_node *n, struct hlist_head *h)
is node the only element of the specified hlist?

Parameters


struct hlist_node *n
Node to check for singularity.

struct hlist_head *h
Header for potentially singular list.

Description

Check whether the node is the only node of the head without
accessing head, thus avoiding unnecessary cache misses.


====================


void hlist_move_list(struct hlist_head *old, struct hlist_head *new)
Move an hlist

Parameters


struct hlist_head *old
hlist_head for old list.

struct hlist_head *new
hlist_head for new list.

Description

Move a list from one list head to another. Fixup the pprev
reference of the first entry if it exists.


====================


hlist_for_each_entry(pos, head, member)
iterate over list of given type

Parameters


pos
the type * to use as a loop cursor.

head
the head for your list.

member
the name of the hlist_node within the struct.


====================


hlist_for_each_entry_continue(pos, member)
iterate over a hlist continuing after current point

Parameters


pos
the type * to use as a loop cursor.

member
the name of the hlist_node within the struct.


====================


hlist_for_each_entry_from(pos, member)
iterate over a hlist continuing from current point

Parameters


pos
the type * to use as a loop cursor.

member
the name of the hlist_node within the struct.


====================


hlist_for_each_entry_safe(pos, n, head, member)
iterate over list of given type safe against removal of list entry

Parameters


pos
the type * to use as a loop cursor.

n
a struct hlist_node to use as temporary storage

head
the head for your list.

member
the name of the hlist_node within the struct.


====================


unsigned long long simple_strtoull(const char *cp, char **endp, unsigned int base)
convert a string to an unsigned long long

Parameters


const char *cp
The start of the string

char **endp
A pointer to the end of the parsed string will be placed here

unsigned int base
The number base to use

Description

This function has caveats. Please use kstrtoull instead.


====================


unsigned long simple_strtoul(const char *cp, char **endp, unsigned int base)
convert a string to an unsigned long

Parameters


const char *cp
The start of the string

char **endp
A pointer to the end of the parsed string will be placed here

unsigned int base
The number base to use

Description

This function has caveats. Please use kstrtoul instead.


====================


long simple_strtol(const char *cp, char **endp, unsigned int base)
convert a string to a signed long

Parameters


const char *cp
The start of the string

char **endp
A pointer to the end of the parsed string will be placed here

unsigned int base
The number base to use

Description

This function has caveats. Please use kstrtol instead.


====================


long long simple_strtoll(const char *cp, char **endp, unsigned int base)
convert a string to a signed long long

Parameters


const char *cp
The start of the string

char **endp
A pointer to the end of the parsed string will be placed here

unsigned int base
The number base to use

Description

This function has caveats. Please use kstrtoll instead.


====================


int vsnprintf(char *buf, size_t size, const char *fmt, va_list args)
Format a string and place it in a buffer

Parameters


char *buf
The buffer to place the result into

size_t size
The size of the buffer, including the trailing null space

const char *fmt
The format string to use

va_list args
Arguments for the format string

Description

This function generally follows C99 vsnprintf, but has some
extensions and a few limitations:



``n`` is unsupported
``p``* is handled by pointer()


See pointer() or How to get printk format specifiers right for more
extensive description.

Please update the documentation in both places when making changes

The return value is the number of characters which would
be generated for the given input, excluding the trailing
'0', as per ISO C99. If you want to have the exact
number of characters written into buf as return value
(not including the trailing '0'), use vscnprintf(). If the
return is greater than or equal to size, the resulting
string is truncated.

If you're not already dealing with a va_list consider using snprintf().


====================


int vscnprintf(char *buf, size_t size, const char *fmt, va_list args)
Format a string and place it in a buffer

Parameters


char *buf
The buffer to place the result into

size_t size
The size of the buffer, including the trailing null space

const char *fmt
The format string to use

va_list args
Arguments for the format string

Description

The return value is the number of characters which have been written into
the buf not including the trailing '0'. If size is == 0 the function
returns 0.

If you're not already dealing with a va_list consider using scnprintf().

See the vsnprintf() documentation for format string extensions over C99.


====================


int snprintf(char *buf, size_t size, const char *fmt, ...)
Format a string and place it in a buffer

Parameters


char *buf
The buffer to place the result into

size_t size
The size of the buffer, including the trailing null space

const char *fmt
The format string to use

...
Arguments for the format string

Description

The return value is the number of characters which would be
generated for the given input, excluding the trailing null,
as per ISO C99.  If the return is greater than or equal to
size, the resulting string is truncated.

See the vsnprintf() documentation for format string extensions over C99.


====================


int scnprintf(char *buf, size_t size, const char *fmt, ...)
Format a string and place it in a buffer

Parameters


char *buf
The buffer to place the result into

size_t size
The size of the buffer, including the trailing null space

const char *fmt
The format string to use

...
Arguments for the format string

Description

The return value is the number of characters written into buf not including
the trailing '0'. If size is == 0 the function returns 0.


====================


int vsprintf(char *buf, const char *fmt, va_list args)
Format a string and place it in a buffer

Parameters


char *buf
The buffer to place the result into

const char *fmt
The format string to use

va_list args
Arguments for the format string

Description

The function returns the number of characters written
into buf. Use vsnprintf() or vscnprintf() in order to avoid
buffer overflows.

If you're not already dealing with a va_list consider using sprintf().

See the vsnprintf() documentation for format string extensions over C99.


====================


int sprintf(char *buf, const char *fmt, ...)
Format a string and place it in a buffer

Parameters


char *buf
The buffer to place the result into

const char *fmt
The format string to use

...
Arguments for the format string

Description

The function returns the number of characters written
into buf. Use snprintf() or scnprintf() in order to avoid
buffer overflows.

See the vsnprintf() documentation for format string extensions over C99.


====================


int vbin_printf(u32 *bin_buf, size_t size, const char *fmt, va_list args)
Parse a format string and place args' binary value in a buffer

Parameters


u32 *bin_buf
The buffer to place args' binary value

size_t size
The size of the buffer(by words(32bits), not characters)

const char *fmt
The format string to use

va_list args
Arguments for the format string

Description

The format follows C99 vsnprintf, except n is ignored, and its argument
is skipped.

The return value is the number of words(32bits) which would be generated for
the given input.

NOTE

If the return value is greater than size, the resulting bin_buf is NOT
valid for bstr_printf().


====================


int bstr_printf(char *buf, size_t size, const char *fmt, const u32 *bin_buf)
Format a string from binary arguments and place it in a buffer

Parameters


char *buf
The buffer to place the result into

size_t size
The size of the buffer, including the trailing null space

const char *fmt
The format string to use

const u32 *bin_buf
Binary arguments for the format string

Description

This function like C99 vsnprintf, but the difference is that vsnprintf gets
arguments from stack, and bstr_printf gets arguments from bin_buf which is
a binary buffer that generated by vbin_printf.


The format follows C99 vsnprintf, but has some extensions:
see vsnprintf comment for details.

The return value is the number of characters which would
be generated for the given input, excluding the trailing
'0', as per ISO C99. If you want to have the exact
number of characters written into buf as return value
(not including the trailing '0'), use vscnprintf(). If the
return is greater than or equal to size, the resulting
string is truncated.


====================


int bprintf(u32 *bin_buf, size_t size, const char *fmt, ...)
Parse a format string and place args' binary value in a buffer

Parameters


u32 *bin_buf
The buffer to place args' binary value

size_t size
The size of the buffer(by words(32bits), not characters)

const char *fmt
The format string to use

...
Arguments for the format string

Description

The function returns the number of words(u32) written
into bin_buf.


====================


int vsscanf(const char *buf, const char *fmt, va_list args)
Unformat a buffer into a list of arguments

Parameters


const char *buf
input buffer

const char *fmt
format of buffer

va_list args
arguments


====================


int sscanf(const char *buf, const char *fmt, ...)
Unformat a buffer into a list of arguments

Parameters


const char *buf
input buffer

const char *fmt
formatting of buffer

...
resulting arguments


====================


int kstrtoul(const char *s, unsigned int base, unsigned long *res)
convert a string to an unsigned long

Parameters


const char *s
The start of the string. The string must be null-terminated, and may also
include a single newline before its terminating null. The first character
may also be a plus sign, but not a minus sign.

unsigned int base
The number base to use. The maximum supported base is 16. If base is
given as 0, then the base of the string is automatically detected with the
conventional semantics - If it begins with 0x the number will be parsed as a
hexadecimal (case insensitive), if it otherwise begins with 0, it will be
parsed as an octal number. Otherwise it will be parsed as a decimal.

unsigned long *res
Where to write the result of the conversion on success.

Description

Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.
Preferred over simple_strtoul(). Return code must be checked.


====================


int kstrtol(const char *s, unsigned int base, long *res)
convert a string to a long

Parameters


const char *s
The start of the string. The string must be null-terminated, and may also
include a single newline before its terminating null. The first character
may also be a plus sign or a minus sign.

unsigned int base
The number base to use. The maximum supported base is 16. If base is
given as 0, then the base of the string is automatically detected with the
conventional semantics - If it begins with 0x the number will be parsed as a
hexadecimal (case insensitive), if it otherwise begins with 0, it will be
parsed as an octal number. Otherwise it will be parsed as a decimal.

long *res
Where to write the result of the conversion on success.

Description

Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.
Preferred over simple_strtol(). Return code must be checked.


====================


int kstrtoull(const char *s, unsigned int base, unsigned long long *res)
convert a string to an unsigned long long

Parameters


const char *s
The start of the string. The string must be null-terminated, and may also
include a single newline before its terminating null. The first character
may also be a plus sign, but not a minus sign.

unsigned int base
The number base to use. The maximum supported base is 16. If base is
given as 0, then the base of the string is automatically detected with the
conventional semantics - If it begins with 0x the number will be parsed as a
hexadecimal (case insensitive), if it otherwise begins with 0, it will be
parsed as an octal number. Otherwise it will be parsed as a decimal.

unsigned long long *res
Where to write the result of the conversion on success.

Description

Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.
Preferred over simple_strtoull(). Return code must be checked.


====================


int kstrtoll(const char *s, unsigned int base, long long *res)
convert a string to a long long

Parameters


const char *s
The start of the string. The string must be null-terminated, and may also
include a single newline before its terminating null. The first character
may also be a plus sign or a minus sign.

unsigned int base
The number base to use. The maximum supported base is 16. If base is
given as 0, then the base of the string is automatically detected with the
conventional semantics - If it begins with 0x the number will be parsed as a
hexadecimal (case insensitive), if it otherwise begins with 0, it will be
parsed as an octal number. Otherwise it will be parsed as a decimal.

long long *res
Where to write the result of the conversion on success.

Description

Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.
Preferred over simple_strtoll(). Return code must be checked.


====================


int kstrtouint(const char *s, unsigned int base, unsigned int *res)
convert a string to an unsigned int

Parameters


const char *s
The start of the string. The string must be null-terminated, and may also
include a single newline before its terminating null. The first character
may also be a plus sign, but not a minus sign.

unsigned int base
The number base to use. The maximum supported base is 16. If base is
given as 0, then the base of the string is automatically detected with the
conventional semantics - If it begins with 0x the number will be parsed as a
hexadecimal (case insensitive), if it otherwise begins with 0, it will be
parsed as an octal number. Otherwise it will be parsed as a decimal.

unsigned int *res
Where to write the result of the conversion on success.

Description

Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.
Preferred over simple_strtoul(). Return code must be checked.


====================


int kstrtoint(const char *s, unsigned int base, int *res)
convert a string to an int

Parameters


const char *s
The start of the string. The string must be null-terminated, and may also
include a single newline before its terminating null. The first character
may also be a plus sign or a minus sign.

unsigned int base
The number base to use. The maximum supported base is 16. If base is
given as 0, then the base of the string is automatically detected with the
conventional semantics - If it begins with 0x the number will be parsed as a
hexadecimal (case insensitive), if it otherwise begins with 0, it will be
parsed as an octal number. Otherwise it will be parsed as a decimal.

int *res
Where to write the result of the conversion on success.

Description

Returns 0 on success, -ERANGE on overflow and -EINVAL on parsing error.
Preferred over simple_strtol(). Return code must be checked.


====================


int kstrtobool(const char *s, bool *res)
convert common user inputs into boolean values

Parameters


const char *s
input string

bool *res
result

Description

This routine returns 0 iff the first character is one of 'Yy1Nn0', or
[oO][NnFf] for 'on' and 'off'. Otherwise it will return -EINVAL.  Value
pointed to by res is updated upon finding a match.


====================


void string_get_size(u64 size, u64 blk_size, const enum string_size_units units, char *buf, int len)
get the size in the specified units

Parameters


u64 size
The size to be converted in blocks

u64 blk_size
Size of the block (use 1 for size in bytes)

const enum string_size_units units
units to use (powers of 1000 or 1024)

char *buf
buffer to format to

int len
length of buffer

Description

This function returns a string formatted to 3 significant figures
giving the size in the required units.  buf should have room for
at least 9 bytes and will always be zero terminated.


====================


int string_unescape(char *src, char *dst, size_t size, unsigned int flags)
unquote characters in the given string

Parameters


char *src
source buffer (escaped)

char *dst
destination buffer (unescaped)

size_t size
size of the destination buffer (0 to unlimit)

unsigned int flags
combination of the flags.

Description

The function unquotes characters in the given string.

Because the size of the output will be the same as or less than the size of
the input, the transformation may be performed in place.

Caller must provide valid source and destination pointers. Be aware that
destination buffer will always be NULL-terminated. Source string must be
NULL-terminated as well.  The supported flags are:

UNESCAPE_SPACE:
        '\f' - form feed
        '\n' - new line
        '\r' - carriage return
        '\t' - horizontal tab
        '\v' - vertical tab
UNESCAPE_OCTAL:
        '\NNN' - byte with octal value NNN (1 to 3 digits)
UNESCAPE_HEX:
        '\xHH' - byte with hexadecimal value HH (1 to 2 digits)
UNESCAPE_SPECIAL:
        '\"' - double quote
        '\\' - backslash
        '\a' - alert (BEL)
        '\e' - escape
UNESCAPE_ANY:
        all previous together


Return

The amount of the characters processed to the destination buffer excluding
trailing '0' is returned.


====================


int string_escape_mem(const char *src, size_t isz, char *dst, size_t osz, unsigned int flags, const char *only)
quote characters in the given memory buffer

Parameters


const char *src
source buffer (unescaped)

size_t isz
source buffer size

char *dst
destination buffer (escaped)

size_t osz
destination buffer size

unsigned int flags
combination of the flags

const char *only
NULL-terminated string containing characters used to limit
the selected escape class. If characters are included in only
that would not normally be escaped by the classes selected
in flags, they will be copied to dst unescaped.

Description

The process of escaping byte buffer includes several parts. They are applied
in the following sequence.



The character is not matched to the one from only string and thus
must go as-is to the output.
The character is matched to the printable and ASCII classes, if asked,
and in case of match it passes through to the output.
The character is matched to the printable or ASCII class, if asked,
and in case of match it passes through to the output.
The character is checked if it falls into the class given by flags.
ESCAPE_OCTAL and ESCAPE_HEX are going last since they cover any
character. Note that they actually can't go together, otherwise
ESCAPE_HEX will be ignored.


Caller must provide valid source and destination pointers. Be aware that
destination buffer will not be NULL-terminated, thus caller have to append
it if needs. The supported flags are:

%ESCAPE_SPACE: (special white space, not space itself)
        '\f' - form feed
        '\n' - new line
        '\r' - carriage return
        '\t' - horizontal tab
        '\v' - vertical tab
%ESCAPE_SPECIAL:
        '\"' - double quote
        '\\' - backslash
        '\a' - alert (BEL)
        '\e' - escape
%ESCAPE_NULL:
        '\0' - null
%ESCAPE_OCTAL:
        '\NNN' - byte with octal value NNN (3 digits)
%ESCAPE_ANY:
        all previous together
%ESCAPE_NP:
        escape only non-printable characters, checked by isprint()
%ESCAPE_ANY_NP:
        all previous together
%ESCAPE_HEX:
        '\xHH' - byte with hexadecimal value HH (2 digits)
%ESCAPE_NA:
        escape only non-ascii characters, checked by isascii()
%ESCAPE_NAP:
        escape only non-printable or non-ascii characters
%ESCAPE_APPEND:
        append characters from @only to be escaped by the given classes


ESCAPE_APPEND would help to pass additional characters to the escaped, when
one of ESCAPE_NP, ESCAPE_NA, or ESCAPE_NAP is provided.

One notable caveat, the ESCAPE_NAP, ESCAPE_NP and ESCAPE_NA have the
higher priority than the rest of the flags (ESCAPE_NAP is the highest).
It doesn't make much sense to use either of them without ESCAPE_OCTAL
or ESCAPE_HEX, because they cover most of the other character classes.
ESCAPE_NAP can utilize ESCAPE_SPACE or ESCAPE_SPECIAL in addition to
the above.

Return

The total size of the escaped output that would be generated for
the given input and flags. To check whether the output was
truncated, compare the return value to osz. There is room left in
dst for a '0' terminator if and only if ret < osz.


====================


char ** kasprintf_strarray(gfp_t gfp, const char *prefix, size_t n)
allocate and fill array of sequential strings

Parameters


gfp_t gfp
flags for the slab allocator

const char *prefix
prefix to be used

size_t n
amount of lines to be allocated and filled

Description

Allocates and fills n strings using pattern 's-````zu', where prefix
is provided by caller. The caller is responsible to free them with
kfree_strarray() after use.

Returns array of strings or NULL when memory can't be allocated.


====================


void kfree_strarray(char **array, size_t n)
free a number of dynamically allocated strings contained in an array and the array itself

Parameters


char **array
Dynamically allocated array of strings to free.

size_t n
Number of strings (starting from the beginning of the array) to free.

Description

Passing a non-NULL array and n == 0 as well as NULL array are valid
use-cases. If array is NULL, the function does nothing.


====================


ssize_t strscpy_pad(char *dest, const char *src, size_t count)
Copy a C-string into a sized buffer

Parameters


char *dest
Where to copy the string to

const char *src
Where to copy the string from

size_t count
Size of destination buffer

Description

Copy the string, or as much of it as fits, into the dest buffer.  The
behavior is undefined if the string buffers overlap.  The destination
buffer is always NUL terminated, unless it's zero-sized.

If the source string is shorter than the destination buffer, zeros
the tail of the destination buffer.

For full explanation of why you may want to consider using the
'strscpy' functions please see the function docstring for strscpy().

Return


The number of characters copied (not including the trailing NUL)
-E2BIG if count is 0 or src was truncated.

====================


char * skip_spaces(const char *str)
Removes leading whitespace from str.

Parameters


const char *str
The string to be stripped.

Description

Returns a pointer to the first non-whitespace character in str.


====================


char * strim(char *s)
Removes leading and trailing whitespace from s.

Parameters


char *s
The string to be stripped.

Description

Note that the first trailing whitespace is replaced with a NUL-terminator
in the given string s. Returns a pointer to the first non-whitespace
character in s.


====================


bool sysfs_streq(const char *s1, const char *s2)
return true if strings are equal, modulo trailing newline

Parameters


const char *s1
one string

const char *s2
another string

Description

This routine returns true iff two strings are equal, treating both
NUL and newline-then-NUL as equivalent string terminations.  It's
geared for use with sysfs input strings, which generally terminate
with newlines but are compared against values without newlines.


====================


int match_string(const char * const *array, size_t n, const char *string)
matches given string in an array

Parameters


const char * const *array
array of strings

size_t n
number of strings in the array or -1 for NULL terminated arrays

const char *string
string to match with

Description

This routine will look for a string in an array of strings up to the
n-th element in the array or until the first NULL element.

Historically the value of -1 for n, was used to search in arrays that
are NULL terminated. However, the function does not make a distinction
when finishing the search: either n elements have been compared OR
the first NULL element was found.

Return

index of a string in the array if matches, or -EINVAL otherwise.


====================


int __sysfs_match_string(const char * const *array, size_t n, const char *str)
matches given string in an array

Parameters


const char * const *array
array of strings

size_t n
number of strings in the array or -1 for NULL terminated arrays

const char *str
string to match with

Description

Returns index of str in the array or -EINVAL, just like match_string().
Uses sysfs_streq instead of strcmp for matching.

This routine will look for a string in an array of strings up to the
n-th element in the array or until the first NULL element.

Historically the value of -1 for n, was used to search in arrays that
are NULL terminated. However, the function does not make a distinction
when finishing the search: either n elements have been compared OR
the first NULL element was found.


====================


char * strreplace(char *s, char old, char new)
Replace all occurrences of character in string.

Parameters


char *s
The string to operate on.

char old
The character being replaced.

char new
The character old is replaced with.

Description

Returns pointer to the nul byte at the end of s.


====================


void memcpy_and_pad(void *dest, size_t dest_len, const void *src, size_t count, int pad)
Copy one buffer to another with padding

Parameters


void *dest
Where to copy to

size_t dest_len
The destination buffer size

const void *src
Where to copy from

size_t count
The number of bytes to copy

int pad
Character to use for padding if space is left in destination.


====================


int strncasecmp(const char *s1, const char *s2, size_t len)
Case insensitive, length-limited string comparison

Parameters


const char *s1
One string

const char *s2
The other string

size_t len
the maximum number of characters to compare


====================


char * strcpy(char *dest, const char *src)
Copy a NUL terminated string

Parameters


char *dest
Where to copy the string to

const char *src
Where to copy the string from


====================


char * strncpy(char *dest, const char *src, size_t count)
Copy a length-limited, C-string

Parameters


char *dest
Where to copy the string to

const char *src
Where to copy the string from

size_t count
The maximum number of bytes to copy

Description

The result is not NUL-terminated if the source exceeds
count bytes.

In the case where the length of src is less than  that  of
count, the remainder of dest will be padded with NUL.


====================


size_t strlcpy(char *dest, const char *src, size_t size)
Copy a C-string into a sized buffer

Parameters


char *dest
Where to copy the string to

const char *src
Where to copy the string from

size_t size
size of destination buffer

Description

Compatible with *BSD: the result is always a valid
NUL-terminated string that fits in the buffer (unless,
of course, the buffer size is zero). It does not pad
out the result like strncpy() does.


====================


ssize_t strscpy(char *dest, const char *src, size_t count)
Copy a C-string into a sized buffer

Parameters


char *dest
Where to copy the string to

const char *src
Where to copy the string from

size_t count
Size of destination buffer

Description

Copy the string, or as much of it as fits, into the dest buffer.  The
behavior is undefined if the string buffers overlap.  The destination
buffer is always NUL terminated, unless it's zero-sized.

Preferred to strlcpy() since the API doesn't require reading memory
from the src string beyond the specified 'count' bytes, and since
the return value is easier to error-check than strlcpy()'s.
In addition, the implementation is robust to the string changing out
from underneath it, unlike the current strlcpy() implementation.

Preferred to strncpy() since it always returns a valid string, and
doesn't unnecessarily force the tail of the destination buffer to be
zeroed.  If zeroing is desired please use strscpy_pad().

Return


The number of characters copied (not including the trailing NUL)
-E2BIG if count is 0 or src was truncated.

====================


char * stpcpy(char *__restrict__ dest, const char *__restrict__ src)
copy a string from src to dest returning a pointer to the new end of dest, including src's NUL-terminator. May overrun dest.

Parameters


char *__restrict__ dest
pointer to end of string being copied into. Must be large enough
to receive copy.

const char *__restrict__ src
pointer to the beginning of string being copied from. Must not overlap
dest.

Description

stpcpy differs from strcpy in a key way: the return value is a pointer
to the new NUL-terminating character in dest. (For strcpy, the return
value is a pointer to the start of dest). This interface is considered
unsafe as it doesn't perform bounds checking of the inputs. As such it's
not recommended for usage. Instead, its definition is provided in case
the compiler lowers other libcalls to stpcpy.


====================


char * strcat(char *dest, const char *src)
Append one NUL-terminated string to another

Parameters


char *dest
The string to be appended to

const char *src
The string to append to it


====================


char * strncat(char *dest, const char *src, size_t count)
Append a length-limited, C-string to another

Parameters


char *dest
The string to be appended to

const char *src
The string to append to it

size_t count
The maximum numbers of bytes to copy

Description

Note that in contrast to strncpy(), strncat() ensures the result is
terminated.


====================


size_t strlcat(char *dest, const char *src, size_t count)
Append a length-limited, C-string to another

Parameters


char *dest
The string to be appended to

const char *src
The string to append to it

size_t count
The size of the destination buffer.


====================


int strcmp(const char *cs, const char *ct)
Compare two strings

Parameters


const char *cs
One string

const char *ct
Another string


====================


int strncmp(const char *cs, const char *ct, size_t count)
Compare two length-limited strings

Parameters


const char *cs
One string

const char *ct
Another string

size_t count
The maximum number of bytes to compare


====================


char * strchr(const char *s, int c)
Find the first occurrence of a character in a string

Parameters


const char *s
The string to be searched

int c
The character to search for

Description

Note that the NUL-terminator is considered part of the string, and can
be searched for.


====================


char * strchrnul(const char *s, int c)
Find and return a character in a string, or end of string

Parameters


const char *s
The string to be searched

int c
The character to search for

Description

Returns pointer to first occurrence of 'c' in s. If c is not found, then
return a pointer to the null byte at the end of s.


====================


char * strrchr(const char *s, int c)
Find the last occurrence of a character in a string

Parameters


const char *s
The string to be searched

int c
The character to search for


====================


char * strnchr(const char *s, size_t count, int c)
Find a character in a length limited string

Parameters


const char *s
The string to be searched

size_t count
The number of characters to be searched

int c
The character to search for

Description

Note that the NUL-terminator is considered part of the string, and can
be searched for.


====================


size_t strlen(const char *s)
Find the length of a string

Parameters


const char *s
The string to be sized


====================


size_t strnlen(const char *s, size_t count)
Find the length of a length-limited string

Parameters


const char *s
The string to be sized

size_t count
The maximum number of bytes to search


====================


size_t strspn(const char *s, const char *accept)
Calculate the length of the initial substring of s which only contain letters in accept

Parameters


const char *s
The string to be searched

const char *accept
The string to search for


====================


size_t strcspn(const char *s, const char *reject)
Calculate the length of the initial substring of s which does not contain letters in reject

Parameters


const char *s
The string to be searched

const char *reject
The string to avoid


====================


char * strpbrk(const char *cs, const char *ct)
Find the first occurrence of a set of characters

Parameters


const char *cs
The string to be searched

const char *ct
The characters to search for


====================


char * strsep(char **s, const char *ct)
Split a string into tokens

Parameters


char **s
The string to be searched

const char *ct
The characters to search for

Description

strsep() updates s to point after the token, ready for the next call.

It returns empty tokens, too, behaving exactly like the libc function
of that name. In fact, it was stolen from glibc2 and de-fancy-fied.
Same semantics, slimmer shape. ;)


====================


void * memset(void *s, int c, size_t count)
Fill a region of memory with the given value

Parameters


void *s
Pointer to the start of the area.

int c
The byte to fill the area with

size_t count
The size of the area.

Description

Do not use memset() to access IO space, use memset_io() instead.


====================


void * memset16(uint16_t *s, uint16_t v, size_t count)
Fill a memory area with a uint16_t

Parameters


uint16_t *s
Pointer to the start of the area.

uint16_t v
The value to fill the area with

size_t count
The number of values to store

Description

Differs from memset() in that it fills with a uint16_t instead
of a byte.  Remember that count is the number of uint16_ts to
store, not the number of bytes.


====================


void * memset32(uint32_t *s, uint32_t v, size_t count)
Fill a memory area with a uint32_t

Parameters


uint32_t *s
Pointer to the start of the area.

uint32_t v
The value to fill the area with

size_t count
The number of values to store

Description

Differs from memset() in that it fills with a uint32_t instead
of a byte.  Remember that count is the number of uint32_ts to
store, not the number of bytes.


====================


void * memset64(uint64_t *s, uint64_t v, size_t count)
Fill a memory area with a uint64_t

Parameters


uint64_t *s
Pointer to the start of the area.

uint64_t v
The value to fill the area with

size_t count
The number of values to store

Description

Differs from memset() in that it fills with a uint64_t instead
of a byte.  Remember that count is the number of uint64_ts to
store, not the number of bytes.


====================


void * memcpy(void *dest, const void *src, size_t count)
Copy one area of memory to another

Parameters


void *dest
Where to copy to

const void *src
Where to copy from

size_t count
The size of the area.

Description

You should not use this function to access IO space, use memcpy_toio()
or memcpy_fromio() instead.


====================


void * memmove(void *dest, const void *src, size_t count)
Copy one area of memory to another

Parameters


void *dest
Where to copy to

const void *src
Where to copy from

size_t count
The size of the area.

Description

Unlike memcpy(), memmove() copes with overlapping areas.


====================


__visible int memcmp(const void *cs, const void *ct, size_t count)
Compare two areas of memory

Parameters


const void *cs
One area of memory

const void *ct
Another area of memory

size_t count
The size of the area.


====================


int bcmp(const void *a, const void *b, size_t len)
returns 0 if and only if the buffers have identical contents.

Parameters


const void *a
pointer to first buffer.

const void *b
pointer to second buffer.

size_t len
size of buffers.

Description

The sign or magnitude of a non-zero return value has no particular
meaning, and architectures may implement their own more efficient bcmp(). So
while this particular implementation is a simple (tail) call to memcmp, do
not rely on anything but whether the return value is zero or non-zero.


====================


void * memscan(void *addr, int c, size_t size)
Find a character in an area of memory.

Parameters


void *addr
The memory area

int c
The byte to search for

size_t size
The size of the area.

Description

returns the address of the first occurrence of c, or 1 byte past
the area if c is not found


====================


char * strstr(const char *s1, const char *s2)
Find the first substring in a NUL terminated string

Parameters


const char *s1
The string to be searched

const char *s2
The string to search for


====================


char * strnstr(const char *s1, const char *s2, size_t len)
Find the first substring in a length-limited string

Parameters


const char *s1
The string to be searched

const char *s2
The string to search for

size_t len
the maximum number of characters to search


====================


void * memchr(const void *s, int c, size_t n)
Find a character in an area of memory.

Parameters


const void *s
The memory area

int c
The byte to search for

size_t n
The size of the area.

Description

returns the address of the first occurrence of c, or NULL
if c is not found


====================


void * memchr_inv(const void *start, int c, size_t bytes)
Find an unmatching character in an area of memory.

Parameters


const void *start
The memory area

int c
Find a character other than c

size_t bytes
The size of the area.

Description

returns the address of the first character other than c, or NULL
if the whole buffer contains just c.


====================


sysfs_match_string(_a, _s)
matches given string in an array

Parameters


_a
array of strings

_s
string to match with

Description

Helper for __sysfs_match_string(). Calculates the size of a automatically.


====================


bool strstarts(const char *str, const char *prefix)
does str start with prefix?

Parameters


const char *str
string to examine

const char *prefix
prefix to look for.


====================


void memzero_explicit(void *s, size_t count)
Fill a region of memory (e.g. sensitive keying data) with 0s.

Parameters


void *s
Pointer to the start of the area.

size_t count
The size of the area.

Note

usually using memset() is just fine (!), but in cases
where clearing out _local_ data at the end of a scope is
necessary, memzero_explicit() should be used instead in
order to prevent the compiler from optimising away zeroing.

Description

memzero_explicit() doesn't need an arch-specific version as
it just invokes the one of memset() implicitly.


====================


const char * kbasename(const char *path)
return the last part of a pathname.

Parameters


const char *path
path to extract the filename from.


====================


memset_after(obj, v, member)
Set a value after a struct member to the end of a struct

Parameters


obj
Address of target struct instance

v
Byte value to repeatedly write

member
after which struct member to start writing bytes

Description

This is good for clearing padding following the given member.


====================


memset_startat(obj, v, member)
Set a value starting at a member to the end of a struct

Parameters


obj
Address of target struct instance

v
Byte value to repeatedly write

member
struct member to start writing at

Description

Note that if there is padding between the prior member and the target
member, memset_after() should be used to clear the prior padding.


====================


size_t str_has_prefix(const char *str, const char *prefix)
Test if a string has a given prefix

Parameters


const char *str
The string to test

const char *prefix
The string to see if str starts with

Description


A common way to test a prefix of a string is to do:
strncmp(str, prefix, sizeof(prefix) - 1)

But this can lead to bugs due to typos, or if prefix is a pointer
and not a constant. Instead use str_has_prefix().

Return


strlen(prefix) if str starts with prefix
0 if str does not start with prefix

====================


char * kstrdup(const char *s, gfp_t gfp)
allocate space for and copy an existing string

Parameters


const char *s
the string to duplicate

gfp_t gfp
the GFP mask used in the kmalloc() call when allocating memory

Return

newly allocated copy of s or NULL in case of error


====================


const char * kstrdup_const(const char *s, gfp_t gfp)
conditionally duplicate an existing const string

Parameters


const char *s
the string to duplicate

gfp_t gfp
the GFP mask used in the kmalloc() call when allocating memory

Note

Strings allocated by kstrdup_const should be freed by kfree_const and
must not be passed to krealloc().

Return

source string if it is in .rodata section otherwise
fallback to kstrdup.


====================


char * kstrndup(const char *s, size_t max, gfp_t gfp)
allocate space for and copy an existing string

Parameters


const char *s
the string to duplicate

size_t max
read at most max chars from s

gfp_t gfp
the GFP mask used in the kmalloc() call when allocating memory

Note

Use kmemdup_nul() instead if the size is known exactly.

Return

newly allocated copy of s or NULL in case of error


====================


void * kmemdup(const void *src, size_t len, gfp_t gfp)
duplicate region of memory

Parameters


const void *src
memory region to duplicate

size_t len
memory region length

gfp_t gfp
GFP mask to use

Return

newly allocated copy of src or NULL in case of error


====================


char * kmemdup_nul(const char *s, size_t len, gfp_t gfp)
Create a NUL-terminated string from unterminated data

Parameters


const char *s
The data to stringify

size_t len
The size of the data

gfp_t gfp
the GFP mask used in the kmalloc() call when allocating memory

Return

newly allocated copy of s with NUL-termination or NULL in
case of error


====================


void * memdup_user(const void __user *src, size_t len)
duplicate memory region from user space

Parameters


const void __user *src
source address in user space

size_t len
number of bytes to copy

Return

an ERR_PTR() on failure.  Result is physically
contiguous, to be freed by kfree().


====================


void * vmemdup_user(const void __user *src, size_t len)
duplicate memory region from user space

Parameters


const void __user *src
source address in user space

size_t len
number of bytes to copy

Return

an ERR_PTR() on failure.  Result may be not
physically contiguous.  Use kvfree() to free.


====================


char * strndup_user(const char __user *s, long n)
duplicate an existing string from user space

Parameters


const char __user *s
The string to duplicate

long n
Maximum number of bytes to copy, including the trailing NUL.

Return

newly allocated copy of s or an ERR_PTR() in case of error


====================


void * memdup_user_nul(const void __user *src, size_t len)
duplicate memory region from user space and NUL-terminate

Parameters


const void __user *src
source address in user space

size_t len
number of bytes to copy

Return

an ERR_PTR() on failure.


====================


void set_bit(long nr, volatile unsigned long *addr)
Atomically set a bit in memory

Parameters


long nr
the bit to set

volatile unsigned long *addr
the address to start counting from

Description

This is a relaxed atomic operation (no implied memory barriers).

Note that nr may be almost arbitrarily large; this function is not
restricted to acting on a single-word quantity.


====================


void clear_bit(long nr, volatile unsigned long *addr)
Clears a bit in memory

Parameters


long nr
Bit to clear

volatile unsigned long *addr
Address to start counting from

Description

This is a relaxed atomic operation (no implied memory barriers).


====================


void change_bit(long nr, volatile unsigned long *addr)
Toggle a bit in memory

Parameters


long nr
Bit to change

volatile unsigned long *addr
Address to start counting from

Description

This is a relaxed atomic operation (no implied memory barriers).

Note that nr may be almost arbitrarily large; this function is not
restricted to acting on a single-word quantity.


====================


bool test_and_set_bit(long nr, volatile unsigned long *addr)
Set a bit and return its old value

Parameters


long nr
Bit to set

volatile unsigned long *addr
Address to count from

Description

This is an atomic fully-ordered operation (implied full memory barrier).


====================


bool test_and_clear_bit(long nr, volatile unsigned long *addr)
Clear a bit and return its old value

Parameters


long nr
Bit to clear

volatile unsigned long *addr
Address to count from

Description

This is an atomic fully-ordered operation (implied full memory barrier).


====================


bool test_and_change_bit(long nr, volatile unsigned long *addr)
Change a bit and return its old value

Parameters


long nr
Bit to change

volatile unsigned long *addr
Address to count from

Description

This is an atomic fully-ordered operation (implied full memory barrier).


====================


void __set_bit(long nr, volatile unsigned long *addr)
Set a bit in memory

Parameters


long nr
the bit to set

volatile unsigned long *addr
the address to start counting from

Description

Unlike set_bit(), this function is non-atomic. If it is called on the same
region of memory concurrently, the effect may be that only one operation
succeeds.


====================


void __clear_bit(long nr, volatile unsigned long *addr)
Clears a bit in memory

Parameters


long nr
the bit to clear

volatile unsigned long *addr
the address to start counting from

Description

Unlike clear_bit(), this function is non-atomic. If it is called on the same
region of memory concurrently, the effect may be that only one operation
succeeds.


====================


void __change_bit(long nr, volatile unsigned long *addr)
Toggle a bit in memory

Parameters


long nr
the bit to change

volatile unsigned long *addr
the address to start counting from

Description

Unlike change_bit(), this function is non-atomic. If it is called on the same
region of memory concurrently, the effect may be that only one operation
succeeds.


====================


bool __test_and_set_bit(long nr, volatile unsigned long *addr)
Set a bit and return its old value

Parameters


long nr
Bit to set

volatile unsigned long *addr
Address to count from

Description

This operation is non-atomic. If two instances of this operation race, one
can appear to succeed but actually fail.


====================


bool __test_and_clear_bit(long nr, volatile unsigned long *addr)
Clear a bit and return its old value

Parameters


long nr
Bit to clear

volatile unsigned long *addr
Address to count from

Description

This operation is non-atomic. If two instances of this operation race, one
can appear to succeed but actually fail.


====================


bool __test_and_change_bit(long nr, volatile unsigned long *addr)
Change a bit and return its old value

Parameters


long nr
Bit to change

volatile unsigned long *addr
Address to count from

Description

This operation is non-atomic. If two instances of this operation race, one
can appear to succeed but actually fail.


====================


bool test_bit(long nr, const volatile unsigned long *addr)
Determine whether a bit is set

Parameters


long nr
bit number to test

const volatile unsigned long *addr
Address to start counting from


====================


void clear_bit_unlock(long nr, volatile unsigned long *addr)
Clear a bit in memory, for unlock

Parameters


long nr
the bit to set

volatile unsigned long *addr
the address to start counting from

Description

This operation is atomic and provides release barrier semantics.


====================


void __clear_bit_unlock(long nr, volatile unsigned long *addr)
Clears a bit in memory

Parameters


long nr
Bit to clear

volatile unsigned long *addr
Address to start counting from

Description

This is a non-atomic operation but implies a release barrier before the
memory operation. It can be used for an unlock if no other CPUs can
concurrently modify other bits in the word.


====================


bool test_and_set_bit_lock(long nr, volatile unsigned long *addr)
Set a bit and return its old value, for lock

Parameters


long nr
Bit to set

volatile unsigned long *addr
Address to count from

Description

This operation is atomic and provides acquire barrier semantics if
the returned value is 0.
It can be used to implement bit locks.


====================


bool clear_bit_unlock_is_negative_byte(long nr, volatile unsigned long *addr)
Clear a bit in memory and test if bottom byte is negative, for unlock.

Parameters


long nr
the bit to clear

volatile unsigned long *addr
the address to start counting from

Description

This operation is atomic and provides release barrier semantics.

This is a bit of a one-trick-pony for the filemap code, which clears
PG_locked and tests PG_waiters,

The problem is once we have a large bitmap, we have a chance to get a
bitmask or list more than one page. Especially for list, it could be
as complex as 0,3,5,7,9,...¦ We have no simple way to know it exact size.
It turns out bin_attribute is a way to break this limit. bin_attribute
has show entry as below:
static ssize_t
example_bin_attribute_show(struct file *filp, struct kobject *kobj,


struct bin_attribute *attr, char *buf,
loff_t offset, size_t count)


{
...¦

}
With the new offset and count parameters, this makes sysfs ABI be able
to support file size more than one page. For example, offset could be
>= 4096.
bitmap_print_bitmask_to_buf(), bitmap_print_list_to_buf() wit their
cpumap wrapper cpumap_print_bitmask_to_buf(), cpumap_print_list_to_buf()
make those drivers be able to support large bitmask and list after they
move to use bin_attribute. In result, we have to pass the corresponding
parameters such as off, count from bin_attribute show entry to this API.

The role of cpumap_print_bitmask_to_buf() and cpumap_print_list_to_buf()
is similar with cpumap_print_to_pagebuf(),  the difference is that
bitmap_print_to_pagebuf() mainly serves sysfs attribute with the assumption
the destination buffer is exactly one page and won't be more than one page.
cpumap_print_bitmask_to_buf() and cpumap_print_list_to_buf(), on the other
hand, mainly serves bin_attribute which doesn't work with exact one page,
and it can break the size limit of converted decimal list and hexadecimal
bitmask.

WARNING!

This function is not a replacement for sprintf() or bitmap_print_to_pagebuf().
It is intended to workaround sysfs limitations discussed above and should be
used carefully in general case for the following reasons:



Time complexity is O(nbits^2/count), comparing to O(nbits) for snprintf().
Memory complexity is O(nbits), comparing to O(1) for snprintf().
off and count are NOT offset and number of bits to print.
If printing part of bitmap as list, the resulting string is not a correct
list representation of bitmap. Particularly, some bits within or out of
related interval may be erroneously set or unset. The format of the string
may be broken, so bitmap_parselist-like parser may fail parsing it.
If printing the whole bitmap as list by parts, user must ensure the order
of calls of the function such that the offset is incremented linearly.
If printing the whole bitmap as list by parts, user must keep bitmap
unchanged between the very first and very last call. Otherwise concatenated
result may be incorrect, and format may be broken.


Returns the number of characters actually printed to buf


====================


int bitmap_print_list_to_buf(char *buf, const unsigned long *maskp, int nmaskbits, loff_t off, size_t count)
convert bitmap to decimal list format ASCII string

Parameters


char *buf
buffer into which string is placed

const unsigned long *maskp
pointer to bitmap to convert

int nmaskbits
size of bitmap, in bits

loff_t off
in the string from which we are copying, We copy to buf

size_t count
the maximum number of bytes to print

Description

Everything is same with the above bitmap_print_bitmask_to_buf() except
the print format.


====================


int bitmap_parselist(const char *buf, unsigned long *maskp, int nmaskbits)
convert list format ASCII string to bitmap

Parameters


const char *buf
read user string from this buffer; must be terminated
with a 0 or n.

unsigned long *maskp
write resulting mask here

int nmaskbits
number of bits in mask to be written

Description

Input format is a comma-separated list of decimal numbers and
ranges.  Consecutively set bits are shown as two hyphen-separated
decimal numbers, the smallest and largest bit numbers set in
the range.
Optionally each range can be postfixed to denote that only parts of it
should be set. The range will divided to groups of specific size.
From each group will be used only defined amount of bits.
Syntax: range:used_size/group_size

Example

0-1023:2/256 ==> 0,1,256,257,512,513,768,769
The value 'N' can be used as a dynamically substituted token for the
maximum allowed value; i.e (nmaskbits - 1).  Keep in mind that it is
dynamic, so if system changes cause the bitmap width to change, such
as more cores in a CPU list, then any ranges using N will also change.

Return

0 on success, -errno on invalid input strings. Error values:



-EINVAL: wrong region format
-EINVAL: invalid character in string
-ERANGE: bit number specified too large for mask
-EOVERFLOW: integer overflow in the input parameters



====================


int bitmap_parselist_user(const char __user *ubuf, unsigned int ulen, unsigned long *maskp, int nmaskbits)
convert user buffer's list format ASCII string to bitmap

Parameters


const char __user *ubuf
pointer to user buffer containing string.

unsigned int ulen
buffer size in bytes.  If string is smaller than this
then it must be terminated with a 0.

unsigned long *maskp
pointer to bitmap array that will contain result.

int nmaskbits
size of bitmap, in bits.

Description

Wrapper for bitmap_parselist(), providing it with user buffer.


====================


int bitmap_parse(const char *start, unsigned int buflen, unsigned long *maskp, int nmaskbits)
convert an ASCII hex string into a bitmap.

Parameters


const char *start
pointer to buffer containing string.

unsigned int buflen
buffer size in bytes.  If string is smaller than this
then it must be terminated with a 0 or n. In that case,
UINT_MAX may be provided instead of string length.

unsigned long *maskp
pointer to bitmap array that will contain result.

int nmaskbits
size of bitmap, in bits.

Description

Commas group hex digits into chunks.  Each chunk defines exactly 32
bits of the resultant bitmask.  No chunk may specify a value larger
than 32 bits (-EOVERFLOW), and if a chunk specifies a smaller value
then leading 0-bits are prepended.  -EINVAL is returned for illegal
characters. Grouping such as '1,,5', ',44', ',' or '' is allowed.
Leading, embedded and trailing whitespace accepted.


====================


void bitmap_remap(unsigned long *dst, const unsigned long *src, const unsigned long *old, const unsigned long *new, unsigned int nbits)
Apply map defined by a pair of bitmaps to another bitmap

Parameters


unsigned long *dst
remapped result

const unsigned long *src
subset to be remapped

const unsigned long *old
defines domain of map

const unsigned long *new
defines range of map

unsigned int nbits
number of bits in each of these bitmaps

Description

Let old and new define a mapping of bit positions, such that
whatever position is held by the n-th set bit in old is mapped
to the n-th set bit in new.  In the more general case, allowing
for the possibility that the weight 'w' of new is less than the
weight of old, map the position of the n-th set bit in old to
the position of the m-th set bit in new, where m == n % w.

If either of the old and new bitmaps are empty, or if src and
dst point to the same location, then this routine copies src
to dst.

The positions of unset bits in old are mapped to themselves
(the identify map).

Apply the above specified mapping to src, placing the result in
dst, clearing any bits previously set in dst.

For example, lets say that old has bits 4 through 7 set, and
new has bits 12 through 15 set.  This defines the mapping of bit
position 4 to 12, 5 to 13, 6 to 14 and 7 to 15, and of all other
bit positions unchanged.  So if say src comes into this routine
with bits 1, 5 and 7 set, then dst should leave with bits 1,
13 and 15 set.


====================


int bitmap_bitremap(int oldbit, const unsigned long *old, const unsigned long *new, int bits)
Apply map defined by a pair of bitmaps to a single bit

Parameters


int oldbit
bit position to be mapped

const unsigned long *old
defines domain of map

const unsigned long *new
defines range of map

int bits
number of bits in each of these bitmaps

Description

Let old and new define a mapping of bit positions, such that
whatever position is held by the n-th set bit in old is mapped
to the n-th set bit in new.  In the more general case, allowing
for the possibility that the weight 'w' of new is less than the
weight of old, map the position of the n-th set bit in old to
the position of the m-th set bit in new, where m == n % w.

The positions of unset bits in old are mapped to themselves
(the identify map).

Apply the above specified mapping to bit position oldbit, returning
the new bit position.

For example, lets say that old has bits 4 through 7 set, and
new has bits 12 through 15 set.  This defines the mapping of bit
position 4 to 12, 5 to 13, 6 to 14 and 7 to 15, and of all other
bit positions unchanged.  So if say oldbit is 5, then this routine
returns 13.


====================


int bitmap_find_free_region(unsigned long *bitmap, unsigned int bits, int order)
find a contiguous aligned mem region

Parameters


unsigned long *bitmap
array of unsigned longs corresponding to the bitmap

unsigned int bits
number of bits in the bitmap

int order
region size (log base 2 of number of bits) to find

Description

Find a region of free (zero) bits in a bitmap of bits bits and
allocate them (set them to one).  Only consider regions of length
a power (order) of two, aligned to that power of two, which
makes the search algorithm much faster.

Return the bit offset in bitmap of the allocated region,
or -errno on failure.


====================


void bitmap_release_region(unsigned long *bitmap, unsigned int pos, int order)
release allocated bitmap region

Parameters


unsigned long *bitmap
array of unsigned longs corresponding to the bitmap

unsigned int pos
beginning of bit region to release

int order
region size (log base 2 of number of bits) to release

Description

This is the complement to __bitmap_find_free_region() and releases
the found region (by clearing it in the bitmap).

No return value.


====================


int bitmap_allocate_region(unsigned long *bitmap, unsigned int pos, int order)
allocate bitmap region

Parameters


unsigned long *bitmap
array of unsigned longs corresponding to the bitmap

unsigned int pos
beginning of bit region to allocate

int order
region size (log base 2 of number of bits) to allocate

Description

Allocate (set bits in) a specified region of a bitmap.

Return 0 on success, or -EBUSY if specified region wasn't
free (not all bits were zero).


====================


void bitmap_copy_le(unsigned long *dst, const unsigned long *src, unsigned int nbits)
copy a bitmap, putting the bits into little-endian order.

Parameters


unsigned long *dst
destination buffer

const unsigned long *src
bitmap to copy

unsigned int nbits
number of bits in the bitmap

Description

Require nbits % BITS_PER_LONG == 0.


====================


void bitmap_from_arr32(unsigned long *bitmap, const u32 *buf, unsigned int nbits)
copy the contents of u32 array of bits to bitmap

Parameters


unsigned long *bitmap
array of unsigned longs, the destination bitmap

const u32 *buf
array of u32 (in host byte order), the source bitmap

unsigned int nbits
number of bits in bitmap


====================


void bitmap_to_arr32(u32 *buf, const unsigned long *bitmap, unsigned int nbits)
copy the contents of bitmap to a u32 array of bits

Parameters


u32 *buf
array of u32 (in host byte order), the dest bitmap

const unsigned long *bitmap
array of unsigned longs, the source bitmap

unsigned int nbits
number of bits in bitmap


====================


int get_option(char **str, int *pint)
Parse integer from an option string

Parameters


char **stroption string

int *pint(optional output) integer value parsed from str
Read an int from an option string; if available accept a subsequent
comma as well.
When pint is NULL the function can be used as a validator of
the current option in the string.
Return values:
0 - no int in string
1 - int found, no subsequent comma
2 - int found including a subsequent comma
3 - hyphen found to denote a range
Leading hyphen without integer is no integer case, but we consume it
for the sake of simplification.



====================


char * get_options(const char *str, int nints, int *ints)
Parse a string into a list of integers

Parameters


const char *strString to be parsed

int nintssize of integer array

int *intsinteger array (must have room for at least one element)
This function parses a string containing a comma-separated
list of integers, a hyphen-separated range of _positive_ integers,
or a combination of both.  The parse halts when the array is
full, or when no more numbers can be retrieved from the
string.
When nints is 0, the function just validates the given str and
returns the amount of parseable integers as described below.


Return


The first element is filled by the number of collected integers
in the range. The rest is what was parsed from the str.
Return value is the character in the string which caused
the parse to end (typically a null terminator, if str is
completely parseable).


====================


unsigned long long memparse(const char *ptr, char **retptr)
parse a string with mem suffixes into a number

Parameters


const char *ptrWhere parse begins

char **retptr(output) Optional pointer to next char after parse completes
Parses a string into a number.  The number stored at ptr is
potentially suffixed with K, M, G, T, P, E.



====================


void sort_r(void *base, size_t num, size_t size, cmp_r_func_t cmp_func, swap_r_func_t swap_func, const void *priv)
sort an array of elements

Parameters


void *base
pointer to data to sort

size_t num
number of elements

size_t size
size of each element

cmp_r_func_t cmp_func
pointer to comparison function

swap_r_func_t swap_func
pointer to swap function or NULL

const void *priv
third argument passed to comparison function

Description

This function does a heapsort on the given array.  You may provide
a swap_func function if you need to do something more than a memory
copy (e.g. fix up pointers or auxiliary data), but the built-in swap
avoids a slow retpoline and so is significantly faster.

Sorting time is O(n log n) both on average and worst-case. While
quicksort is slightly faster on average, it suffers from exploitable
O(n*n) worst-case behavior and extra memory requirements that make
it less suitable for kernel use.


====================


void list_sort(void *priv, struct list_head *head, list_cmp_func_t cmp)
sort a list

Parameters


void *priv
private data, opaque to list_sort(), passed to cmp

struct list_head *head
the list to sort

list_cmp_func_t cmp
the elements comparison function

Description

The comparison function cmp must return > 0 if a should sort after
b ('a > b' if you want an ascending sort), and <= 0 if a should
sort before b or their original order should be preserved.  It is
always called with the element that came first in the input in a,
and list_sort is a stable sort, so it is not necessary to distinguish
the a < b and a == b cases.

This is compatible with two styles of cmp function:
- The traditional style which returns <0 / =0 / >0, or
- Returning a boolean 0/1.
The latter offers a chance to save a few cycles in the comparison
(which is used by e.g. plug_ctx_cmp() in block/blk-mq.c).

A good way to write a multi-word comparison is:

if (a->high != b->high)
        return a->high > b->high;
if (a->middle != b->middle)
        return a->middle > b->middle;
return a->low > b->low;


This mergesort is as eager as possible while always performing at least
2:1 balanced merges.  Given two pending sublists of size 2^k, they are
merged to a size-2^(k+1) list as soon as we have 2^k following elements.

Thus, it will avoid cache thrashing as long as 3*2^k elements can
fit into the cache.  Not quite as good as a fully-eager bottom-up
mergesort, but it does use 0.2*n fewer comparisons, so is faster in
the common case that everything fits into L1.

The merging is controlled by 'count', the number of elements in the
pending lists.  This is beautifully simple code, but rather subtle.

Each time we increment 'count', we set one bit (bit k) and clear
bits k-1 .. 0.  Each time this happens (except the very first time
for each bit, when count increments to 2^k), we merge two lists of
size 2^k into one list of size 2^(k+1).

This merge happens exactly when the count reaches an odd multiple of
2^k, which is when we have 2^k elements pending in smaller lists,
so it's safe to merge away two lists of size 2^k.

After this happens twice, we have created two lists of size 2^(k+1),
which will be merged into a list of size 2^(k+2) before we create
a third list of size 2^(k+1), so there are never more than two pending.

The number of pending lists of size 2^k is determined by the
state of bit k of 'count' plus two extra pieces of information:


The state of bit k-1 (when k == 0, consider bit -1 always set), and
Whether the higher-order bits are zero or non-zero (i.e.
is count >= 2^(k+1)).
There are six states we distinguish.  'x' represents some arbitrary
bits, and 'y' represents some arbitrary non-zero bits:
0:  00x: 0 pending of size 2^k;           x pending of sizes < 2^k
1:  01x: 0 pending of size 2^k; 2^(k-1) + x pending of sizes < 2^k
2: x10x: 0 pending of size 2^k; 2^k     + x pending of sizes < 2^k
3: x11x: 1 pending of size 2^k; 2^(k-1) + x pending of sizes < 2^k
4: y00x: 1 pending of size 2^k; 2^k     + x pending of sizes < 2^k
5: y01x: 2 pending of size 2^k; 2^(k-1) + x pending of sizes < 2^k
(merge and loop back to state 2)

We gain lists of size 2^k in the 2->3 and 4->5 transitions (because
bit k-1 is set while the more significant bits are non-zero) and
merge them away in the 5->2 transition.  Note in particular that just
before the 5->2 transition, all lower-order bits are 11 (state 3),
so there is one list of each smaller size.

When we reach the end of the input, we merge all the pending
lists, from smallest to largest.  If you work through cases 2 to
5 above, you can see that the number of elements we merge with a list
of size 2^k varies from 2^(k-1) (cases 3 and 5 when x == 0) to
2^(k+1) - 1 (second merge of case 5 when x == 2^(k-1) - 1).

INTRODUCTION


The textsearch infrastructure provides text searching facilities for
both linear and non-linear data. Individual search algorithms are
implemented in modules and chosen by the user.

ARCHITECTURE

  User
  +----------------+
  |        finish()|<--------------(6)-----------------+
  |get_next_block()|<--------------(5)---------------+ |
  |                |                     Algorithm   | |
  |                |                    +------------------------------+
  |                |                    |  init()   find()   destroy() |
  |                |                    +------------------------------+
  |                |       Core API           ^       ^          ^
  |                |      +---------------+  (2)     (4)        (8)
  |             (1)|----->| prepare()     |---+       |          |
  |             (3)|----->| find()/next() |-----------+          |
  |             (7)|----->| destroy()     |----------------------+
  +----------------+      +---------------+

(1) User configures a search by calling textsearch_prepare() specifying
    the search parameters such as the pattern and algorithm name.
(2) Core requests the algorithm to allocate and initialize a search
    configuration according to the specified parameters.
(3) User starts the search(es) by calling textsearch_find() or
    textsearch_next() to fetch subsequent occurrences. A state variable
    is provided to the algorithm to store persistent variables.
(4) Core eventually resets the search offset and forwards the find()
    request to the algorithm.
(5) Algorithm calls get_next_block() provided by the user continuously
    to fetch the data to be searched in block by block.
(6) Algorithm invokes finish() after the last call to get_next_block
    to clean up any leftovers from get_next_block. (Optional)
(7) User destroys the configuration by calling textsearch_destroy().
(8) Core notifies the algorithm to destroy algorithm specific
    allocations. (Optional)


USAGE


Before a search can be performed, a configuration must be created
by calling textsearch_prepare() specifying the searching algorithm,
the pattern to look for and flags. As a flag, you can set TS_IGNORECASE
to perform case insensitive matching. But it might slow down
performance of algorithm, so you should use it at own your risk.
The returned configuration may then be used for an arbitrary
amount of times and even in parallel as long as a separate struct
ts_state variable is provided to every instance.
The actual search is performed by either calling
textsearch_find_continuous() for linear data or by providing
an own get_next_block() implementation and
calling textsearch_find(). Both functions return
the position of the first occurrence of the pattern or UINT_MAX if
no match was found. Subsequent occurrences can be found by calling
textsearch_next() regardless of the linearity of the data.
Once you're done using a configuration it must be given back via
textsearch_destroy.

EXAMPLE:

int pos;
struct ts_config *conf;
struct ts_state state;
const char *pattern = "chicken";
const char *example = "We dance the funky chicken";

conf = textsearch_prepare("kmp", pattern, strlen(pattern),
                          GFP_KERNEL, TS_AUTOLOAD);
if (IS_ERR(conf)) {
    err = PTR_ERR(conf);
    goto errout;
}

pos = textsearch_find_continuous(conf, &state, example, strlen(example));
if (pos != UINT_MAX)
    panic("Oh my god, dancing chickens at %d\n", pos);

textsearch_destroy(conf);



====================


int textsearch_register(struct ts_ops *ops)
register a textsearch module

Parameters


struct ts_ops *ops
operations lookup table

Description

This function must be called by textsearch modules to announce
their presence. The specified &**ops** must have name set to a
unique identifier and the callbacks find(), init(), get_pattern(),
and get_pattern_len() must be implemented.

Returns 0 or -EEXISTS if another module has already registered
with same name.


====================


int textsearch_unregister(struct ts_ops *ops)
unregister a textsearch module

Parameters


struct ts_ops *ops
operations lookup table

Description

This function must be called by textsearch modules to announce
their disappearance for examples when the module gets unloaded.
The ops parameter must be the same as the one during the
registration.

Returns 0 on success or -ENOENT if no matching textsearch
registration was found.


====================


unsigned int textsearch_find_continuous(struct ts_config *conf, struct ts_state *state, const void *data, unsigned int len)
search a pattern in continuous/linear data

Parameters


struct ts_config *conf
search configuration

struct ts_state *state
search state

const void *data
data to search in

unsigned int len
length of data

Description

A simplified version of textsearch_find() for continuous/linear data.
Call textsearch_next() to retrieve subsequent matches.

Returns the position of first occurrence of the pattern or
UINT_MAX if no occurrence was found.


====================


struct ts_config * textsearch_prepare(const char *algo, const void *pattern, unsigned int len, gfp_t gfp_mask, int flags)
Prepare a search

Parameters


const char *algo
name of search algorithm

const void *pattern
pattern data

unsigned int len
length of pattern

gfp_t gfp_mask
allocation mask

int flags
search flags

Description

Looks up the search algorithm module and creates a new textsearch
configuration for the specified pattern.

Returns a new textsearch configuration according to the specified
parameters or a ERR_PTR(). If a zero length pattern is passed, this
function returns EINVAL.

Note


The format of the pattern may not be compatible between
the various search algorithms.


====================


void textsearch_destroy(struct ts_config *conf)
destroy a search configuration

Parameters


struct ts_config *conf
search configuration

Description

Releases all references of the configuration and frees
up the memory.


====================


unsigned int textsearch_next(struct ts_config *conf, struct ts_state *state)
continue searching for a pattern

Parameters


struct ts_config *conf
search configuration

struct ts_state *state
search state

Description

Continues a search looking for more occurrences of the pattern.
textsearch_find() must be called to find the first occurrence
in order to reset the state.

Returns the position of the next occurrence of the pattern or
UINT_MAX if not match was found.


====================


unsigned int textsearch_find(struct ts_config *conf, struct ts_state *state)
start searching for a pattern

Parameters


struct ts_config *conf
search configuration

struct ts_state *state
search state

Description

Returns the position of first occurrence of the pattern or
UINT_MAX if no match was found.


====================


void * textsearch_get_pattern(struct ts_config *conf)
return head of the pattern

Parameters


struct ts_config *conf
search configuration


====================


unsigned int textsearch_get_pattern_len(struct ts_config *conf)
return length of the pattern

Parameters


struct ts_config *conf
search configuration


====================


uint8_t crc4(uint8_t c, uint64_t x, int bits)
calculate the 4-bit crc of a value.

Parameters


uint8_t c
starting crc4

uint64_t x
value to checksum

int bits
number of bits in x to checksum

Description

Returns the crc4 value of x, using polynomial 0b10111.

The x value is treated as left-aligned, and bits above bits are ignored
in the crc calculations.


====================


u8 crc7_be(u8 crc, const u8 *buffer, size_t len)
update the CRC7 for the data buffer

Parameters


u8 crc
previous CRC7 value

const u8 *buffer
data pointer

size_t len
number of bytes in the buffer

Context

any

Description

Returns the updated CRC7 value.
The CRC7 is left-aligned in the byte (the lsbit is always 0), as that
makes the computation easier, and all callers want it in that form.


====================


void crc8_populate_msb(u8 table[CRC8_TABLE_SIZE], u8 polynomial)
fill crc table for given polynomial in reverse bit order.

Parameters


u8 table[CRC8_TABLE_SIZE]
table to be filled.

u8 polynomial
polynomial for which table is to be filled.


====================


void crc8_populate_lsb(u8 table[CRC8_TABLE_SIZE], u8 polynomial)
fill crc table for given polynomial in regular bit order.

Parameters


u8 table[CRC8_TABLE_SIZE]
table to be filled.

u8 polynomial
polynomial for which table is to be filled.


====================


u8 crc8(const u8 table[CRC8_TABLE_SIZE], const u8 *pdata, size_t nbytes, u8 crc)
calculate a crc8 over the given input data.

Parameters


const u8 table[CRC8_TABLE_SIZE]
crc table used for calculation.

const u8 *pdata
pointer to data buffer.

size_t nbytes
number of bytes in data buffer.

u8 crc
previous returned crc8 value.


====================


u16 crc16(u16 crc, u8 const *buffer, size_t len)
compute the CRC-16 for the data buffer

Parameters


u16 crc
previous CRC value

u8 const *buffer
data pointer

size_t len
number of bytes in the buffer

Description

Returns the updated CRC value.


====================


u32 __pure crc32_le_generic(u32 crc, unsigned char const *p, size_t len, const u32 (*tab)[256], u32 polynomial)
Calculate bitwise little-endian Ethernet AUTODIN II CRC32/CRC32C

Parameters


u32 crc
seed value for computation.  ~0 for Ethernet, sometimes 0 for other
uses, or the previous crc32/crc32c value if computing incrementally.

unsigned char const *p
pointer to buffer over which CRC32/CRC32C is run

size_t len
length of buffer p

const u32 (*tab)[256]
little-endian Ethernet table

u32 polynomial
CRC32/CRC32c LE polynomial


====================


u32 crc32_generic_shift(u32 crc, size_t len, u32 polynomial)
Append len 0 bytes to crc, in logarithmic time

Parameters


u32 crc
The original little-endian CRC (i.e. lsbit is x^31 coefficient)

size_t len
The number of bytes. crc is multiplied by x^(8***len**)

u32 polynomial
The modulus used to reduce the result to 32 bits.

Description

It's possible to parallelize CRC computations by computing a CRC
over separate ranges of a buffer, then summing them.
This shifts the given CRC by 8*len bits (i.e. produces the same effect
as appending len bytes of zero to the data), in time proportional
to log(len).


====================


u32 __pure crc32_be_generic(u32 crc, unsigned char const *p, size_t len, const u32 (*tab)[256], u32 polynomial)
Calculate bitwise big-endian Ethernet AUTODIN II CRC32

Parameters


u32 crc
seed value for computation.  ~0 for Ethernet, sometimes 0 for
other uses, or the previous crc32 value if computing incrementally.

unsigned char const *p
pointer to buffer over which CRC32 is run

size_t len
length of buffer p

const u32 (*tab)[256]
big-endian Ethernet table

u32 polynomial
CRC32 BE polynomial


====================


u16 crc_ccitt(u16 crc, u8 const *buffer, size_t len)
recompute the CRC (CRC-CCITT variant) for the data buffer

Parameters


u16 crc
previous CRC value

u8 const *buffer
data pointer

size_t len
number of bytes in the buffer


====================


u16 crc_ccitt_false(u16 crc, u8 const *buffer, size_t len)
recompute the CRC (CRC-CCITT-FALSE variant) for the data buffer

Parameters


u16 crc
previous CRC value

u8 const *buffer
data pointer

size_t len
number of bytes in the buffer


====================


u16 crc_itu_t(u16 crc, const u8 *buffer, size_t len)
Compute the CRC-ITU-T for the data buffer

Parameters


u16 crc
previous CRC value

const u8 *buffer
data pointer

size_t len
number of bytes in the buffer

Description

Returns the updated CRC value


====================


bool is_power_of_2(unsigned long n)
check if a value is a power of two

Parameters


unsigned long n
the value to check

Description

Determine whether some value is a power of two, where zero is
not considered a power of two.

Return

true if n is a power of 2, otherwise false.


====================


unsigned long __roundup_pow_of_two(unsigned long n)
round up to nearest power of two

Parameters


unsigned long n
value to round up


====================


unsigned long __rounddown_pow_of_two(unsigned long n)
round down to nearest power of two

Parameters


unsigned long n
value to round down


====================


const_ilog2(n)
log base 2 of 32-bit or a 64-bit constant unsigned value

Parameters


n
parameter

Description

Use this where sparse expects a true constant expression, e.g. for array
indices.


====================


ilog2(n)
log base 2 of 32-bit or a 64-bit unsigned value

Parameters


n
parameter

Description

constant-capable log of base 2 calculation
- this can be used to initialise global variables from constant data, hence
the massive ternary operator construction

selects the appropriately-sized optimised version depending on sizeof(n)


====================


roundup_pow_of_two(n)
round the given value up to nearest power of two

Parameters


n
parameter

Description

round the given value up to the nearest power of two
- the result is undefined when n == 0
- this can be used to initialise global variables from constant data


====================


rounddown_pow_of_two(n)
round the given value down to nearest power of two

Parameters


n
parameter

Description

round the given value down to the nearest power of two
- the result is undefined when n == 0
- this can be used to initialise global variables from constant data


====================


order_base_2(n)
calculate the (rounded up) base 2 order of the argument

Parameters


n
parameter

Description


The first few values calculated by this routine:
ob2(0) = 0
ob2(1) = 0
ob2(2) = 1
ob2(3) = 2
ob2(4) = 2
ob2(5) = 3
...¦ and so on.


====================


bits_per(n)
calculate the number of bits required for the argument

Parameters


n
parameter

Description

This is constant-capable and can be used for compile time
initializations, e.g bitfields.

The first few values calculated by this routine:
bf(0) = 1
bf(1) = 1
bf(2) = 2
bf(3) = 2
bf(4) = 3
...¦ and so on.


====================


u64 int_pow(u64 base, unsigned int exp)
computes the exponentiation of the given base and exponent

Parameters


u64 base
base which will be raised to the given power

unsigned int exp
power to be raised to

Description

Computes: pow(base, exp), i.e. base raised to the exp power


====================


unsigned long int_sqrt(unsigned long x)
computes the integer square root

Parameters


unsigned long x
integer of which to calculate the sqrt

Description

Computes: floor(sqrt(x))


====================


u32 int_sqrt64(u64 x)
strongly typed int_sqrt function when minimum 64 bit input is expected.

Parameters


u64 x
64bit integer of which to calculate the sqrt


====================


do_div(n, base)
returns 2 values: calculate remainder and update new dividend

Parameters


n
uint64_t dividend (will be updated)

base
uint32_t divisor

Description

Summary:
uint32_t remainder = n % base;
n = n / base;

Return

(uint32_t)remainder

NOTE

macro parameter n is evaluated multiple times,
beware of side effects!


====================


u64 div_u64_rem(u64 dividend, u32 divisor, u32 *remainder)
unsigned 64bit divide with 32bit divisor with remainder

Parameters


u64 dividend
unsigned 64bit dividend

u32 divisor
unsigned 32bit divisor

u32 *remainder
pointer to unsigned 32bit remainder

Return

sets *remainder, then returns dividend / divisor

Description

This is commonly provided by 32bit archs to provide an optimized 64bit
divide.


====================


u64 div_u64(u64 dividend, u32 divisor)
unsigned 64bit divide with 32bit divisor

Parameters


u64 dividend
unsigned 64bit dividend

u32 divisor
unsigned 32bit divisor

Description

This is the most common 64bit divide and should be used if possible,
as many 32bit archs can optimize this variant better than a full 64bit
divide.


====================


s64 div_s64(s64 dividend, s32 divisor)
signed 64bit divide with 32bit divisor

Parameters


s64 dividend
signed 64bit dividend

s32 divisor
signed 32bit divisor


====================


DIV64_U64_ROUND_CLOSEST(dividend, divisor)
unsigned 64bit divide with 64bit divisor rounded to nearest integer

Parameters


dividend
unsigned 64bit dividend

divisor
unsigned 64bit divisor

Description

Divide unsigned 64bit dividend by unsigned 64bit divisor
and round to closest integer.

Return

dividend / divisor rounded to nearest integer


====================


s64 div_s64_rem(s64 dividend, s32 divisor, s32 *remainder)
signed 64bit divide with 64bit divisor and remainder

Parameters


s64 dividend
64bit dividend

s32 divisor
64bit divisor

s32 *remainder
64bit remainder


====================


u64 div64_u64_rem(u64 dividend, u64 divisor, u64 *remainder)
unsigned 64bit divide with 64bit divisor and remainder

Parameters


u64 dividend
64bit dividend

u64 divisor
64bit divisor

u64 *remainder
64bit remainder

Description

This implementation is a comparable to algorithm used by div64_u64.
But this operation, which includes math for calculating the remainder,
is kept distinct to avoid slowing down the div64_u64 operation on 32bit
systems.


====================


u64 div64_u64(u64 dividend, u64 divisor)
unsigned 64bit divide with 64bit divisor

Parameters


u64 dividend
64bit dividend

u64 divisor
64bit divisor

Description

This implementation is a modified version of the algorithm proposed
by the book 'Hacker's Delight'.  The original source and full proof
can be found here and is available for use without restriction.

'http://www.hackersdelight.org/hdcodetxt/divDouble.c.txt'


====================


s64 div64_s64(s64 dividend, s64 divisor)
signed 64bit divide with 64bit divisor

Parameters


s64 dividend
64bit dividend

s64 divisor
64bit divisor


====================


unsigned long gcd(unsigned long a, unsigned long b)
calculate and return the greatest common divisor of 2 unsigned longs

Parameters


unsigned long a
first value

unsigned long b
second value


====================


void generate_random_uuid(unsigned char uuid[16])
generate a random UUID

Parameters


unsigned char uuid[16]
where to put the generated UUID

Description

Random UUID interface

Used to create a Boot ID or a filesystem UUID/GUID, but can be
useful for other kernel drivers.


====================


bool uuid_is_valid(const char *uuid)
checks if a UUID string is valid

Parameters


const char *uuid
UUID string to check

Description


It checks if the UUID string is following the format:
xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx

where x is a hex digit.

Return

true if input is valid UUID string.


====================


int ipc_init(void)
initialise ipc subsystem

Parameters


void
no arguments

Description

The various sysv ipc resources (semaphores, messages and shared
memory) are initialised.

A callback routine is registered into the memory hotplug notifier
chain: since msgmni scales to lowmem this callback routine will be
called upon successful memory add / remove to recompute msmgni.


====================


void ipc_init_ids(struct ipc_ids *ids)
initialise ipc identifiers

Parameters


struct ipc_ids *ids
ipc identifier set

Description

Set up the sequence range to use for the ipc identifier range (limited
below ipc_mni) then initialise the keys hashtable and ids idr.


====================


void ipc_init_proc_interface(const char *path, const char *header, int ids, int (*show)(struct seq_file *, void *))
create a proc interface for sysipc types using a seq_file interface.

Parameters


const char *path
Path in procfs

const char *header
Banner to be printed at the beginning of the file.

int ids
ipc id table to iterate.

int (*show)(struct seq_file *, void *)
show routine.


====================


struct kern_ipc_perm * ipc_findkey(struct ipc_ids *ids, key_t key)
find a key in an ipc identifier set

Parameters


struct ipc_ids *ids
ipc identifier set

key_t key
key to find

Description

Returns the locked pointer to the ipc structure if found or NULL
otherwise. If key is found ipc points to the owning ipc structure

Called with writer ipc_ids.rwsem held.


====================


int ipc_addid(struct ipc_ids *ids, struct kern_ipc_perm *new, int limit)
add an ipc identifier

Parameters


struct ipc_ids *ids
ipc identifier set

struct kern_ipc_perm *new
new ipc permission set

int limit
limit for the number of used ids

Description

Add an entry 'new' to the ipc ids idr. The permissions object is
initialised and the first free entry is set up and the index assigned
is returned. The 'new' entry is returned in a locked state on success.

On failure the entry is not locked and a negative err-code is returned.
The caller must use ipc_rcu_putref() to free the identifier.

Called with writer ipc_ids.rwsem held.


====================


int ipcget_new(struct ipc_namespace *ns, struct ipc_ids *ids, const struct ipc_ops *ops, struct ipc_params *params)
create a new ipc object

Parameters


struct ipc_namespace *ns
ipc namespace

struct ipc_ids *ids
ipc identifier set

const struct ipc_ops *ops
the actual creation routine to call

struct ipc_params *params
its parameters

Description

This routine is called by sys_msgget, sys_semget() and sys_shmget()
when the key is IPC_PRIVATE.


====================


int ipc_check_perms(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp, const struct ipc_ops *ops, struct ipc_params *params)
check security and permissions for an ipc object

Parameters


struct ipc_namespace *ns
ipc namespace

struct kern_ipc_perm *ipcp
ipc permission set

const struct ipc_ops *ops
the actual security routine to call

struct ipc_params *params
its parameters

Description

This routine is called by sys_msgget(), sys_semget() and sys_shmget()
when the key is not IPC_PRIVATE and that key already exists in the
ds IDR.

On success, the ipc id is returned.

It is called with ipc_ids.rwsem and ipcp->lock held.


====================


int ipcget_public(struct ipc_namespace *ns, struct ipc_ids *ids, const struct ipc_ops *ops, struct ipc_params *params)
get an ipc object or create a new one

Parameters


struct ipc_namespace *ns
ipc namespace

struct ipc_ids *ids
ipc identifier set

const struct ipc_ops *ops
the actual creation routine to call

struct ipc_params *params
its parameters

Description

This routine is called by sys_msgget, sys_semget() and sys_shmget()
when the key is not IPC_PRIVATE.
It adds a new entry if the key is not found and does some permission
/ security checkings if the key is found.

On success, the ipc id is returned.


====================


void ipc_kht_remove(struct ipc_ids *ids, struct kern_ipc_perm *ipcp)
remove an ipc from the key hashtable

Parameters


struct ipc_ids *ids
ipc identifier set

struct kern_ipc_perm *ipcp
ipc perm structure containing the key to remove

Description

ipc_ids.rwsem (as a writer) and the spinlock for this ID are held
before this function is called, and remain locked on the exit.


====================


int ipc_search_maxidx(struct ipc_ids *ids, int limit)
search for the highest assigned index

Parameters


struct ipc_ids *ids
ipc identifier set

int limit
known upper limit for highest assigned index

Description

The function determines the highest assigned index in ids. It is intended
to be called when ids->max_idx needs to be updated.
Updating ids->max_idx is necessary when the current highest index ipc
object is deleted.
If no ipc object is allocated, then -1 is returned.

ipc_ids.rwsem needs to be held by the caller.


====================


void ipc_rmid(struct ipc_ids *ids, struct kern_ipc_perm *ipcp)
remove an ipc identifier

Parameters


struct ipc_ids *ids
ipc identifier set

struct kern_ipc_perm *ipcp
ipc perm structure containing the identifier to remove

Description

ipc_ids.rwsem (as a writer) and the spinlock for this ID are held
before this function is called, and remain locked on the exit.


====================


void ipc_set_key_private(struct ipc_ids *ids, struct kern_ipc_perm *ipcp)
switch the key of an existing ipc to IPC_PRIVATE

Parameters


struct ipc_ids *ids
ipc identifier set

struct kern_ipc_perm *ipcp
ipc perm structure containing the key to modify

Description

ipc_ids.rwsem (as a writer) and the spinlock for this ID are held
before this function is called, and remain locked on the exit.


====================


int ipcperms(struct ipc_namespace *ns, struct kern_ipc_perm *ipcp, short flag)
check ipc permissions

Parameters


struct ipc_namespace *ns
ipc namespace

struct kern_ipc_perm *ipcp
ipc permission set

short flag
desired permission set

Description

Check user, group, other permissions for access
to ipc resources. return 0 if allowed

flag will most probably be 0 or S_...UGO from <linux/stat.h>


====================


void kernel_to_ipc64_perm(struct kern_ipc_perm *in, struct ipc64_perm *out)
convert kernel ipc permissions to user

Parameters


struct kern_ipc_perm *in
kernel permissions

struct ipc64_perm *out
new style ipc permissions

Description

Turn the kernel object in into a set of permissions descriptions
for returning to userspace (out).


====================


void ipc64_perm_to_ipc_perm(struct ipc64_perm *in, struct ipc_perm *out)
convert new ipc permissions to old

Parameters


struct ipc64_perm *in
new style ipc permissions

struct ipc_perm *out
old style ipc permissions

Description

Turn the new style permissions object in into a compatibility
object and store it into the out pointer.


====================


struct kern_ipc_perm * ipc_obtain_object_idr(struct ipc_ids *ids, int id)

Parameters


struct ipc_ids *ids
ipc identifier set

int id
ipc id to look for

Description

Look for an id in the ipc ids idr and return associated ipc object.

Call inside the RCU critical section.
The ipc object is not locked on exit.


====================


struct kern_ipc_perm * ipc_obtain_object_check(struct ipc_ids *ids, int id)

Parameters


struct ipc_ids *ids
ipc identifier set

int id
ipc id to look for

Description

Similar to ipc_obtain_object_idr() but also checks the ipc object
sequence number.

Call inside the RCU critical section.
The ipc object is not locked on exit.


====================


int ipcget(struct ipc_namespace *ns, struct ipc_ids *ids, const struct ipc_ops *ops, struct ipc_params *params)
Common sys_*get() code

Parameters


struct ipc_namespace *ns
namespace

struct ipc_ids *ids
ipc identifier set

const struct ipc_ops *ops
operations to be called on ipc object creation, permission checks
and further checks

struct ipc_params *params
the parameters needed by the previous operations.

Description

Common routine called by sys_msgget(), sys_semget() and sys_shmget().


====================


int ipc_update_perm(struct ipc64_perm *in, struct kern_ipc_perm *out)
update the permissions of an ipc object

Parameters


struct ipc64_perm *in
the permission given as input.

struct kern_ipc_perm *out
the permission of the ipc to set.


====================


struct kern_ipc_perm * ipcctl_obtain_check(struct ipc_namespace *ns, struct ipc_ids *ids, int id, int cmd, struct ipc64_perm *perm, int extra_perm)
retrieve an ipc object and check permissions

Parameters


struct ipc_namespace *ns
ipc namespace

struct ipc_ids *ids
the table of ids where to look for the ipc

int id
the id of the ipc to retrieve

int cmd
the cmd to check

struct ipc64_perm *perm
the permission to set

int extra_perm
one extra permission parameter used by msq

Description

This function does some common audit and permissions check for some IPC_XXX
cmd and is called from semctl_down, shmctl_down and msgctl_down.


It:

retrieves the ipc object with the given id in the given table.
performs some audit and permission check, depending on the given cmd
returns a pointer to the ipc object or otherwise, the corresponding
error.


Call holding the both the rwsem and the rcu read lock.


====================


int ipc_parse_version(int *cmd)
ipc call version

Parameters


int *cmd
pointer to command

Description

Return IPC_64 for new style IPC and IPC_OLD for old style IPC.
The cmd value is turned from an encoding command and version into
just the command code.


====================


DECLARE_KFIFO_PTR(fifo, type)
macro to declare a fifo pointer object

Parameters


fifo
name of the declared fifo

type
type of the fifo elements


====================


DECLARE_KFIFO(fifo, type, size)
macro to declare a fifo object

Parameters


fifo
name of the declared fifo

type
type of the fifo elements

size
the number of elements in the fifo, this must be a power of 2


====================


INIT_KFIFO(fifo)
Initialize a fifo declared by DECLARE_KFIFO

Parameters


fifo
name of the declared fifo datatype


====================


DEFINE_KFIFO(fifo, type, size)
macro to define and initialize a fifo

Parameters


fifo
name of the declared fifo datatype

type
type of the fifo elements

size
the number of elements in the fifo, this must be a power of 2

Note

the macro can be used for global and local fifo data type variables.


====================


kfifo_initialized(fifo)
Check if the fifo is initialized

Parameters


fifo
address of the fifo to check

Description

Return true if fifo is initialized, otherwise false.
Assumes the fifo was 0 before.


====================


kfifo_esize(fifo)
returns the size of the element managed by the fifo

Parameters


fifo
address of the fifo to be used


====================


kfifo_recsize(fifo)
returns the size of the record length field

Parameters


fifo
address of the fifo to be used


====================


kfifo_size(fifo)
returns the size of the fifo in elements

Parameters


fifo
address of the fifo to be used


====================


kfifo_reset(fifo)
removes the entire fifo content

Parameters


fifo
address of the fifo to be used

Note

usage of kfifo_reset() is dangerous. It should be only called when the
fifo is exclusived locked or when it is secured that no other thread is
accessing the fifo.


====================


kfifo_reset_out(fifo)
skip fifo content

Parameters


fifo
address of the fifo to be used

Note

The usage of kfifo_reset_out() is safe until it will be only called
from the reader thread and there is only one concurrent reader. Otherwise
it is dangerous and must be handled in the same way as kfifo_reset().


====================


kfifo_len(fifo)
returns the number of used elements in the fifo

Parameters


fifo
address of the fifo to be used


====================


kfifo_is_empty(fifo)
returns true if the fifo is empty

Parameters


fifo
address of the fifo to be used


====================


kfifo_is_empty_spinlocked(fifo, lock)
returns true if the fifo is empty using a spinlock for locking

Parameters


fifo
address of the fifo to be used

lock
spinlock to be used for locking


====================


kfifo_is_empty_spinlocked_noirqsave(fifo, lock)
returns true if the fifo is empty using a spinlock for locking, doesn't disable interrupts

Parameters


fifo
address of the fifo to be used

lock
spinlock to be used for locking


====================


kfifo_is_full(fifo)
returns true if the fifo is full

Parameters


fifo
address of the fifo to be used


====================


kfifo_avail(fifo)
returns the number of unused elements in the fifo

Parameters


fifo
address of the fifo to be used


====================


kfifo_skip(fifo)
skip output data

Parameters


fifo
address of the fifo to be used


====================


kfifo_peek_len(fifo)
gets the size of the next fifo record

Parameters


fifo
address of the fifo to be used

Description

This function returns the size of the next fifo record in number of bytes.


====================


kfifo_alloc(fifo, size, gfp_mask)
dynamically allocates a new fifo buffer

Parameters


fifo
pointer to the fifo

size
the number of elements in the fifo, this must be a power of 2

gfp_mask
get_free_pages mask, passed to kmalloc()

Description

This macro dynamically allocates a new fifo buffer.

The number of elements will be rounded-up to a power of 2.
The fifo will be release with kfifo_free().
Return 0 if no error, otherwise an error code.


====================


kfifo_free(fifo)
frees the fifo

Parameters


fifo
the fifo to be freed


====================


kfifo_init(fifo, buffer, size)
initialize a fifo using a preallocated buffer

Parameters


fifo
the fifo to assign the buffer

buffer
the preallocated buffer to be used

size
the size of the internal buffer, this have to be a power of 2

Description

This macro initializes a fifo using a preallocated buffer.

The number of elements will be rounded-up to a power of 2.
Return 0 if no error, otherwise an error code.


====================


kfifo_put(fifo, val)
put data into the fifo

Parameters


fifo
address of the fifo to be used

val
the data to be added

Description

This macro copies the given value into the fifo.
It returns 0 if the fifo was full. Otherwise it returns the number
processed elements.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macro.


====================


kfifo_get(fifo, val)
get data from the fifo

Parameters


fifo
address of the fifo to be used

val
address where to store the data

Description

This macro reads the data from the fifo.
It returns 0 if the fifo was empty. Otherwise it returns the number
processed elements.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macro.


====================


kfifo_peek(fifo, val)
get data from the fifo without removing

Parameters


fifo
address of the fifo to be used

val
address where to store the data

Description

This reads the data from the fifo without removing it from the fifo.
It returns 0 if the fifo was empty. Otherwise it returns the number
processed elements.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macro.


====================


kfifo_in(fifo, buf, n)
put data into the fifo

Parameters


fifo
address of the fifo to be used

buf
the data to be added

n
number of elements to be added

Description

This macro copies the given buffer into the fifo and returns the
number of copied elements.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macro.


====================


kfifo_in_spinlocked(fifo, buf, n, lock)
put data into the fifo using a spinlock for locking

Parameters


fifo
address of the fifo to be used

buf
the data to be added

n
number of elements to be added

lock
pointer to the spinlock to use for locking

Description

This macro copies the given values buffer into the fifo and returns the
number of copied elements.


====================


kfifo_in_spinlocked_noirqsave(fifo, buf, n, lock)
put data into fifo using a spinlock for locking, don't disable interrupts

Parameters


fifo
address of the fifo to be used

buf
the data to be added

n
number of elements to be added

lock
pointer to the spinlock to use for locking

Description

This is a variant of kfifo_in_spinlocked() but uses spin_lock/unlock()
for locking and doesn't disable interrupts.


====================


kfifo_out(fifo, buf, n)
get data from the fifo

Parameters


fifo
address of the fifo to be used

buf
pointer to the storage buffer

n
max. number of elements to get

Description

This macro get some data from the fifo and return the numbers of elements
copied.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macro.


====================


kfifo_out_spinlocked(fifo, buf, n, lock)
get data from the fifo using a spinlock for locking

Parameters


fifo
address of the fifo to be used

buf
pointer to the storage buffer

n
max. number of elements to get

lock
pointer to the spinlock to use for locking

Description

This macro get the data from the fifo and return the numbers of elements
copied.


====================


kfifo_out_spinlocked_noirqsave(fifo, buf, n, lock)
get data from the fifo using a spinlock for locking, don't disable interrupts

Parameters


fifo
address of the fifo to be used

buf
pointer to the storage buffer

n
max. number of elements to get

lock
pointer to the spinlock to use for locking

Description

This is a variant of kfifo_out_spinlocked() which uses spin_lock/unlock()
for locking and doesn't disable interrupts.


====================


kfifo_from_user(fifo, from, len, copied)
puts some data from user space into the fifo

Parameters


fifo
address of the fifo to be used

from
pointer to the data to be added

len
the length of the data to be added

copied
pointer to output variable to store the number of copied bytes

Description

This macro copies at most len bytes from the from into the
fifo, depending of the available space and returns -EFAULT/0.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macro.


====================


kfifo_to_user(fifo, to, len, copied)
copies data from the fifo into user space

Parameters


fifo
address of the fifo to be used

to
where the data must be copied

len
the size of the destination buffer

copied
pointer to output variable to store the number of copied bytes

Description

This macro copies at most len bytes from the fifo into the
to buffer and returns -EFAULT/0.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macro.


====================


kfifo_dma_in_prepare(fifo, sgl, nents, len)
setup a scatterlist for DMA input

Parameters


fifo
address of the fifo to be used

sgl
pointer to the scatterlist array

nents
number of entries in the scatterlist array

len
number of elements to transfer

Description

This macro fills a scatterlist for DMA input.
It returns the number entries in the scatterlist array.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macros.


====================


kfifo_dma_in_finish(fifo, len)
finish a DMA IN operation

Parameters


fifo
address of the fifo to be used

len
number of bytes to received

Description

This macro finish a DMA IN operation. The in counter will be updated by
the len parameter. No error checking will be done.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macros.


====================


kfifo_dma_out_prepare(fifo, sgl, nents, len)
setup a scatterlist for DMA output

Parameters


fifo
address of the fifo to be used

sgl
pointer to the scatterlist array

nents
number of entries in the scatterlist array

len
number of elements to transfer

Description

This macro fills a scatterlist for DMA output which at most len bytes
to transfer.
It returns the number entries in the scatterlist array.
A zero means there is no space available and the scatterlist is not filled.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macros.


====================


kfifo_dma_out_finish(fifo, len)
finish a DMA OUT operation

Parameters


fifo
address of the fifo to be used

len
number of bytes transferred

Description

This macro finish a DMA OUT operation. The out counter will be updated by
the len parameter. No error checking will be done.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macros.


====================


kfifo_out_peek(fifo, buf, n)
gets some data from the fifo

Parameters


fifo
address of the fifo to be used

buf
pointer to the storage buffer

n
max. number of elements to get

Description

This macro get the data from the fifo and return the numbers of elements
copied. The data is not removed from the fifo.

Note that with only one concurrent reader and one concurrent
writer, you don't need extra locking to use these macro.


====================


int relay_buf_full(struct rchan_buf *buf)
boolean, is the channel buffer full?

Parameters


struct rchan_buf *bufchannel buffer
Returns 1 if the buffer is full, 0 otherwise.



====================


void relay_reset(struct rchan *chan)
reset the channel

Parameters


struct rchan *chanthe channel
This has the effect of erasing all data from all channel buffers
and restarting the channel in its initial state.  The buffers
are not freed, so any mappings are still in effect.
NOTE. Care should be taken that the channel isn't actually
being used by anything when this call is made.



====================


struct rchan * relay_open(const char *base_filename, struct dentry *parent, size_t subbuf_size, size_t n_subbufs, const struct rchan_callbacks *cb, void *private_data)
create a new relay channel

Parameters


const char *base_filenamebase name of files to create, NULL for buffering only

struct dentry *parentdentry of parent directory, NULL for root directory or buffer

size_t subbuf_sizesize of sub-buffers

size_t n_subbufsnumber of sub-buffers

const struct rchan_callbacks *cbclient callback functions

void *private_datauser-defined data
Returns channel pointer if successful, NULL otherwise.
Creates a channel buffer for each cpu using the sizes and
attributes specified.  The created channel buffer files
will be named base_filename0...¦base_filenameN-1.  File
permissions will be S_IRUSR.
If opening a buffer (parent = NULL) that you later wish to register
in a filesystem, call relay_late_setup_files() once the parent dentry
is available.



====================


int relay_late_setup_files(struct rchan *chan, const char *base_filename, struct dentry *parent)
triggers file creation

Parameters


struct rchan *chanchannel to operate on

const char *base_filenamebase name of files to create

struct dentry *parentdentry of parent directory, NULL for root directory
Returns 0 if successful, non-zero otherwise.
Use to setup files for a previously buffer-only channel created
by relay_open() with a NULL parent dentry.
For example, this is useful for perfomring early tracing in kernel,
before VFS is up and then exposing the early results once the dentry
is available.



====================


size_t relay_switch_subbuf(struct rchan_buf *buf, size_t length)
switch to a new sub-buffer

Parameters


struct rchan_buf *bufchannel buffer

size_t lengthsize of current event
Returns either the length passed in or 0 if full.
Performs sub-buffer-switch tasks such as invoking callbacks,
updating padding counts, waking up readers, etc.



====================


void relay_subbufs_consumed(struct rchan *chan, unsigned int cpu, size_t subbufs_consumed)
update the buffer's sub-buffers-consumed count

Parameters


struct rchan *chanthe channel

unsigned int cputhe cpu associated with the channel buffer to update

size_t subbufs_consumednumber of sub-buffers to add to current buf's count
Adds to the channel buffer's consumed sub-buffer count.
subbufs_consumed should be the number of sub-buffers newly consumed,
not the total consumed.
NOTE. Kernel clients don't need to call this function if the channel
mode is 'overwrite'.



====================


void relay_close(struct rchan *chan)
close the channel

Parameters


struct rchan *chanthe channel
Closes all channel buffers and frees the channel.



====================


void relay_flush(struct rchan *chan)
close the channel

Parameters


struct rchan *chanthe channel
Flushes all channel buffers, i.e. forces buffer switch.



====================


int relay_mmap_buf(struct rchan_buf *buf, struct vm_area_struct *vma)

mmap channel buffer to process address space


Parameters


struct rchan_buf *bufrelay channel buffer

struct vm_area_struct *vmavm_area_struct describing memory to be mapped
Returns 0 if ok, negative on error
Caller should already have grabbed mmap_lock.



====================


void * relay_alloc_buf(struct rchan_buf *buf, size_t *size)
allocate a channel buffer

Parameters


struct rchan_buf *bufthe buffer struct

size_t *sizetotal size of the buffer
Returns a pointer to the resulting buffer, NULL if unsuccessful. The
passed in size will get page aligned, if it isn't already.



====================


struct rchan_buf * relay_create_buf(struct rchan *chan)
allocate and initialize a channel buffer

Parameters


struct rchan *chanthe relay channel
Returns channel buffer if successful, NULL otherwise.



====================


void relay_destroy_channel(struct kref *kref)
free the channel struct

Parameters


struct kref *kreftarget kernel reference that contains the relay channel
Should only be called from kref_put().



====================


void relay_destroy_buf(struct rchan_buf *buf)
destroy an rchan_buf struct and associated buffer

Parameters


struct rchan_buf *buf
the buffer struct


====================


void relay_remove_buf(struct kref *kref)
remove a channel buffer

Parameters


struct kref *kreftarget kernel reference that contains the relay buffer
Removes the file from the filesystem, which also frees the
rchan_buf_struct and the channel buffer.  Should only be called from
kref_put().



====================


int relay_buf_empty(struct rchan_buf *buf)
boolean, is the channel buffer empty?

Parameters


struct rchan_buf *bufchannel buffer
Returns 1 if the buffer is empty, 0 otherwise.



====================


void wakeup_readers(struct irq_work *work)
wake up readers waiting on a channel

Parameters


struct irq_work *workcontains the channel buffer
This is the function used to defer reader waking



====================


void __relay_reset(struct rchan_buf *buf, unsigned int init)
reset a channel buffer

Parameters


struct rchan_buf *bufthe channel buffer

unsigned int init1 if this is a first-time initialization
See relay_reset() for description of effect.



====================


void relay_close_buf(struct rchan_buf *buf)
close a channel buffer

Parameters


struct rchan_buf *bufchannel buffer
Marks the buffer finalized and restores the default callbacks.
The channel buffer and channel buffer data structure are then freed
automatically when the last reference is given up.



====================


int relay_file_open(struct inode *inode, struct file *filp)
open file op for relay files

Parameters


struct inode *inodethe inode

struct file *filpthe file
Increments the channel buffer refcount.



====================


int relay_file_mmap(struct file *filp, struct vm_area_struct *vma)
mmap file op for relay files

Parameters


struct file *filpthe file

struct vm_area_struct *vmathe vma describing what to map
Calls upon relay_mmap_buf() to map the file into user space.



====================


__poll_t relay_file_poll(struct file *filp, poll_table *wait)
poll file op for relay files

Parameters


struct file *filpthe file

poll_table *waitpoll table
Poll implemention.



====================


int relay_file_release(struct inode *inode, struct file *filp)
release file op for relay files

Parameters


struct inode *inodethe inode

struct file *filpthe file
Decrements the channel refcount, as the filesystem is
no longer using it.



====================


size_t relay_file_read_subbuf_avail(size_t read_pos, struct rchan_buf *buf)
return bytes available in sub-buffer

Parameters


size_t read_pos
file read position

struct rchan_buf *buf
relay channel buffer


====================


size_t relay_file_read_start_pos(struct rchan_buf *buf)
find the first available byte to read

Parameters


struct rchan_buf *bufrelay channel buffer
If the read_pos is in the middle of padding, return the
position of the first actually available byte, otherwise
return the original value.



====================


size_t relay_file_read_end_pos(struct rchan_buf *buf, size_t read_pos, size_t count)
return the new read position

Parameters


struct rchan_buf *buf
relay channel buffer

size_t read_pos
file read position

size_t count
number of bytes to be read


====================


int __request_module(bool wait, const char *fmt, ...)
try to load a kernel module

Parameters


bool wait
wait (or not) for the operation to complete

const char *fmt
printf style format string for the name of the module

...
arguments as specified in the format string

Description

Load a module using the user mode module loader. The function returns
zero on success or a negative errno code or positive exit code from
'modprobe' on failure. Note that a successful module load does not mean
the module did not then unload and exit on an error of its own. Callers
must check that the service they requested is now available not blindly
invoke it.

If module auto-loading support is disabled then this function
simply returns -ENOENT.

Refer to the file kernel/module.c for more information.


====================


int request_dma(unsigned int dmanr, const char * device_id)
request and reserve a system DMA channel

Parameters


unsigned int dmanr
DMA channel number

const char * device_id
reserving device ID string, used in /proc/dma


====================


void free_dma(unsigned int dmanr)
free a reserved system DMA channel

Parameters


unsigned int dmanr
DMA channel number


====================


struct resource * request_resource_conflict(struct resource *root, struct resource *new)
request and reserve an I/O or memory resource

Parameters


struct resource *root
root resource descriptor

struct resource *new
resource descriptor desired by caller

Description

Returns 0 for success, conflict resource on error.


====================


int find_next_iomem_res(resource_size_t start, resource_size_t end, unsigned long flags, unsigned long desc, struct resource *res)
Finds the lowest iomem resource that covers part of [start..**end**].

Parameters


resource_size_t start
start address of the resource searched for

resource_size_t end
end address of same resource

unsigned long flags
flags which the resource must have

unsigned long desc
descriptor the resource must have

struct resource *res
return ptr, if resource found

Description

If a resource is found, returns 0 and ***res is overwritten with the part
of the resource that's within [**start..**end**]; if none is found, returns
-ENODEV.  Returns -EINVAL for invalid parameters.

The caller must specify start, end, flags, and desc
(which may be IORES_DESC_NONE).


====================


int reallocate_resource(struct resource *root, struct resource *old, resource_size_t newsize, struct resource_constraint *constraint)
allocate a slot in the resource tree given range & alignment. The resource will be relocated if the new size cannot be reallocated in the current location.

Parameters


struct resource *root
root resource descriptor

struct resource *old
resource descriptor desired by caller

resource_size_t newsize
new size of the resource descriptor

struct resource_constraint *constraint
the size and alignment constraints to be met.


====================


struct resource * lookup_resource(struct resource *root, resource_size_t start)
find an existing resource by a resource start address

Parameters


struct resource *root
root resource descriptor

resource_size_t start
resource start address

Description

Returns a pointer to the resource if found, NULL otherwise


====================


struct resource * insert_resource_conflict(struct resource *parent, struct resource *new)
Inserts resource in the resource tree

Parameters


struct resource *parent
parent of the new resource

struct resource *new
new resource to insert

Description

Returns 0 on success, conflict resource if the resource can't be inserted.

This function is equivalent to request_resource_conflict when no conflict
happens. If a conflict happens, and the conflicting resources
entirely fit within the range of the new resource, then the new
resource is inserted and the conflicting resources become children of
the new resource.

This function is intended for producers of resources, such as FW modules
and bus drivers.


====================


void insert_resource_expand_to_fit(struct resource *root, struct resource *new)
Insert a resource into the resource tree

Parameters


struct resource *root
root resource descriptor

struct resource *new
new resource to insert

Description

Insert a resource into the resource tree, possibly expanding it in order
to make it encompass any conflicting resources.


====================


resource_size_t resource_alignment(struct resource *res)
calculate resource's alignment

Parameters


struct resource *res
resource pointer

Description

Returns alignment on success, 0 (invalid alignment) on failure.


====================


void release_mem_region_adjustable(resource_size_t start, resource_size_t size)
release a previously reserved memory region

Parameters


resource_size_t start
resource start address

resource_size_t size
resource region size

Description

This interface is intended for memory hot-delete.  The requested region
is released from a currently busy memory resource.  The requested region
must either match exactly or fit into a single busy resource entry.  In
the latter case, the remaining resource is adjusted accordingly.
Existing children of the busy memory resource must be immutable in the
request.

Note


Additional release conditions, such as overlapping region, can be
supported after they are confirmed as valid cases.
When a busy memory resource gets split into two entries, the code
assumes that all children remain in the lower address entry for
simplicity.  Enhance this logic when necessary.

====================


void merge_system_ram_resource(struct resource *res)
mark the System RAM resource mergeable and try to merge it with adjacent, mergeable resources

Parameters


struct resource *res
resource descriptor

Description

This interface is intended for memory hotplug, whereby lots of contiguous
system ram resources are added (e.g., via add_memory*()) by a driver, and
the actual resource boundaries are not of interest (e.g., it might be
relevant for DIMMs). Only resources that are marked mergeable, that have the
same parent, and that don't have any children are considered. All mergeable
resources must be immutable during the request.

Note


The caller has to make sure that no pointers to resources that are
marked mergeable are used anymore after this call - the resource might
be freed and the pointer might be stale!
release_mem_region_adjustable() will split on demand on memory hotunplug

====================


int request_resource(struct resource *root, struct resource *new)
request and reserve an I/O or memory resource

Parameters


struct resource *root
root resource descriptor

struct resource *new
resource descriptor desired by caller

Description

Returns 0 for success, negative error code on error.


====================


int release_resource(struct resource *old)
release a previously reserved resource

Parameters


struct resource *old
resource pointer


====================


int walk_iomem_res_desc(unsigned long desc, unsigned long flags, u64 start, u64 end, void *arg, int (*func)(struct resource *, void *))
Walks through iomem resources and calls func() with matching resource ranges. *

Parameters


unsigned long desc
I/O resource descriptor. Use IORES_DESC_NONE to skip desc check.

unsigned long flags
I/O resource flags

u64 start
start addr

u64 end
end addr

void *arg
function argument for the callback func

int (*func)(struct resource *, void *)
callback function that is called for each qualifying resource area

Description

All the memory ranges which overlap start,end and also match flags and
desc are valid candidates.

NOTE

For a new descriptor search, define a new IORES_DESC in
<linux/ioport.h> and set it in 'desc' of a target resource entry.


====================


int region_intersects(resource_size_t start, size_t size, unsigned long flags, unsigned long desc)
determine intersection of region with known resources

Parameters


resource_size_t start
region start address

size_t size
size of region

unsigned long flags
flags of resource (in iomem_resource)

unsigned long desc
descriptor of resource (in iomem_resource) or IORES_DESC_NONE

Description

Check if the specified region partially overlaps or fully eclipses a
resource identified by flags and desc (optional with IORES_DESC_NONE).
Return REGION_DISJOINT if the region does not overlap flags/desc,
return REGION_MIXED if the region overlaps flags/desc and another
resource, and return REGION_INTERSECTS if the region overlaps flags/desc
and no other defined resource. Note that REGION_INTERSECTS is also
returned in the case when the specified region overlaps RAM and undefined
memory holes.

region_intersect() is used by memory remapping functions to ensure
the user is not remapping RAM and is a vast speed up over walking
through the resource table page by page.


====================


int allocate_resource(struct resource *root, struct resource *new, resource_size_t size, resource_size_t min, resource_size_t max, resource_size_t align, resource_size_t (*alignf)(void *, const struct resource *, resource_size_t, resource_size_t), void *alignf_data)
allocate empty slot in the resource tree given range & alignment. The resource will be reallocated with a new size if it was already allocated

Parameters


struct resource *root
root resource descriptor

struct resource *new
resource descriptor desired by caller

resource_size_t size
requested resource region size

resource_size_t min
minimum boundary to allocate

resource_size_t max
maximum boundary to allocate

resource_size_t align
alignment requested, in bytes

resource_size_t (*alignf)(void *, const struct resource *, resource_size_t, resource_size_t)
alignment function, optional, called if not NULL

void *alignf_data
arbitrary data to pass to the alignf function


====================


int insert_resource(struct resource *parent, struct resource *new)
Inserts a resource in the resource tree

Parameters


struct resource *parent
parent of the new resource

struct resource *new
new resource to insert

Description

Returns 0 on success, -EBUSY if the resource can't be inserted.

This function is intended for producers of resources, such as FW modules
and bus drivers.


====================


int remove_resource(struct resource *old)
Remove a resource in the resource tree

Parameters


struct resource *old
resource to remove

Description

Returns 0 on success, -EINVAL if the resource is not valid.

This function removes a resource previously inserted by insert_resource()
or insert_resource_conflict(), and moves the children (if any) up to
where they were before.  insert_resource() and insert_resource_conflict()
insert a new resource, and move any conflicting resources down to the
children of the new resource.

insert_resource(), insert_resource_conflict() and remove_resource() are
intended for producers of resources, such as FW modules and bus drivers.


====================


int adjust_resource(struct resource *res, resource_size_t start, resource_size_t size)
modify a resource's start and size

Parameters


struct resource *res
resource to modify

resource_size_t start
new start value

resource_size_t size
new size

Description

Given an existing resource, change its start and size to match the
arguments.  Returns 0 on success, -EBUSY if it can't fit.
Existing children of the resource are assumed to be immutable.


====================


struct resource * __request_region(struct resource *parent, resource_size_t start, resource_size_t n, const char *name, int flags)
create a new busy resource region

Parameters


struct resource *parent
parent resource descriptor

resource_size_t start
resource start address

resource_size_t n
resource region size

const char *name
reserving caller's ID string

int flags
IO resource flags


====================


void __release_region(struct resource *parent, resource_size_t start, resource_size_t n)
release a previously reserved resource region

Parameters


struct resource *parent
parent resource descriptor

resource_size_t start
resource start address

resource_size_t n
resource region size

Description

The described resource region must match a currently busy region.


====================


int devm_request_resource(struct device *dev, struct resource *root, struct resource *new)
request and reserve an I/O or memory resource

Parameters


struct device *dev
device for which to request the resource

struct resource *root
root of the resource tree from which to request the resource

struct resource *new
descriptor of the resource to request

Description

This is a device-managed version of request_resource(). There is usually
no need to release resources requested by this function explicitly since
that will be taken care of when the device is unbound from its driver.
If for some reason the resource needs to be released explicitly, because
of ordering issues for example, drivers must call devm_release_resource()
rather than the regular release_resource().

When a conflict is detected between any existing resources and the newly
requested resource, an error message will be printed.

Returns 0 on success or a negative error code on failure.


====================


void devm_release_resource(struct device *dev, struct resource *new)
release a previously requested resource

Parameters


struct device *dev
device for which to release the resource

struct resource *new
descriptor of the resource to release

Description

Releases a resource previously requested using devm_request_resource().


====================


struct resource * devm_request_free_mem_region(struct device *dev, struct resource *base, unsigned long size)
find free region for device private memory

Parameters


struct device *dev
device struct to bind the resource to

struct resource *base
resource tree to look in

unsigned long size
size in bytes of the device memory to add

Description

This function tries to find an empty range of physical address big enough to
contain the new resource, so that it can later be hotplugged as ZONE_DEVICE
memory, which in turn allocates struct pages.


====================


int arch_phys_wc_add(unsigned long base, unsigned long size)
add a WC MTRR and handle errors if PAT is unavailable

Parameters


unsigned long base
Physical base address

unsigned long size
Size of region

Description

If PAT is available, this does nothing.  If PAT is unavailable, it
attempts to add a WC MTRR covering size bytes starting at base and
logs an error if this fails.

The called should provide a power of two size on an equivalent
power of two boundary.

Drivers must store the return value to pass to mtrr_del_wc_if_needed,
but drivers should not try to interpret that return value.


====================


int security_init(void)
initializes the security framework

Parameters


void
no arguments

Description

This should be called early in the kernel initialization sequence.


====================


void security_add_hooks(struct security_hook_list *hooks, int count, char *lsm)
Add a modules hooks to the hook lists.

Parameters


struct security_hook_list *hooks
the hooks to add

int count
the number of hooks to add

char *lsm
the name of the security module

Description

Each LSM has to register its hooks with the infrastructure.


====================


int lsm_cred_alloc(struct cred *cred, gfp_t gfp)
allocate a composite cred blob

Parameters


struct cred *cred
the cred that needs a blob

gfp_t gfp
allocation type

Description

Allocate the cred blob for all the modules

Returns 0, or -ENOMEM if memory can't be allocated.


====================


void lsm_early_cred(struct cred *cred)
during initialization allocate a composite cred blob

Parameters


struct cred *cred
the cred that needs a blob

Description

Allocate the cred blob for all the modules


====================


int lsm_file_alloc(struct file *file)
allocate a composite file blob

Parameters


struct file *file
the file that needs a blob

Description

Allocate the file blob for all the modules

Returns 0, or -ENOMEM if memory can't be allocated.


====================


int lsm_inode_alloc(struct inode *inode)
allocate a composite inode blob

Parameters


struct inode *inode
the inode that needs a blob

Description

Allocate the inode blob for all the modules

Returns 0, or -ENOMEM if memory can't be allocated.


====================


int lsm_task_alloc(struct task_struct *task)
allocate a composite task blob

Parameters


struct task_struct *task
the task that needs a blob

Description

Allocate the task blob for all the modules

Returns 0, or -ENOMEM if memory can't be allocated.


====================


int lsm_ipc_alloc(struct kern_ipc_perm *kip)
allocate a composite ipc blob

Parameters


struct kern_ipc_perm *kip
the ipc that needs a blob

Description

Allocate the ipc blob for all the modules

Returns 0, or -ENOMEM if memory can't be allocated.


====================


int lsm_msg_msg_alloc(struct msg_msg *mp)
allocate a composite msg_msg blob

Parameters


struct msg_msg *mp
the msg_msg that needs a blob

Description

Allocate the ipc blob for all the modules

Returns 0, or -ENOMEM if memory can't be allocated.


====================


void lsm_early_task(struct task_struct *task)
during initialization allocate a composite task blob

Parameters


struct task_struct *task
the task that needs a blob

Description

Allocate the task blob for all the modules


====================


int lsm_superblock_alloc(struct super_block *sb)
allocate a composite superblock blob

Parameters


struct super_block *sb
the superblock that needs a blob

Description

Allocate the superblock blob for all the modules

Returns 0, or -ENOMEM if memory can't be allocated.


====================


struct dentry * securityfs_create_file(const char *name, umode_t mode, struct dentry *parent, void *data, const struct file_operations *fops)
create a file in the securityfs filesystem

Parameters


const char *name
a pointer to a string containing the name of the file to create.

umode_t mode
the permission that the file should have

struct dentry *parent
a pointer to the parent dentry for this file.  This should be a
directory dentry if set.  If this parameter is NULL, then the
file will be created in the root of the securityfs filesystem.

void *data
a pointer to something that the caller will want to get to later
on.  The inode.i_private pointer will point to this value on
the open() call.

const struct file_operations *fops
a pointer to a struct file_operations that should be used for
this file.

Description

This function creates a file in securityfs with the given name.

This function returns a pointer to a dentry if it succeeds.  This
pointer must be passed to the securityfs_remove() function when the file is
to be removed (no automatic cleanup happens if your module is unloaded,
you are responsible here).  If an error occurs, the function will return
the error value (via ERR_PTR).

If securityfs is not enabled in the kernel, the value -ENODEV is
returned.


====================


struct dentry * securityfs_create_dir(const char *name, struct dentry *parent)
create a directory in the securityfs filesystem

Parameters


const char *name
a pointer to a string containing the name of the directory to
create.

struct dentry *parent
a pointer to the parent dentry for this file.  This should be a
directory dentry if set.  If this parameter is NULL, then the
directory will be created in the root of the securityfs filesystem.

Description

This function creates a directory in securityfs with the given name.

This function returns a pointer to a dentry if it succeeds.  This
pointer must be passed to the securityfs_remove() function when the file is
to be removed (no automatic cleanup happens if your module is unloaded,
you are responsible here).  If an error occurs, the function will return
the error value (via ERR_PTR).

If securityfs is not enabled in the kernel, the value -ENODEV is
returned.


====================


struct dentry * securityfs_create_symlink(const char *name, struct dentry *parent, const char *target, const struct inode_operations *iops)
create a symlink in the securityfs filesystem

Parameters


const char *name
a pointer to a string containing the name of the symlink to
create.

struct dentry *parent
a pointer to the parent dentry for the symlink.  This should be a
directory dentry if set.  If this parameter is NULL, then the
directory will be created in the root of the securityfs filesystem.

const char *target
a pointer to a string containing the name of the symlink's target.
If this parameter is NULL, then the iops parameter needs to be
setup to handle .readlink and .get_link inode_operations.

const struct inode_operations *iops
a pointer to the struct inode_operations to use for the symlink. If
this parameter is NULL, then the default simple_symlink_inode
operations will be used.

Description

This function creates a symlink in securityfs with the given name.

This function returns a pointer to a dentry if it succeeds.  This
pointer must be passed to the securityfs_remove() function when the file is
to be removed (no automatic cleanup happens if your module is unloaded,
you are responsible here).  If an error occurs, the function will return
the error value (via ERR_PTR).

If securityfs is not enabled in the kernel, the value -ENODEV is
returned.


====================


void securityfs_remove(struct dentry *dentry)
removes a file or directory from the securityfs filesystem

Parameters


struct dentry *dentry
a pointer to a the dentry of the file or directory to be removed.

Description

This function removes a file or directory in securityfs that was previously
created with a call to another securityfs function (like
securityfs_create_file() or variants thereof.)

This function is required to be called in order for the file to be
removed. No automatic cleanup of files will happen when a module is
removed; you are responsible here.


====================


struct audit_buffer * audit_log_start(struct audit_context *ctx, gfp_t gfp_mask, int type)
obtain an audit buffer

Parameters


struct audit_context *ctx
audit_context (may be NULL)

gfp_t gfp_mask
type of allocation

int type
audit message type

Description

Returns audit_buffer pointer on success or NULL on error.

Obtain an audit buffer.  This routine does locking to obtain the
audit buffer, but then no locking is required for calls to
audit_log_*format.  If the task (ctx) is a task that is currently in a
syscall, then the syscall is marked as auditable and an audit record
will be written at syscall exit.  If there is no associated task, then
task context (ctx) should be NULL.


====================


void audit_log_format(struct audit_buffer *ab, const char *fmt, ...)
format a message into the audit buffer.

Parameters


struct audit_buffer *ab
audit_buffer

const char *fmt
format string

...
optional parameters matching fmt string

Description

All the work is done in audit_log_vformat.


====================


void audit_log_end(struct audit_buffer *ab)
end one audit record

Parameters


struct audit_buffer *ab
the audit_buffer

Description

We can not do a netlink send inside an irq context because it blocks (last
arg, flags, is not set to MSG_DONTWAIT), so the audit buffer is placed on a
queue and a kthread is scheduled to remove them from the queue outside the
irq context.  May be called in any context.


====================


void audit_log(struct audit_context *ctx, gfp_t gfp_mask, int type, const char *fmt, ...)
Log an audit record

Parameters


struct audit_context *ctx
audit context

gfp_t gfp_mask
type of allocation

int type
audit message type

const char *fmt
format string to use

...
variable parameters matching the format string

Description

This is a convenience function that calls audit_log_start,
audit_log_vformat, and audit_log_end.  It may be called
in any context.


====================


void audit_filter_uring(struct task_struct *tsk, struct audit_context *ctx)
apply filters to an io_uring operation

Parameters


struct task_struct *tsk
associated task

struct audit_context *ctx
audit context


====================


void audit_reset_context(struct audit_context *ctx)
reset a audit_context structure

Parameters


struct audit_context *ctx
the audit_context to reset

Description

All fields in the audit_context will be reset to an initial state, all
references held by fields will be dropped, and private memory will be
released.  When this function returns the audit_context will be suitable
for reuse, so long as the passed context is not NULL or a dummy context.


====================


int audit_alloc(struct task_struct *tsk)
allocate an audit context block for a task

Parameters


struct task_struct *tsk
task

Description

Filter on the task information and allocate a per-task audit context
if necessary.  Doing so turns on system call auditing for the
specified task.  This is called from copy_process, so no lock is
needed.


====================


int audit_alloc_kernel(struct task_struct *tsk)
allocate an audit_context for a kernel task

Parameters


struct task_struct *tsk
the kernel task

Description

Similar to the audit_alloc() function, but intended for kernel private
threads.  Returns zero on success, negative values on failure.


====================


void audit_log_uring(struct audit_context *ctx)
generate a AUDIT_URINGOP record

Parameters


struct audit_context *ctx
the audit context


====================


void __audit_free(struct task_struct *tsk)
free a per-task audit context

Parameters


struct task_struct *tsk
task whose audit context block to free

Description

Called from copy_process, do_exit, and the io_uring code


====================


void audit_return_fixup(struct audit_context *ctx, int success, long code)
fixup the return codes in the audit_context

Parameters


struct audit_context *ctx
the audit_context

int success
true/false value to indicate if the operation succeeded or not

long code
operation return code

Description

We need to fixup the return code in the audit logs if the actual return
codes are later going to be fixed by the arch specific signal handlers.


====================


void __audit_uring_entry(u8 op)
prepare the kernel task's audit context for io_uring

Parameters


u8 op
the io_uring opcode

Description

This is similar to audit_syscall_entry() but is intended for use by io_uring
operations.  This function should only ever be called from
audit_uring_entry() as we rely on the audit context checking present in that
function.


====================


void __audit_uring_exit(int success, long code)
wrap up the kernel task's audit context after io_uring

Parameters


int success
true/false value to indicate if the operation succeeded or not

long code
operation return code

Description

This is similar to audit_syscall_exit() but is intended for use by io_uring
operations.  This function should only ever be called from
audit_uring_exit() as we rely on the audit context checking present in that
function.


====================


void __audit_syscall_entry(int major, unsigned long a1, unsigned long a2, unsigned long a3, unsigned long a4)
fill in an audit record at syscall entry

Parameters


int major
major syscall type (function)

unsigned long a1
additional syscall register 1

unsigned long a2
additional syscall register 2

unsigned long a3
additional syscall register 3

unsigned long a4
additional syscall register 4

Description

Fill in audit context at syscall entry.  This only happens if the
audit context was created when the task was created and the state or
filters demand the audit context be built.  If the state from the
per-task filter or from the per-syscall filter is AUDIT_STATE_RECORD,
then the record will be written at syscall exit time (otherwise, it
will only be written if another part of the kernel requests that it
be written).


====================


void __audit_syscall_exit(int success, long return_code)
deallocate audit context after a system call

Parameters


int success
success value of the syscall

long return_code
return value of the syscall

Description

Tear down after system call.  If the audit context has been marked as
auditable (either because of the AUDIT_STATE_RECORD state from
filtering, or because some other part of the kernel wrote an audit
message), then write out the syscall information.  In call cases,
free the names stored from getname().


====================


struct filename * __audit_reusename(const __user char *uptr)
fill out filename with info from existing entry

Parameters


const __user char *uptr
userland ptr to pathname

Description

Search the audit_names list for the current audit context. If there is an
existing entry with a matching 'uptr' then return the filename
associated with that audit_name. If not, return NULL.


====================


void __audit_getname(struct filename *name)
add a name to the list

Parameters


struct filename *name
name to add

Description

Add a name to the list of audit names for this context.
Called from fs/namei.c:getname().


====================


void __audit_inode(struct filename *name, const struct dentry *dentry, unsigned int flags)
store the inode and device from a lookup

Parameters


struct filename *name
name being audited

const struct dentry *dentry
dentry being audited

unsigned int flags
attributes for this particular entry


====================


int auditsc_get_stamp(struct audit_context *ctx, struct timespec64 *t, unsigned int *serial)
get local copies of audit_context values

Parameters


struct audit_context *ctx
audit_context for the task

struct timespec64 *t
timespec64 to store time recorded in the audit_context

unsigned int *serial
serial value that is recorded in the audit_context

Description

Also sets the context as auditable.


====================


void __audit_mq_open(int oflag, umode_t mode, struct mq_attr *attr)
record audit data for a POSIX MQ open

Parameters


int oflag
open flag

umode_t mode
mode bits

struct mq_attr *attr
queue attributes


====================


void __audit_mq_sendrecv(mqd_t mqdes, size_t msg_len, unsigned int msg_prio, const struct timespec64 *abs_timeout)
record audit data for a POSIX MQ timed send/receive

Parameters


mqd_t mqdes
MQ descriptor

size_t msg_len
Message length

unsigned int msg_prio
Message priority

const struct timespec64 *abs_timeout
Message timeout in absolute time


====================


void __audit_mq_notify(mqd_t mqdes, const struct sigevent *notification)
record audit data for a POSIX MQ notify

Parameters


mqd_t mqdes
MQ descriptor

const struct sigevent *notification
Notification event


====================


void __audit_mq_getsetattr(mqd_t mqdes, struct mq_attr *mqstat)
record audit data for a POSIX MQ get/set attribute

Parameters


mqd_t mqdes
MQ descriptor

struct mq_attr *mqstat
MQ flags


====================


void __audit_ipc_obj(struct kern_ipc_perm *ipcp)
record audit data for ipc object

Parameters


struct kern_ipc_perm *ipcp
ipc permissions


====================


void __audit_ipc_set_perm(unsigned long qbytes, uid_t uid, gid_t gid, umode_t mode)
record audit data for new ipc permissions

Parameters


unsigned long qbytes
msgq bytes

uid_t uid
msgq user id

gid_t gid
msgq group id

umode_t mode
msgq mode (permissions)

Description

Called only after audit_ipc_obj().


====================


int __audit_socketcall(int nargs, unsigned long *args)
record audit data for sys_socketcall

Parameters


int nargs
number of args, which should not be more than AUDITSC_ARGS.

unsigned long *args
args array


====================


void __audit_fd_pair(int fd1, int fd2)
record audit data for pipe and socketpair

Parameters


int fd1
the first file descriptor

int fd2
the second file descriptor


====================


int __audit_sockaddr(int len, void *a)
record audit data for sys_bind, sys_connect, sys_sendto

Parameters


int len
data length in user space

void *a
data address in kernel space

Description

Returns 0 for success or NULL context or < 0 on error.


====================


int audit_signal_info_syscall(struct task_struct *t)
record signal info for syscalls

Parameters


struct task_struct *t
task being signaled

Description

If the audit subsystem is being terminated, record the task (pid)
and uid that is doing that.


====================


int __audit_log_bprm_fcaps(struct linux_binprm *bprm, const struct cred *new, const struct cred *old)
store information about a loading bprm and relevant fcaps

Parameters


struct linux_binprm *bprm
pointer to the bprm being processed

const struct cred *new
the proposed new credentials

const struct cred *old
the old credentials

Description

Simply check if the proc already has the caps given by the file and if not
store the priv escalation info for later auditing at the end of the syscall

-Eric


====================


void __audit_log_capset(const struct cred *new, const struct cred *old)
store information about the arguments to the capset syscall

Parameters


const struct cred *new
the new credentials

const struct cred *old
the old (current) credentials

Description

Record the arguments userspace sent to sys_capset for later printing by the
audit system if applicable


====================


void audit_core_dumps(long signr)
record information about processes that end abnormally

Parameters


long signr
signal value

Description

If a process ends with a core dump, something fishy is going on and we
should record the event for investigation.


====================


void audit_seccomp(unsigned long syscall, long signr, int code)
record information about a seccomp action

Parameters


unsigned long syscall
syscall number

long signr
signal value

int code
the seccomp action

Description

Record the information associated with a seccomp action. Event filtering for
seccomp actions that are not to be logged is done in seccomp_log().
Therefore, this function forces auditing independent of the audit_enabled
and dummy context state because seccomp actions should be logged even when
audit is not in use.


====================


int audit_rule_change(int type, int seq, void *data, size_t datasz)
apply all rules to the specified message type

Parameters


int type
audit message type

int seq
netlink audit message sequence (serial) number

void *data
payload data

size_t datasz
size of payload data


====================


int audit_list_rules_send(struct sk_buff *request_skb, int seq)
list the audit rules

Parameters


struct sk_buff *request_skb
skb of request we are replying to (used to target the reply)

int seq
netlink audit message sequence (serial) number


====================


int parent_len(const char *path)
find the length of the parent portion of a pathname

Parameters


const char *path
pathname of which to determine length


====================


int audit_compare_dname_path(const struct qstr *dname, const char *path, int parentlen)
compare given dentry name with last component in given path. Return of 0 indicates a match.

Parameters


const struct qstr *dname
dentry name that we're comparing

const char *path
full pathname that we're comparing

int parentlen
length of the parent if known. Passing in AUDIT_NAME_FULL
here indicates that we must compute this value.


====================


long sys_acct(const char __user * name)
enable/disable process accounting

Parameters


const char __user * name
file name for accounting records or NULL to shutdown accounting

Description

sys_acct() is the only system call needed to implement process
accounting. It takes the name of the file where accounting records
should be written. If the filename is NULL, accounting will be
shutdown.

Return

0 for success or negative errno values for failure.


====================


void acct_collect(long exitcode, int group_dead)
collect accounting information into pacct_struct

Parameters


long exitcode
task exit code

int group_dead
not 0, if this thread is the last one in the process.


====================


void acct_process(void)
handles process accounting for an exiting task

Parameters


void
no arguments


====================


void bio_advance(struct bio *bio, unsigned int nbytes)
increment/complete a bio by some number of bytes

Parameters


struct bio *bio
bio to advance

unsigned int nbytes
number of bytes to complete

Description

This updates bi_sector, bi_size and bi_idx; if the number of bytes to
complete doesn't align with a bvec boundary, then bv_len and bv_offset will
be updated on the last bvec as well.

bio will then represent the remaining, uncompleted portion of the io.



struct folio_iter
State for iterating all folios in a bio.

Definition

struct folio_iter {
  struct folio *folio;
  size_t offset;
  size_t length;
};


Members


folio
The current folio we're iterating.  NULL after the last folio.

offset
The byte offset within the current folio.

length
The number of bytes in this iteration (will not cross folio
boundary).


====================


bio_for_each_folio_all(fi, bio)
Iterate over each folio in a bio.

Parameters


fi
struct folio_iter which is updated for each folio.

bio
struct bio to iterate over.


====================


struct bio * bio_next_split(struct bio *bio, int sectors, gfp_t gfp, struct bio_set *bs)
get next sectors from a bio, splitting if necessary

Parameters


struct bio *bio
bio to split

int sectors
number of sectors to split from the front of bio

gfp_t gfp
gfp mask

struct bio_set *bs
bio set to allocate from

Return

a bio representing the next sectors of bio - if the bio is smaller
than sectors, returns the original bio unchanged.


====================


void blk_queue_flag_set(unsigned int flag, struct request_queue *q)
atomically set a queue flag

Parameters


unsigned int flag
flag to be set

struct request_queue *q
request queue


====================


void blk_queue_flag_clear(unsigned int flag, struct request_queue *q)
atomically clear a queue flag

Parameters


unsigned int flag
flag to be cleared

struct request_queue *q
request queue


====================


bool blk_queue_flag_test_and_set(unsigned int flag, struct request_queue *q)
atomically test and set a queue flag

Parameters


unsigned int flag
flag to be set

struct request_queue *q
request queue

Description

Returns the previous value of flag - 0 if the flag was not set and 1 if
the flag was already set.


====================


const char * blk_op_str(unsigned int op)
Return string XXX in the REQ_OP_XXX.

Parameters


unsigned int op
REQ_OP_XXX.

Description

Centralize block layer function to convert REQ_OP_XXX into
string format. Useful in the debugging and tracing bio or request. For
invalid REQ_OP_XXX it returns string 'UNKNOWN'.


====================


void blk_sync_queue(struct request_queue *q)
cancel any pending callbacks on a queue

Parameters


struct request_queue *q
the queue

Description


The block layer may perform asynchronous callback activity
on a queue, such as calling the unplug function after a timeout.
A block device may call blk_sync_queue to ensure that any
such activity is cancelled, thus allowing it to release resources
that the callbacks might use. The caller must already have made sure
that its ->submit_bio will not re-add plugging prior to calling
this function.
This function does not cancel any asynchronous activity arising
out of elevator or throttling code. That would require elevator_exit()
and blkcg_exit_queue() to be called with queue lock initialized.


====================


void blk_set_pm_only(struct request_queue *q)
increment pm_only counter

Parameters


struct request_queue *q
request queue pointer


====================


void blk_put_queue(struct request_queue *q)
decrement the request_queue refcount

Parameters


struct request_queue *q
the request_queue structure to decrement the refcount for

Description

Decrements the refcount of the request_queue kobject. When this reaches 0
we'll have blk_release_queue() called.

Context

Any context, but the last reference must not be dropped from
atomic context.


====================


void blk_cleanup_queue(struct request_queue *q)
shutdown a request queue

Parameters


struct request_queue *q
request queue to shutdown

Description

Mark q DYING, drain all pending requests, mark q DEAD, destroy and
put it.  All future requests will be failed immediately with -ENODEV.

Context

can sleep


====================


bool blk_get_queue(struct request_queue *q)
increment the request_queue refcount

Parameters


struct request_queue *q
the request_queue structure to increment the refcount for

Description

Increment the refcount of the request_queue kobject.

Context

Any context.


====================


void submit_bio_noacct(struct bio *bio)
re-submit a bio to the block device layer for I/O

Parameters


struct bio *bio
The bio describing the location in memory and on the device.

Description

This is a version of submit_bio() that shall only be used for I/O that is
resubmitted to lower level drivers by stacking block drivers.  All file
systems and other upper level users of the block layer should use
submit_bio() instead.


====================


void submit_bio(struct bio *bio)
submit a bio to the block device layer for I/O

Parameters


struct bio *bio
The struct bio which describes the I/O

Description

submit_bio() is used to submit I/O requests to block devices.  It is passed a
fully set up struct bio that describes the I/O that needs to be done.  The
bio will be send to the device described by the bi_bdev field.

The success/failure status of the request, along with notification of
completion, is delivered asynchronously through the ->bi_end_io() callback
in bio.  The bio must NOT be touched by thecaller until ->bi_end_io() has
been called.


====================


int bio_poll(struct bio *bio, struct io_comp_batch *iob, unsigned int flags)
poll for BIO completions

Parameters


struct bio *bio
bio to poll for

struct io_comp_batch *iob
batches of IO

unsigned int flags
BLK_POLL_* flags that control the behavior

Description

Poll for completions on queue associated with the bio. Returns number of
completed entries found.

Note

the caller must either be the context that submitted bio, or
be in a RCU critical section to prevent freeing of bio.


====================


void bio_start_io_acct_time(struct bio *bio, unsigned long start_time)
start I/O accounting for bio based drivers

Parameters


struct bio *bio
bio to start account for

unsigned long start_time
start time that should be passed back to bio_end_io_acct().


====================


unsigned long bio_start_io_acct(struct bio *bio)
start I/O accounting for bio based drivers

Parameters


struct bio *bio
bio to start account for

Description

Returns the start time that should be passed back to bio_end_io_acct().


====================


int blk_lld_busy(struct request_queue *q)
Check if underlying low-level drivers of a device are busy

Parameters


struct request_queue *q
the queue of the device being checked

Description


Check if underlying low-level drivers of a device are busy.
If the drivers want to export their busy state, they must set own
exporting function using blk_queue_lld_busy() first.
Basically, this function is used only by request stacking drivers
to stop dispatching requests to underlying devices when underlying
devices are busy.  This behavior helps more I/O merging on the queue
of the request stacking driver and prevents I/O throughput regression
on burst I/O load.

Return


0 - Not busy (The request stacking driver should dispatch request)
1 - Busy (The request stacking driver should stop dispatching request)


====================


void blk_start_plug(struct blk_plug *plug)
initialize blk_plug and track it inside the task_struct

Parameters


struct blk_plug *plug
The struct blk_plug that needs to be initialized

Description


blk_start_plug() indicates to the block layer an intent by the caller
to submit multiple I/O requests in a batch.  The block layer may use
this hint to defer submitting I/Os from the caller until blk_finish_plug()
is called.  However, the block layer may choose to submit requests
before a call to blk_finish_plug() if the number of queued I/Os
exceeds BLK_MAX_REQUEST_COUNT, or if the size of the I/O is larger than
BLK_PLUG_FLUSH_SIZE.  The queued I/Os may also be submitted early if
the task schedules (see below).
Tracking blk_plug inside the task_struct will help with auto-flushing the
pending I/O should the task end up blocking between blk_start_plug() and
blk_finish_plug(). This is important from a performance perspective, but
also ensures that we don't deadlock. For instance, if the task is blocking
for a memory allocation, memory reclaim could end up wanting to free a
page belonging to that request that is currently residing in our private
plug. By flushing the pending I/O when the process goes to sleep, we avoid
this kind of deadlock.


====================


void blk_finish_plug(struct blk_plug *plug)
mark the end of a batch of submitted I/O

Parameters


struct blk_plug *plug
The struct blk_plug passed to blk_start_plug()

Description

Indicate that a batch of I/O submissions is complete.  This function
must be paired with an initial call to blk_start_plug().  The intent
is to allow the block layer to optimize I/O submission.  See the
documentation for blk_start_plug() for more information.


====================


int blk_queue_enter(struct request_queue *q, blk_mq_req_flags_t flags)
try to increase q->q_usage_counter

Parameters


struct request_queue *q
request queue pointer

blk_mq_req_flags_t flags
BLK_MQ_REQ_NOWAIT and/or BLK_MQ_REQ_PM


====================


int blk_rq_map_user_iov(struct request_queue *q, struct request *rq, struct rq_map_data *map_data, const struct iov_iter *iter, gfp_t gfp_mask)
map user data to a request, for passthrough requests

Parameters


struct request_queue *q
request queue where request should be inserted

struct request *rq
request to map data to

struct rq_map_data *map_data
pointer to the rq_map_data holding pages (if necessary)

const struct iov_iter *iter
iovec iterator

gfp_t gfp_mask
memory allocation flags

Description


Data will be mapped directly for zero copy I/O, if possible. Otherwise
a kernel bounce buffer is used.
A matching blk_rq_unmap_user() must be issued at the end of I/O, while
still in process context.


====================


int blk_rq_unmap_user(struct bio *bio)
unmap a request with user data

Parameters


struct bio *bio
start of bio list

Description


Unmap a rq previously mapped by blk_rq_map_user(). The caller must
supply the original rq->bio from the blk_rq_map_user() return, since
the I/O completion may have changed rq->bio.


====================


int blk_rq_map_kern(struct request_queue *q, struct request *rq, void *kbuf, unsigned int len, gfp_t gfp_mask)
map kernel data to a request, for passthrough requests

Parameters


struct request_queue *q
request queue where request should be inserted

struct request *rq
request to fill

void *kbuf
the kernel buffer

unsigned int len
length of user data

gfp_t gfp_mask
memory allocation flags

Description


Data will be mapped directly if possible. Otherwise a bounce
buffer is used. Can be called multiple times to append multiple
buffers.


====================


void blk_release_queue(struct kobject *kobj)
releases all allocated resources of the request_queue

Parameters


struct kobject *kobj
pointer to a kobject, whose container is a request_queue

Description

This function releases all allocated resources of the request queue.

The struct request_queue refcount is incremented with blk_get_queue() and
decremented with blk_put_queue(). Once the refcount reaches 0 this function
is called.

For drivers that have a request_queue on a gendisk and added with
__device_add_disk() the refcount to request_queue will reach 0 with
the last put_disk() called by the driver. For drivers which don't use
__device_add_disk() this happens with blk_cleanup_queue().

Drivers exist which depend on the release of the request_queue to be
synchronous, it should not be deferred.

Context

can sleep


====================


int blk_register_queue(struct gendisk *disk)
register a block layer queue with sysfs

Parameters


struct gendisk *disk
Disk of which the request queue should be registered with sysfs.


====================


void blk_unregister_queue(struct gendisk *disk)
counterpart of blk_register_queue()

Parameters


struct gendisk *disk
Disk of which the request queue should be unregistered from sysfs.

Note

the caller is responsible for guaranteeing that this function is called
after blk_register_queue() has finished.


====================


void blk_set_default_limits(struct queue_limits *lim)
reset limits to default values

Parameters


struct queue_limits *lim
the queue_limits structure to reset

Description


Returns a queue_limit struct to its default state.


====================


void blk_set_stacking_limits(struct queue_limits *lim)
set default limits for stacking devices

Parameters


struct queue_limits *lim
the queue_limits structure to reset

Description


Returns a queue_limit struct to its default state. Should be used
by stacking drivers like DM that have no internal limits.


====================


void blk_queue_bounce_limit(struct request_queue *q, enum blk_bounce bounce)
set bounce buffer limit for queue

Parameters


struct request_queue *q
the request queue for the device

enum blk_bounce bounce
bounce limit to enforce

Description


Force bouncing for ISA DMA ranges or highmem.
DEPRECATED, don't use in new code.


====================


void blk_queue_max_hw_sectors(struct request_queue *q, unsigned int max_hw_sectors)
set max sectors for a request for this queue

Parameters


struct request_queue *q
the request queue for the device

unsigned int max_hw_sectors
max hardware sectors in the usual 512b unit

Description


Enables a low level driver to set a hard upper limit,
max_hw_sectors, on the size of requests.  max_hw_sectors is set by
the device driver based upon the capabilities of the I/O
controller.
max_dev_sectors is a hard limit imposed by the storage device for
READ/WRITE requests. It is set by the disk driver.
max_sectors is a soft limit imposed by the block layer for
filesystem type requests.  This value can be overridden on a
per-device basis in /sys/block/<device>/queue/max_sectors_kb.
The soft limit can not exceed max_hw_sectors.


====================


void blk_queue_chunk_sectors(struct request_queue *q, unsigned int chunk_sectors)
set size of the chunk for this queue

Parameters


struct request_queue *q
the request queue for the device

unsigned int chunk_sectors
chunk sectors in the usual 512b unit

Description


If a driver doesn't want IOs to cross a given chunk size, it can set
this limit and prevent merging across chunks. Note that the block layer
must accept a page worth of data at any offset. So if the crossing of
chunks is a hard limitation in the driver, it must still be prepared
to split single page bios.


====================


void blk_queue_max_discard_sectors(struct request_queue *q, unsigned int max_discard_sectors)
set max sectors for a single discard

Parameters


struct request_queue *q
the request queue for the device

unsigned int max_discard_sectors
maximum number of sectors to discard


====================


void blk_queue_max_write_zeroes_sectors(struct request_queue *q, unsigned int max_write_zeroes_sectors)
set max sectors for a single write zeroes

Parameters


struct request_queue *q
the request queue for the device

unsigned int max_write_zeroes_sectors
maximum number of sectors to write per command


====================


void blk_queue_max_zone_append_sectors(struct request_queue *q, unsigned int max_zone_append_sectors)
set max sectors for a single zone append

Parameters


struct request_queue *q
the request queue for the device

unsigned int max_zone_append_sectors
maximum number of sectors to write per command


====================


void blk_queue_max_segments(struct request_queue *q, unsigned short max_segments)
set max hw segments for a request for this queue

Parameters


struct request_queue *q
the request queue for the device

unsigned short max_segments
max number of segments

Description


Enables a low level driver to set an upper limit on the number of
hw data segments in a request.


====================


void blk_queue_max_discard_segments(struct request_queue *q, unsigned short max_segments)
set max segments for discard requests

Parameters


struct request_queue *q
the request queue for the device

unsigned short max_segments
max number of segments

Description


Enables a low level driver to set an upper limit on the number of
segments in a discard request.


====================


void blk_queue_max_segment_size(struct request_queue *q, unsigned int max_size)
set max segment size for blk_rq_map_sg

Parameters


struct request_queue *q
the request queue for the device

unsigned int max_size
max size of segment in bytes

Description


Enables a low level driver to set an upper limit on the size of a
coalesced segment


====================


void blk_queue_logical_block_size(struct request_queue *q, unsigned int size)
set logical block size for the queue

Parameters


struct request_queue *q
the request queue for the device

unsigned int size
the logical block size, in bytes

Description


This should be set to the lowest possible block size that the
storage device can address.  The default of 512 covers most
hardware.


====================


void blk_queue_physical_block_size(struct request_queue *q, unsigned int size)
set physical block size for the queue

Parameters


struct request_queue *q
the request queue for the device

unsigned int size
the physical block size, in bytes

Description


This should be set to the lowest possible sector size that the
hardware can operate on without reverting to read-modify-write
operations.


====================


void blk_queue_zone_write_granularity(struct request_queue *q, unsigned int size)
set zone write granularity for the queue

Parameters


struct request_queue *q
the request queue for the zoned device

unsigned int size
the zone write granularity size, in bytes

Description


This should be set to the lowest possible size allowing to write in
sequential zones of a zoned block device.


====================


void blk_queue_alignment_offset(struct request_queue *q, unsigned int offset)
set physical block alignment offset

Parameters


struct request_queue *q
the request queue for the device

unsigned int offset
alignment offset in bytes

Description


Some devices are naturally misaligned to compensate for things like
the legacy DOS partition table 63-sector offset.  Low-level drivers
should call this function for devices whose first sector is not
naturally aligned.


====================


void blk_limits_io_min(struct queue_limits *limits, unsigned int min)
set minimum request size for a device

Parameters


struct queue_limits *limits
the queue limits

unsigned int min
smallest I/O size in bytes

Description


Some devices have an internal block size bigger than the reported
hardware sector size.  This function can be used to signal the
smallest I/O the device can perform without incurring a performance
penalty.


====================


void blk_queue_io_min(struct request_queue *q, unsigned int min)
set minimum request size for the queue

Parameters


struct request_queue *q
the request queue for the device

unsigned int min
smallest I/O size in bytes

Description


Storage devices may report a granularity or preferred minimum I/O
size which is the smallest request the device can perform without
incurring a performance penalty.  For disk drives this is often the
physical block size.  For RAID arrays it is often the stripe chunk
size.  A properly aligned multiple of minimum_io_size is the
preferred request size for workloads where a high number of I/O
operations is desired.


====================


void blk_limits_io_opt(struct queue_limits *limits, unsigned int opt)
set optimal request size for a device

Parameters


struct queue_limits *limits
the queue limits

unsigned int opt
smallest I/O size in bytes

Description


Storage devices may report an optimal I/O size, which is the
device's preferred unit for sustained I/O.  This is rarely reported
for disk drives.  For RAID arrays it is usually the stripe width or
the internal track size.  A properly aligned multiple of
optimal_io_size is the preferred request size for workloads where
sustained throughput is desired.


====================


void blk_queue_io_opt(struct request_queue *q, unsigned int opt)
set optimal request size for the queue

Parameters


struct request_queue *q
the request queue for the device

unsigned int opt
optimal request size in bytes

Description


Storage devices may report an optimal I/O size, which is the
device's preferred unit for sustained I/O.  This is rarely reported
for disk drives.  For RAID arrays it is usually the stripe width or
the internal track size.  A properly aligned multiple of
optimal_io_size is the preferred request size for workloads where
sustained throughput is desired.


====================


int blk_stack_limits(struct queue_limits *t, struct queue_limits *b, sector_t start)
adjust queue_limits for stacked devices

Parameters


struct queue_limits *t
the stacking driver limits (top device)

struct queue_limits *b
the underlying queue limits (bottom, component device)

sector_t start
first data sector within component device

Description


This function is used by stacking drivers like MD and DM to ensure
that all component devices have compatible block sizes and
alignments.  The stacking driver must provide a queue_limits
struct (top) and then iteratively call the stacking function for
all component (bottom) devices.  The stacking function will
attempt to combine the values and ensure proper alignment.
Returns 0 if the top and bottom queue_limits are compatible.  The
top device's block sizes and alignment offsets may be adjusted to
ensure alignment with the bottom device. If no compatible sizes
and alignments exist, -1 is returned and the resulting top
queue_limits will have the misaligned flag set to indicate that
the alignment_offset is undefined.


====================


void disk_stack_limits(struct gendisk *disk, struct block_device *bdev, sector_t offset)
adjust queue limits for stacked drivers

Parameters


struct gendisk *disk
MD/DM gendisk (top)

struct block_device *bdev
the underlying block device (bottom)

sector_t offset
offset to beginning of data within component device

Description


Merges the limits for a top level gendisk and a bottom level
block_device.


====================


void blk_queue_update_dma_pad(struct request_queue *q, unsigned int mask)
update pad mask

Parameters


struct request_queue *q
the request queue for the device

unsigned int mask
pad mask

Description

Update dma pad mask.

Appending pad buffer to a request modifies the last entry of a
scatter list such that it includes the pad buffer.


====================


void blk_queue_segment_boundary(struct request_queue *q, unsigned long mask)
set boundary rules for segment merging

Parameters


struct request_queue *q
the request queue for the device

unsigned long mask
the memory boundary mask


====================


void blk_queue_virt_boundary(struct request_queue *q, unsigned long mask)
set boundary rules for bio merging

Parameters


struct request_queue *q
the request queue for the device

unsigned long mask
the memory boundary mask


====================


void blk_queue_dma_alignment(struct request_queue *q, int mask)
set dma length and memory alignment

Parameters


struct request_queue *q
the request queue for the device

int mask
alignment mask

Description


set required memory and length alignment for direct dma transactions.
this is used when building direct io requests for the queue.


====================


void blk_queue_update_dma_alignment(struct request_queue *q, int mask)
update dma length and memory alignment

Parameters


struct request_queue *q
the request queue for the device

int mask
alignment mask

Description


update required memory and length alignment for direct dma transactions.
If the requested alignment is larger than the current alignment, then
the current queue alignment is updated to the new value, otherwise it
is left alone.  The design of this is to allow multiple objects
(driver, device, transport etc) to set their respective
alignments without having them interfere.


====================


void blk_set_queue_depth(struct request_queue *q, unsigned int depth)
tell the block layer about the device queue depth

Parameters


struct request_queue *q
the request queue for the device

unsigned int depth
queue depth


====================


void blk_queue_write_cache(struct request_queue *q, bool wc, bool fua)
configure queue's write cache

Parameters


struct request_queue *q
the request queue for the device

bool wc
write back cache on or off

bool fua
device supports FUA writes, if true

Description

Tell the block layer about the write cache of q.


====================


void blk_queue_required_elevator_features(struct request_queue *q, unsigned int features)
Set a queue required elevator features

Parameters


struct request_queue *q
the request queue for the target device

unsigned int features
Required elevator features OR'ed together

Description

Tell the block layer that for the device controlled through q, only the
only elevators that can be used are those that implement at least the set of
features specified by features.


====================


bool blk_queue_can_use_dma_map_merging(struct request_queue *q, struct device *dev)
configure queue for merging segments.

Parameters


struct request_queue *q
the request queue for the device

struct device *dev
the device pointer for dma

Description

Tell the block layer about merging the segments by dma map of q.


====================


void blk_queue_set_zoned(struct gendisk *disk, enum blk_zoned_model model)
configure a disk queue zoned model.

Parameters


struct gendisk *disk
the gendisk of the queue to configure

enum blk_zoned_model model
the zoned model to set

Description

Set the zoned model of the request queue of disk according to model.
When model is BLK_ZONED_HM (host managed), this should be called only
if zoned block device support is enabled (CONFIG_BLK_DEV_ZONED option).
If model specifies BLK_ZONED_HA (host aware), the effective model used
depends on CONFIG_BLK_DEV_ZONED settings and on the existence of partitions
on the disk.


====================


int blkdev_issue_flush(struct block_device *bdev)
queue a flush

Parameters


struct block_device *bdev
blockdev to issue flush for

Description


Issue a flush for the block device in question.


====================


int blkdev_issue_discard(struct block_device *bdev, sector_t sector, sector_t nr_sects, gfp_t gfp_mask, unsigned long flags)
queue a discard

Parameters


struct block_device *bdev
blockdev to issue discard for

sector_t sector
start sector

sector_t nr_sects
number of sectors to discard

gfp_t gfp_mask
memory allocation flags (for bio_alloc)

unsigned long flags
BLKDEV_DISCARD_* flags to control behaviour

Description


Issue a discard request for the sectors in question.


====================


int __blkdev_issue_zeroout(struct block_device *bdev, sector_t sector, sector_t nr_sects, gfp_t gfp_mask, struct bio **biop, unsigned flags)
generate number of zero filed write bios

Parameters


struct block_device *bdev
blockdev to issue

sector_t sector
start sector

sector_t nr_sects
number of sectors to write

gfp_t gfp_mask
memory allocation flags (for bio_alloc)

struct bio **biop
pointer to anchor bio

unsigned flags
controls detailed behavior

Description


Zero-fill a block range, either using hardware offload or by explicitly
writing zeroes to the device.
If a device is using logical block provisioning, the underlying space will
not be released if flags contains BLKDEV_ZERO_NOUNMAP.
If flags contains BLKDEV_ZERO_NOFALLBACK, the function will return
-EOPNOTSUPP if no explicit hardware offload for zeroing is provided.


====================


int blkdev_issue_zeroout(struct block_device *bdev, sector_t sector, sector_t nr_sects, gfp_t gfp_mask, unsigned flags)
zero-fill a block range

Parameters


struct block_device *bdev
blockdev to write

sector_t sector
start sector

sector_t nr_sects
number of sectors to write

gfp_t gfp_mask
memory allocation flags (for bio_alloc)

unsigned flags
controls detailed behavior

Description


Zero-fill a block range, either using hardware offload or by explicitly
writing zeroes to the device.  See __blkdev_issue_zeroout() for the
valid values for flags.


====================


int blk_rq_count_integrity_sg(struct request_queue *q, struct bio *bio)
Count number of integrity scatterlist elements

Parameters


struct request_queue *q
request queue

struct bio *bio
bio with integrity metadata attached

Description

Returns the number of elements required in a
scatterlist corresponding to the integrity metadata in a bio.


====================


int blk_rq_map_integrity_sg(struct request_queue *q, struct bio *bio, struct scatterlist *sglist)
Map integrity metadata into a scatterlist

Parameters


struct request_queue *q
request queue

struct bio *bio
bio with integrity metadata attached

struct scatterlist *sglist
target scatterlist

Description

Map the integrity vectors in request into a
scatterlist.  The scatterlist must be big enough to hold all
elements.  I.e. sized using blk_rq_count_integrity_sg().


====================


int blk_integrity_compare(struct gendisk *gd1, struct gendisk *gd2)
Compare integrity profile of two disks

Parameters


struct gendisk *gd1
Disk to compare

struct gendisk *gd2
Disk to compare

Description

Meta-devices like DM and MD need to verify that all
sub-devices use the same integrity format before advertising to
upper layers that they can send/receive integrity metadata.  This
function can be used to check whether two gendisk devices have
compatible integrity formats.


====================


void blk_integrity_register(struct gendisk *disk, struct blk_integrity *template)
Register a gendisk as being integrity-capable

Parameters


struct gendisk *disk
struct gendisk pointer to make integrity-aware

struct blk_integrity *template
block integrity profile to register

Description

When a device needs to advertise itself as being able to
send/receive integrity metadata it must use this function to register
the capability with the block layer. The template is a blk_integrity
struct with values appropriate for the underlying hardware. See
Data Integrity.


====================


void blk_integrity_unregister(struct gendisk *disk)
Unregister block integrity profile

Parameters


struct gendisk *disk
disk whose integrity profile to unregister

Description

This function unregisters the integrity capability from
a block device.


====================


int blk_trace_ioctl(struct block_device *bdev, unsigned cmd, char __user *arg)

handle the ioctls associated with tracing


Parameters


struct block_device *bdev
the block device

unsigned cmd
the ioctl cmd

char __user *arg
the argument data, if any


====================


void blk_trace_shutdown(struct request_queue *q)

stop and cleanup trace structures


Parameters


struct request_queue *q
the request queue associated with the device


====================


void blk_add_trace_rq(struct request *rq, blk_status_t error, unsigned int nr_bytes, u32 what, u64 cgid)
Add a trace for a request oriented action

Parameters


struct request *rq
the source request

blk_status_t error
return status to log

unsigned int nr_bytes
number of completed bytes

u32 what
the action

u64 cgid
the cgroup info

Description


Records an action against a request. Will log the bio offset + size.


====================


void blk_add_trace_bio(struct request_queue *q, struct bio *bio, u32 what, int error)
Add a trace for a bio oriented action

Parameters


struct request_queue *q
queue the io is for

struct bio *bio
the source bio

u32 what
the action

int error
error, if any

Description


Records an action against a bio. Will log the bio offset + size.


====================


void blk_add_trace_bio_remap(void *ignore, struct bio *bio, dev_t dev, sector_t from)
Add a trace for a bio-remap operation

Parameters


void *ignore
trace callback data parameter (not used)

struct bio *bio
the source bio

dev_t dev
source device

sector_t from
source sector

Description

Called after a bio is remapped to a different device and/or sector.


====================


void blk_add_trace_rq_remap(void *ignore, struct request *rq, dev_t dev, sector_t from)
Add a trace for a request-remap operation

Parameters


void *ignore
trace callback data parameter (not used)

struct request *rq
the source request

dev_t dev
target device

sector_t from
source sector

Description


Device mapper remaps request to other devices.
Add a trace for that action.


====================


void disk_release(struct device *dev)
releases all allocated resources of the gendisk

Parameters


struct device *dev
the device representing this disk

Description

This function releases all allocated resources of the gendisk.

Drivers which used __device_add_disk() have a gendisk with a request_queue
assigned. Since the request_queue sits on top of the gendisk for these
drivers we also call blk_put_queue() for them, and we expect the
request_queue refcount to reach 0 at this point, and so the request_queue
will also be freed prior to the disk.

Context

can sleep


====================


int __register_blkdev(unsigned int major, const char *name, void (*probe)(dev_t devt))
register a new block device

Parameters


unsigned int major
the requested major device number [1..BLKDEV_MAJOR_MAX-1]. If
major = 0, try to allocate any unused major number.

const char *name
the name of the new block device as a zero terminated string

void (*probe)(dev_t devt)
pre-devtmpfs / pre-udev callback used to create disks when their
pre-created device node is accessed. When a probe call uses
add_disk() and it fails the driver must cleanup resources. This
interface may soon be removed.

Description

The name must be unique within the system.

The return value depends on the major input parameter:



if a major device number was requested in range [1..BLKDEV_MAJOR_MAX-1]
then the function returns zero on success, or a negative error code
if any unused major number was requested with major = 0 parameter
then the return value is the allocated major number in range
[1..BLKDEV_MAJOR_MAX-1] or a negative error code otherwise


See Linux allocated devices (4.x+ version) for the list of allocated
major numbers.

Use register_blkdev instead for any new code.


====================


int device_add_disk(struct device *parent, struct gendisk *disk, const struct attribute_group **groups)
add disk information to kernel list

Parameters


struct device *parent
parent device for the disk

struct gendisk *disk
per-device partitioning information

const struct attribute_group **groups
Additional per-device sysfs groups

Description

This function registers the partitioning information in disk
with the kernel.


====================


void blk_mark_disk_dead(struct gendisk *disk)
mark a disk as dead

Parameters


struct gendisk *disk
disk to mark as dead

Description

Mark as disk as dead (e.g. surprise removed) and don't accept any new I/O
to this disk.


====================


void del_gendisk(struct gendisk *disk)
remove the gendisk

Parameters


struct gendisk *disk
the struct gendisk to remove

Description

Removes the gendisk and all its associated resources. This deletes the
partitions associated with the gendisk, and unregisters the associated
request_queue.

This is the counter to the respective __device_add_disk() call.

The final removal of the struct gendisk happens when its refcount reaches 0
with put_disk(), which should be called after del_gendisk(), if
__device_add_disk() was used.

Drivers exist which depend on the release of the gendisk to be synchronous,
it should not be deferred.

Context

can sleep


====================


void invalidate_disk(struct gendisk *disk)
invalidate the disk

Parameters


struct gendisk *disk
the struct gendisk to invalidate

Description

A helper to invalidates the disk. It will clean the disk's associated
buffer/page caches and reset its internal states so that the disk
can be reused by the drivers.

Context

can sleep


====================


void put_disk(struct gendisk *disk)
decrements the gendisk refcount

Parameters


struct gendisk *disk
the struct gendisk to decrement the refcount for

Description

This decrements the refcount for the struct gendisk. When this reaches 0
we'll have disk_release() called.

Context

Any context, but the last reference must not be dropped from
atomic context.


====================


void blk_cleanup_disk(struct gendisk *disk)
shutdown a gendisk allocated by blk_alloc_disk

Parameters


struct gendisk *disk
gendisk to shutdown

Description

Mark the queue hanging off disk DYING, drain all pending requests, then mark
the queue DEAD, destroy and put it and the gendisk structure.

Context

can sleep


====================


void set_disk_ro(struct gendisk *disk, bool read_only)
set a gendisk read-only

Parameters


struct gendisk *disk
gendisk to operate on

bool read_only
true to set the disk read-only, false set the disk read/write

Description

This function is used to indicate whether a given disk device should have its
read-only flag set. set_disk_ro() is typically used by device drivers to
indicate whether the underlying physical device is write-protected.


====================


int freeze_bdev(struct block_device *bdev)

lock a filesystem and force it into a consistent state


Parameters


struct block_device *bdev
blockdevice to lock

Description

If a superblock is found on this device, we take the s_umount semaphore
on it to make sure nobody unmounts until the snapshot creation is done.
The reference counter (bd_fsfreeze_count) guarantees that only the last
unfreeze process can unfreeze the frozen filesystem actually when multiple
freeze requests arrive simultaneously. It counts up in freeze_bdev() and
count down in thaw_bdev(). When it becomes 0, thaw_bdev() will unfreeze
actually.


====================


int thaw_bdev(struct block_device *bdev)

unlock filesystem


Parameters


struct block_device *bdev
blockdevice to unlock

Description

Unlocks the filesystem and marks it writeable again after freeze_bdev().


====================


int bd_prepare_to_claim(struct block_device *bdev, void *holder)
claim a block device

Parameters


struct block_device *bdev
block device of interest

void *holder
holder trying to claim bdev

Description

Claim bdev.  This function fails if bdev is already claimed by another
holder and waits if another claiming is in progress. return, the caller
has ownership of bd_claiming and bd_holder[s].

Return

0 if bdev can be claimed, -EBUSY otherwise.


====================


void bd_abort_claiming(struct block_device *bdev, void *holder)
abort claiming of a block device

Parameters


struct block_device *bdev
block device of interest

void *holder
holder that has claimed bdev

Description

Abort claiming of a block device when the exclusive open failed. This can be
also used when exclusive open is not actually desired and we just needed
to block other exclusive openers for a while.


====================


struct block_device * blkdev_get_by_dev(dev_t dev, fmode_t mode, void *holder)
open a block device by device number

Parameters


dev_t dev
device number of block device to open

fmode_t mode
FMODE_* mask

void *holder
exclusive holder identifier

Description

Open the block device described by device number dev. If mode includes
FMODE_EXCL, the block device is opened with exclusive access.  Specifying
FMODE_EXCL with a NULL holder is invalid.  Exclusive opens may nest for
the same holder.

Use this interface ONLY if you really do not have anything better - i.e. when
you are behind a truly sucky interface and all you are given is a device
number.  Everything else should use blkdev_get_by_path().

Context

Might sleep.

Return

Reference to the block_device on success, ERR_PTR(-errno) on failure.


====================


struct block_device * blkdev_get_by_path(const char *path, fmode_t mode, void *holder)
open a block device by name

Parameters


const char *path
path to the block device to open

fmode_t mode
FMODE_* mask

void *holder
exclusive holder identifier

Description

Open the block device described by the device file at path.  If mode
includes FMODE_EXCL, the block device is opened with exclusive access.
Specifying FMODE_EXCL with a NULL holder is invalid.  Exclusive opens may
nest for the same holder.

Context

Might sleep.

Return

Reference to the block_device on success, ERR_PTR(-errno) on failure.


====================


int lookup_bdev(const char *pathname, dev_t *dev)
Look up a struct block_device by name.

Parameters


const char *pathname
Name of the block device in the filesystem.

dev_t *dev
Pointer to the block device's dev_t, if found.

Description

Lookup the block device's dev_t at pathname in the current
namespace if possible and return it in dev.

Context

May sleep.

Return

0 if succeeded, negative errno otherwise.


====================


int register_chrdev_region(dev_t from, unsigned count, const char *name)
register a range of device numbers

Parameters


dev_t from
the first in the desired range of device numbers; must include
the major number.

unsigned count
the number of consecutive device numbers required

const char *name
the name of the device or driver.

Description

Return value is zero on success, a negative error code on failure.


====================


int alloc_chrdev_region(dev_t *dev, unsigned baseminor, unsigned count, const char *name)
register a range of char device numbers

Parameters


dev_t *dev
output parameter for first assigned number

unsigned baseminor
first of the requested range of minor numbers

unsigned count
the number of minor numbers required

const char *name
the name of the associated device or driver

Description

Allocates a range of char device numbers.  The major number will be
chosen dynamically, and returned (along with the first minor number)
in dev.  Returns zero or a negative error code.


====================


int __register_chrdev(unsigned int major, unsigned int baseminor, unsigned int count, const char *name, const struct file_operations *fops)
create and register a cdev occupying a range of minors

Parameters


unsigned int major
major device number or 0 for dynamic allocation

unsigned int baseminor
first of the requested range of minor numbers

unsigned int count
the number of minor numbers required

const char *name
name of this range of devices

const struct file_operations *fops
file operations associated with this devices

Description

If major == 0 this functions will dynamically allocate a major and return
its number.

If major > 0 this function will attempt to reserve a device with the given
major number and will return zero on success.

Returns a -ve errno on failure.

The name of this device has nothing to do with the name of the device in
/dev. It only helps to keep track of the different owners of devices. If
your module name has only one type of devices it's ok to use e.g. the name
of the module here.


====================


void unregister_chrdev_region(dev_t from, unsigned count)
unregister a range of device numbers

Parameters


dev_t from
the first in the range of numbers to unregister

unsigned count
the number of device numbers to unregister

Description

This function will unregister a range of count device numbers,
starting with from.  The caller should normally be the one who
allocated those numbers in the first place...¦


====================


void __unregister_chrdev(unsigned int major, unsigned int baseminor, unsigned int count, const char *name)
unregister and destroy a cdev

Parameters


unsigned int major
major device number

unsigned int baseminor
first of the range of minor numbers

unsigned int count
the number of minor numbers this cdev is occupying

const char *name
name of this range of devices

Description

Unregister and destroy the cdev occupying the region described by
major, baseminor and count.  This function undoes what
__register_chrdev() did.


====================


int cdev_add(struct cdev *p, dev_t dev, unsigned count)
add a char device to the system

Parameters


struct cdev *p
the cdev structure for the device

dev_t dev
the first device number for which this device is responsible

unsigned count
the number of consecutive minor numbers corresponding to this
device

Description

cdev_add() adds the device represented by p to the system, making it
live immediately.  A negative error code is returned on failure.


====================


void cdev_set_parent(struct cdev *p, struct kobject *kobj)
set the parent kobject for a char device

Parameters


struct cdev *p
the cdev structure

struct kobject *kobj
the kobject to take a reference to

Description

cdev_set_parent() sets a parent kobject which will be referenced
appropriately so the parent is not freed before the cdev. This
should be called before cdev_add.


====================


int cdev_device_add(struct cdev *cdev, struct device *dev)
add a char device and it's corresponding struct device, linkink

Parameters


struct cdev *cdev
the cdev structure

struct device *dev
the device structure

Description

cdev_device_add() adds the char device represented by cdev to the system,
just as cdev_add does. It then adds dev to the system using device_add
The dev_t for the char device will be taken from the struct device which
needs to be initialized first. This helper function correctly takes a
reference to the parent device so the parent will not get released until
all references to the cdev are released.

This helper uses dev->devt for the device number. If it is not set
it will not add the cdev and it will be equivalent to device_add.

This function should be used whenever the struct cdev and the
struct device are members of the same structure whose lifetime is
managed by the struct device.

NOTE

Callers must assume that userspace was able to open the cdev and
can call cdev fops callbacks at any time, even if this function fails.


====================


void cdev_device_del(struct cdev *cdev, struct device *dev)
inverse of cdev_device_add

Parameters


struct cdev *cdev
the cdev structure

struct device *dev
the device structure

Description

cdev_device_del() is a helper function to call cdev_del and device_del.
It should be used whenever cdev_device_add is used.

If dev->devt is not set it will not remove the cdev and will be equivalent
to device_del.

NOTE

This guarantees that associated sysfs callbacks are not running
or runnable, however any cdevs already open will remain and their fops
will still be callable even after this function returns.


====================


void cdev_del(struct cdev *p)
remove a cdev from the system

Parameters


struct cdev *p
the cdev structure to be removed

Description

cdev_del() removes p from the system, possibly freeing the structure
itself.

NOTE

This guarantees that cdev device will no longer be able to be
opened, however any cdevs already open will remain and their fops will
still be callable even after cdev_del returns.


====================


struct cdev * cdev_alloc(void)
allocate a cdev structure

Parameters


void
no arguments

Description

Allocates and returns a cdev structure, or NULL on failure.


====================


void cdev_init(struct cdev *cdev, const struct file_operations *fops)
initialize a cdev structure

Parameters


struct cdev *cdev
the structure to initialize

const struct file_operations *fops
the file_operations for this device

Description

Initializes cdev, remembering fops, making it ready to add to the
system with cdev_add().

The clock framework defines programming interfaces to support software
management of the system clock tree. This framework is widely used with
System-On-Chip (SOC) platforms to support power management and various
devices which may need custom clock rates. Note that these 'clocks'
don't relate to timekeeping or real time clocks (RTCs), each of which
have separate frameworks. These struct clk
instances may be used to manage for example a 96 MHz signal that is used
to shift bits into and out of peripherals or busses, or otherwise
trigger synchronous state machine transitions in system hardware.

Power management is supported by explicit software clock gating: unused
clocks are disabled, so the system doesn't waste power changing the
state of transistors that aren't in active use. On some systems this may
be backed by hardware clock gating, where clocks are gated without being
disabled in software. Sections of chips that are powered but not clocked
may be able to retain their last state. This low power state is often
called a retention mode. This mode still incurs leakage currents,
especially with finer circuit geometries, but for CMOS circuits power is
mostly used by clocked state changes.

Power-aware drivers only enable their clocks when the device they manage
is in active use. Also, system sleep states often differ according to
which clock domains are active: while a 'standby' state may allow wakeup
from several active domains, a 'mem' (suspend-to-RAM) state may require
a more wholesale shutdown of clocks derived from higher speed PLLs and
oscillators, limiting the number of possible wakeup event sources. A
driver's suspend method may need to be aware of system-specific clock
constraints on the target sleep state.

Some platforms support programmable clock generators. These can be used
by external chips of various kinds, such as other CPUs, multimedia
codecs, and devices with strict requirements for interface clocking.



struct clk_notifier
associate a clk with a notifier

Definition

struct clk_notifier {
  struct clk                      *clk;
  struct srcu_notifier_head       notifier_head;
  struct list_head                node;
};


Members


clk
struct clk * to associate the notifier with

notifier_head
a blocking_notifier_head for this clk

node
linked list pointers

Description

A list of struct clk_notifier is maintained by the notifier code.
An entry is created whenever code registers the first notifier on a
particular clk.  Future notifiers on that clk are added to the
notifier_head.



struct clk_notifier_data
rate data to pass to the notifier callback

Definition

struct clk_notifier_data {
  struct clk              *clk;
  unsigned long           old_rate;
  unsigned long           new_rate;
};


Members


clk
struct clk * being changed

old_rate
previous rate of this clk

new_rate
new rate of this clk

Description

For a pre-notifier, old_rate is the clk's rate before this rate
change, and new_rate is what the rate will be in the future.  For a
post-notifier, old_rate and new_rate are both set to the clk's
current rate (this was done to optimize the implementation).



struct clk_bulk_data
Data used for bulk clk operations.

Definition

struct clk_bulk_data {
  const char              *id;
  struct clk              *clk;
};


Members


id
clock consumer ID

clk
struct clk * to store the associated clock

Description

The CLK APIs provide a series of clk_bulk_() API calls as
a convenience to consumers which require multiple clks.  This
structure is used to manage data for these calls.


====================


int clk_notifier_register(struct clk *clk, struct notifier_block *nb)
register a clock rate-change notifier callback

Parameters


struct clk *clk
clock whose rate we are interested in

struct notifier_block *nb
notifier block with callback function pointer

Description

ProTip: debugging across notifier chains can be frustrating. Make sure that
your notifier callback function prints a nice big warning in case of
failure.


====================


int clk_notifier_unregister(struct clk *clk, struct notifier_block *nb)
unregister a clock rate-change notifier callback

Parameters


struct clk *clk
clock whose rate we are no longer interested in

struct notifier_block *nb
notifier block which will be unregistered


====================


int devm_clk_notifier_register(struct device *dev, struct clk *clk, struct notifier_block *nb)
register a managed rate-change notifier callback

Parameters


struct device *dev
device for clock 'consumer'

struct clk *clk
clock whose rate we are interested in

struct notifier_block *nb
notifier block with callback function pointer

Description

Returns 0 on success, -EERROR otherwise


====================


long clk_get_accuracy(struct clk *clk)
obtain the clock accuracy in ppb (parts per billion) for a clock source.

Parameters


struct clk *clk
clock source

Description

This gets the clock source accuracy expressed in ppb.
A perfect clock returns 0.


====================


int clk_set_phase(struct clk *clk, int degrees)
adjust the phase shift of a clock signal

Parameters


struct clk *clk
clock signal source

int degrees
number of degrees the signal is shifted

Description

Shifts the phase of a clock signal by the specified degrees. Returns 0 on
success, -EERROR otherwise.


====================


int clk_get_phase(struct clk *clk)
return the phase shift of a clock signal

Parameters


struct clk *clk
clock signal source

Description

Returns the phase shift of a clock node in degrees, otherwise returns
-EERROR.


====================


int clk_set_duty_cycle(struct clk *clk, unsigned int num, unsigned int den)
adjust the duty cycle ratio of a clock signal

Parameters


struct clk *clk
clock signal source

unsigned int num
numerator of the duty cycle ratio to be applied

unsigned int den
denominator of the duty cycle ratio to be applied

Description

Adjust the duty cycle of a clock signal by the specified ratio. Returns 0 on
success, -EERROR otherwise.


====================


int clk_get_scaled_duty_cycle(struct clk *clk, unsigned int scale)
return the duty cycle ratio of a clock signal

Parameters


struct clk *clk
clock signal source

unsigned int scale
scaling factor to be applied to represent the ratio as an integer

Description

Returns the duty cycle ratio multiplied by the scale provided, otherwise
returns -EERROR.


====================


bool clk_is_match(const struct clk *p, const struct clk *q)
check if two clk's point to the same hardware clock

Parameters


const struct clk *p
clk compared against q

const struct clk *q
clk compared against p

Description

Returns true if the two struct clk pointers both point to the same hardware
clock node. Put differently, returns true if p and q
share the same struct clk_core object.

Returns false otherwise. Note that two NULL clks are treated as matching.


====================


int clk_prepare(struct clk *clk)
prepare a clock source

Parameters


struct clk *clk
clock source

Description

This prepares the clock source for use.

Must not be called from within atomic context.


====================


bool clk_is_enabled_when_prepared(struct clk *clk)
indicate if preparing a clock also enables it.

Parameters


struct clk *clk
clock source

Description

Returns true if clk_prepare() implicitly enables the clock, effectively
making clk_enable()/clk_disable() no-ops, false otherwise.

This is of interest mainly to the power management code where actually
disabling the clock also requires unpreparing it to have any material
effect.

Regardless of the value returned here, the caller must always invoke
clk_enable() or clk_prepare_enable()  and counterparts for usage counts
to be right.


====================


void clk_unprepare(struct clk *clk)
undo preparation of a clock source

Parameters


struct clk *clk
clock source

Description

This undoes a previously prepared clock.  The caller must balance
the number of prepare and unprepare calls.

Must not be called from within atomic context.


====================


struct clk * clk_get(struct device *dev, const char *id)
lookup and obtain a reference to a clock producer.

Parameters


struct device *dev
device for clock 'consumer'

const char *id
clock consumer ID

Description

Returns a struct clk corresponding to the clock producer, or
valid IS_ERR() condition containing errno.  The implementation
uses dev and id to determine the clock consumer, and thereby
the clock producer.  (IOW, id may be identical strings, but
clk_get may return different clock producers depending on dev.)

Drivers must assume that the clock source is not enabled.

clk_get should not be called from within interrupt context.


====================


int clk_bulk_get(struct device *dev, int num_clks, struct clk_bulk_data *clks)
lookup and obtain a number of references to clock producer.

Parameters


struct device *dev
device for clock 'consumer'

int num_clks
the number of clk_bulk_data

struct clk_bulk_data *clks
the clk_bulk_data table of consumer

Description

This helper function allows drivers to get several clk consumers in one
operation. If any of the clk cannot be acquired then any clks
that were obtained will be freed before returning to the caller.

Returns 0 if all clocks specified in clk_bulk_data table are obtained
successfully, or valid IS_ERR() condition containing errno.
The implementation uses dev and clk_bulk_data.id to determine the
clock consumer, and thereby the clock producer.
The clock returned is stored in each clk_bulk_data.clk field.

Drivers must assume that the clock source is not enabled.

clk_bulk_get should not be called from within interrupt context.


====================


int clk_bulk_get_all(struct device *dev, struct clk_bulk_data **clks)
lookup and obtain all available references to clock producer.

Parameters


struct device *dev
device for clock 'consumer'

struct clk_bulk_data **clks
pointer to the clk_bulk_data table of consumer

Description

This helper function allows drivers to get all clk consumers in one
operation. If any of the clk cannot be acquired then any clks
that were obtained will be freed before returning to the caller.

Returns a positive value for the number of clocks obtained while the
clock references are stored in the clk_bulk_data table in clks field.
Returns 0 if there're none and a negative value if something failed.

Drivers must assume that the clock source is not enabled.

clk_bulk_get should not be called from within interrupt context.


====================


int clk_bulk_get_optional(struct device *dev, int num_clks, struct clk_bulk_data *clks)
lookup and obtain a number of references to clock producer

Parameters


struct device *dev
device for clock 'consumer'

int num_clks
the number of clk_bulk_data

struct clk_bulk_data *clks
the clk_bulk_data table of consumer

Description

Behaves the same as clk_bulk_get() except where there is no clock producer.
In this case, instead of returning -ENOENT, the function returns 0 and
NULL for a clk for which a clock producer could not be determined.


====================


int devm_clk_bulk_get(struct device *dev, int num_clks, struct clk_bulk_data *clks)
managed get multiple clk consumers

Parameters


struct device *dev
device for clock 'consumer'

int num_clks
the number of clk_bulk_data

struct clk_bulk_data *clks
the clk_bulk_data table of consumer

Description

Return 0 on success, an errno on failure.

This helper function allows drivers to get several clk
consumers in one operation with management, the clks will
automatically be freed when the device is unbound.


====================


int devm_clk_bulk_get_optional(struct device *dev, int num_clks, struct clk_bulk_data *clks)
managed get multiple optional consumer clocks

Parameters


struct device *dev
device for clock 'consumer'

int num_clks
the number of clk_bulk_data

struct clk_bulk_data *clks
pointer to the clk_bulk_data table of consumer

Description

Behaves the same as devm_clk_bulk_get() except where there is no clock
producer.  In this case, instead of returning -ENOENT, the function returns
NULL for given clk. It is assumed all clocks in clk_bulk_data are optional.

Returns 0 if all clocks specified in clk_bulk_data table are obtained
successfully or for any clk there was no clk provider available, otherwise
returns valid IS_ERR() condition containing errno.
The implementation uses dev and clk_bulk_data.id to determine the
clock consumer, and thereby the clock producer.
The clock returned is stored in each clk_bulk_data.clk field.

Drivers must assume that the clock source is not enabled.

clk_bulk_get should not be called from within interrupt context.


====================


int devm_clk_bulk_get_all(struct device *dev, struct clk_bulk_data **clks)
managed get multiple clk consumers

Parameters


struct device *dev
device for clock 'consumer'

struct clk_bulk_data **clks
pointer to the clk_bulk_data table of consumer

Description

Returns a positive value for the number of clocks obtained while the
clock references are stored in the clk_bulk_data table in clks field.
Returns 0 if there're none and a negative value if something failed.

This helper function allows drivers to get several clk
consumers in one operation with management, the clks will
automatically be freed when the device is unbound.


====================


struct clk * devm_clk_get(struct device *dev, const char *id)
lookup and obtain a managed reference to a clock producer.

Parameters


struct device *dev
device for clock 'consumer'

const char *id
clock consumer ID

Description

Returns a struct clk corresponding to the clock producer, or
valid IS_ERR() condition containing errno.  The implementation
uses dev and id to determine the clock consumer, and thereby
the clock producer.  (IOW, id may be identical strings, but
clk_get may return different clock producers depending on dev.)

Drivers must assume that the clock source is not enabled.

devm_clk_get should not be called from within interrupt context.

The clock will automatically be freed when the device is unbound
from the bus.


====================


struct clk * devm_clk_get_optional(struct device *dev, const char *id)
lookup and obtain a managed reference to an optional clock producer.

Parameters


struct device *dev
device for clock 'consumer'

const char *id
clock consumer ID

Description

Behaves the same as devm_clk_get() except where there is no clock producer.
In this case, instead of returning -ENOENT, the function returns NULL.


====================


struct clk * devm_get_clk_from_child(struct device *dev, struct device_node *np, const char *con_id)
lookup and obtain a managed reference to a clock producer from child node.

Parameters


struct device *dev
device for clock 'consumer'

struct device_node *np
pointer to clock consumer node

const char *con_id
clock consumer ID

Description

This function parses the clocks, and uses them to look up the
struct clk from the registered list of clock providers by using
np and con_id

The clock will automatically be freed when the device is unbound
from the bus.


====================


int clk_rate_exclusive_get(struct clk *clk)
get exclusivity over the rate control of a producer

Parameters


struct clk *clk
clock source

Description

This function allows drivers to get exclusive control over the rate of a
provider. It prevents any other consumer to execute, even indirectly,
opereation which could alter the rate of the provider or cause glitches

If exlusivity is claimed more than once on clock, even by the same driver,
the rate effectively gets locked as exclusivity can't be preempted.

Must not be called from within atomic context.

Returns success (0) or negative errno.


====================


void clk_rate_exclusive_put(struct clk *clk)
release exclusivity over the rate control of a producer

Parameters


struct clk *clk
clock source

Description

This function allows drivers to release the exclusivity it previously got
from clk_rate_exclusive_get()

The caller must balance the number of clk_rate_exclusive_get() and
clk_rate_exclusive_put() calls.

Must not be called from within atomic context.


====================


int clk_enable(struct clk *clk)
inform the system when the clock source should be running.

Parameters


struct clk *clk
clock source

Description

If the clock can not be enabled/disabled, this should return success.

May be called from atomic contexts.

Returns success (0) or negative errno.


====================


int clk_bulk_enable(int num_clks, const struct clk_bulk_data *clks)
inform the system when the set of clks should be running.

Parameters


int num_clks
the number of clk_bulk_data

const struct clk_bulk_data *clks
the clk_bulk_data table of consumer

Description

May be called from atomic contexts.

Returns success (0) or negative errno.


====================


void clk_disable(struct clk *clk)
inform the system when the clock source is no longer required.

Parameters


struct clk *clk
clock source

Description

Inform the system that a clock source is no longer required by
a driver and may be shut down.

May be called from atomic contexts.

Implementation detail: if the clock source is shared between
multiple drivers, clk_enable() calls must be balanced by the
same number of clk_disable() calls for the clock source to be
disabled.


====================


void clk_bulk_disable(int num_clks, const struct clk_bulk_data *clks)
inform the system when the set of clks is no longer required.

Parameters


int num_clks
the number of clk_bulk_data

const struct clk_bulk_data *clks
the clk_bulk_data table of consumer

Description

Inform the system that a set of clks is no longer required by
a driver and may be shut down.

May be called from atomic contexts.

Implementation detail: if the set of clks is shared between
multiple drivers, clk_bulk_enable() calls must be balanced by the
same number of clk_bulk_disable() calls for the clock source to be
disabled.


====================


unsigned long clk_get_rate(struct clk *clk)
obtain the current clock rate (in Hz) for a clock source. This is only valid once the clock source has been enabled.

Parameters


struct clk *clk
clock source


====================


void clk_put(struct clk *clk)
'free' the clock source

Parameters


struct clk *clk
clock source

Note

drivers must ensure that all clk_enable calls made on this
clock source are balanced by clk_disable calls prior to calling
this function.

Description

clk_put should not be called from within interrupt context.


====================


void clk_bulk_put(int num_clks, struct clk_bulk_data *clks)
'free' the clock source

Parameters


int num_clks
the number of clk_bulk_data

struct clk_bulk_data *clks
the clk_bulk_data table of consumer

Note

drivers must ensure that all clk_bulk_enable calls made on this
clock source are balanced by clk_bulk_disable calls prior to calling
this function.

Description

clk_bulk_put should not be called from within interrupt context.


====================


void clk_bulk_put_all(int num_clks, struct clk_bulk_data *clks)
'free' all the clock source

Parameters


int num_clks
the number of clk_bulk_data

struct clk_bulk_data *clks
the clk_bulk_data table of consumer

Note

drivers must ensure that all clk_bulk_enable calls made on this
clock source are balanced by clk_bulk_disable calls prior to calling
this function.

Description

clk_bulk_put_all should not be called from within interrupt context.


====================


void devm_clk_put(struct device *dev, struct clk *clk)
'free' a managed clock source

Parameters


struct device *dev
device used to acquire the clock

struct clk *clk
clock source acquired with devm_clk_get()

Note

drivers must ensure that all clk_enable calls made on this
clock source are balanced by clk_disable calls prior to calling
this function.

Description

clk_put should not be called from within interrupt context.


====================


long clk_round_rate(struct clk *clk, unsigned long rate)
adjust a rate to the exact rate a clock can provide

Parameters


struct clk *clk
clock source

unsigned long rate
desired clock rate in Hz

Description

This answers the question 'if I were to pass rate to clk_set_rate(),
what clock rate would I end up with?' without changing the hardware
in any way.  In other words:


rate = clk_round_rate(clk, r);

and:


clk_set_rate(clk, r);
rate = clk_get_rate(clk);

are equivalent except the former does not modify the clock hardware
in any way.

Returns rounded clock rate in Hz, or negative errno.


====================


int clk_set_rate(struct clk *clk, unsigned long rate)
set the clock rate for a clock source

Parameters


struct clk *clk
clock source

unsigned long rate
desired clock rate in Hz

Description

Updating the rate starts at the top-most affected clock and then
walks the tree down to the bottom-most clock that needs updating.

Returns success (0) or negative errno.


====================


int clk_set_rate_exclusive(struct clk *clk, unsigned long rate)
set the clock rate and claim exclusivity over clock source

Parameters


struct clk *clk
clock source

unsigned long rate
desired clock rate in Hz

Description

This helper function allows drivers to atomically set the rate of a producer
and claim exclusivity over the rate control of the producer.

It is essentially a combination of clk_set_rate() and
clk_rate_exclusite_get(). Caller must balance this call with a call to
clk_rate_exclusive_put()

Returns success (0) or negative errno.


====================


bool clk_has_parent(struct clk *clk, struct clk *parent)
check if a clock is a possible parent for another

Parameters


struct clk *clk
clock source

struct clk *parent
parent clock source

Description

This function can be used in drivers that need to check that a clock can be
the parent of another without actually changing the parent.

Returns true if parent is a possible parent for clk, false otherwise.


====================


int clk_set_rate_range(struct clk *clk, unsigned long min, unsigned long max)
set a rate range for a clock source

Parameters


struct clk *clk
clock source

unsigned long min
desired minimum clock rate in Hz, inclusive

unsigned long max
desired maximum clock rate in Hz, inclusive

Description

Returns success (0) or negative errno.


====================


int clk_set_min_rate(struct clk *clk, unsigned long rate)
set a minimum clock rate for a clock source

Parameters


struct clk *clk
clock source

unsigned long rate
desired minimum clock rate in Hz, inclusive

Description

Returns success (0) or negative errno.


====================


int clk_set_max_rate(struct clk *clk, unsigned long rate)
set a maximum clock rate for a clock source

Parameters


struct clk *clk
clock source

unsigned long rate
desired maximum clock rate in Hz, inclusive

Description

Returns success (0) or negative errno.


====================


int clk_set_parent(struct clk *clk, struct clk *parent)
set the parent clock source for this clock

Parameters


struct clk *clk
clock source

struct clk *parent
parent clock source

Description

Returns success (0) or negative errno.


====================


struct clk * clk_get_parent(struct clk *clk)
get the parent clock source for this clock

Parameters


struct clk *clk
clock source

Description

Returns struct clk corresponding to parent clock source, or
valid IS_ERR() condition containing errno.


====================


struct clk * clk_get_sys(const char *dev_id, const char *con_id)
get a clock based upon the device name

Parameters


const char *dev_id
device name

const char *con_id
connection ID

Description

Returns a struct clk corresponding to the clock producer, or
valid IS_ERR() condition containing errno.  The implementation
uses dev_id and con_id to determine the clock consumer, and
thereby the clock producer. In contrast to clk_get() this function
takes the device name instead of the device itself for identification.

Drivers must assume that the clock source is not enabled.

clk_get_sys should not be called from within interrupt context.


====================


int clk_save_context(void)
save clock context for poweroff

Parameters


void
no arguments

Description

Saves the context of the clock register for powerstates in which the
contents of the registers will be lost. Occurs deep within the suspend
code so locking is not necessary.


====================


void clk_restore_context(void)
restore clock context after poweroff

Parameters


void
no arguments

Description

This occurs with all clocks enabled. Occurs deep within the resume code
so locking is not necessary.


====================


int clk_drop_range(struct clk *clk)
Reset any range set on that clock

Parameters


struct clk *clk
clock source

Description

Returns success (0) or negative errno.


====================


struct clk * clk_get_optional(struct device *dev, const char *id)
lookup and obtain a reference to an optional clock producer.

Parameters


struct device *dev
device for clock 'consumer'

const char *id
clock consumer ID

Description

Behaves the same as clk_get() except where there is no clock producer. In
this case, instead of returning -ENOENT, the function returns NULL.


====================


RCU_NONIDLE(a)
Indicate idle-loop code that needs RCU readers

Parameters


a
Code that RCU needs to pay attention to.

Description

RCU read-side critical sections are forbidden in the inner idle loop,
that is, between the rcu_idle_enter() and the rcu_idle_exit() ... RCU
will happily ignore any such read-side critical sections.  However,
things like powertop need tracepoints in the inner idle loop.

This macro provides the way out:  RCU_NONIDLE(do_something_with_RCU())
will tell RCU that it needs to pay attention, invoke its argument
(in this example, calling the do_something_with_RCU() function),
and then tell RCU to go back to ignoring this CPU.  It is permissible
to nest RCU_NONIDLE() wrappers, but not indefinitely (but the limit is
on the order of a million or so, even on 32-bit systems).  It is
not legal to block within RCU_NONIDLE(), nor is it permissible to
transfer control either into or out of RCU_NONIDLE()'s statement.


====================


cond_resched_tasks_rcu_qs()
Report potential quiescent states to RCU

Parameters

Description

This macro resembles cond_resched(), except that it is defined to
report potential quiescent states to RCU-tasks even if the cond_resched()
machinery were to be shut off, as some advocate for PREEMPTION kernels.


====================


RCU_LOCKDEP_WARN(c, s)
emit lockdep splat if specified condition is met

Parameters


c
condition to check

s
informative message


====================


unrcu_pointer(p)
mark a pointer as not being RCU protected

Parameters


p
pointer needing to lose its __rcu property

Description

Converts p from an __rcu pointer to a __kernel pointer.
This allows an __rcu pointer to be used with xchg() and friends.


====================


RCU_INITIALIZER(v)
statically initialize an RCU-protected global variable

Parameters


v
The value to statically initialize with.


====================


rcu_assign_pointer(p, v)
assign to RCU-protected pointer

Parameters


p
pointer to assign to

v
value to assign (publish)

Description

Assigns the specified value to the specified RCU-protected
pointer, ensuring that any concurrent RCU readers will see
any prior initialization.

Inserts memory barriers on architectures that require them
(which is most of them), and also prevents the compiler from
reordering the code that initializes the structure after the pointer
assignment.  More importantly, this call documents which pointers
will be dereferenced by RCU read-side code.

In some special cases, you may use RCU_INIT_POINTER() instead
of rcu_assign_pointer().  RCU_INIT_POINTER() is a bit faster due
to the fact that it does not constrain either the CPU or the compiler.
That said, using RCU_INIT_POINTER() when you should have used
rcu_assign_pointer() is a very bad thing that results in
impossible-to-diagnose memory corruption.  So please be careful.
See the RCU_INIT_POINTER() comment header for details.

Note that rcu_assign_pointer() evaluates each of its arguments only
once, appearances notwithstanding.  One of the 'extra' evaluations
is in typeof() and the other visible only to sparse (__CHECKER__),
neither of which actually execute the argument.  As with most cpp
macros, this execute-arguments-only-once property is important, so
please be careful when making changes to rcu_assign_pointer() and the
other macros that it invokes.


====================


rcu_replace_pointer(rcu_ptr, ptr, c)
replace an RCU pointer, returning its old value

Parameters


rcu_ptr
RCU pointer, whose old value is returned

ptr
regular pointer

c
the lockdep conditions under which the dereference will take place

Description

Perform a replacement, where rcu_ptr is an RCU-annotated
pointer and c is the lockdep argument that is passed to the
rcu_dereference_protected() call used to read that pointer.  The old
value of rcu_ptr is returned, and rcu_ptr is set to ptr.


====================


rcu_access_pointer(p)
fetch RCU pointer with no dereferencing

Parameters


p
The pointer to read

Description

Return the value of the specified RCU-protected pointer, but omit the
lockdep checks for being in an RCU read-side critical section.  This is
useful when the value of this pointer is accessed, but the pointer is
not dereferenced, for example, when testing an RCU-protected pointer
against NULL.  Although rcu_access_pointer() may also be used in cases
where update-side locks prevent the value of the pointer from changing,
you should instead use rcu_dereference_protected() for this use case.

It is also permissible to use rcu_access_pointer() when read-side
access to the pointer was removed at least one grace period ago, as
is the case in the context of the RCU callback that is freeing up
the data, or after a synchronize_rcu() returns.  This can be useful
when tearing down multi-linked structures after a grace period
has elapsed.


====================


rcu_dereference_check(p, c)
rcu_dereference with debug checking

Parameters


p
The pointer to read, prior to dereferencing

c
The conditions under which the dereference will take place

Description

Do an rcu_dereference(), but check that the conditions under which the
dereference will take place are correct.  Typically the conditions
indicate the various locking conditions that should be held at that
point.  The check should return true if the conditions are satisfied.
An implicit check for being in an RCU read-side critical section
(rcu_read_lock()) is included.

For example:


bar = rcu_dereference_check(foo->bar, lockdep_is_held(foo->lock));

could be used to indicate to lockdep that foo->bar may only be dereferenced
if either rcu_read_lock() is held, or that the lock required to replace
the bar struct at foo->bar is held.

Note that the list of conditions may also include indications of when a lock
need not be held, for example during initialisation or destruction of the
target struct:



bar = rcu_dereference_check(foo->bar, lockdep_is_held(foo->lock) ||atomic_read(foo->usage) == 0);



Inserts memory barriers on architectures that require them
(currently only the Alpha), prevents the compiler from refetching
(and from merging fetches), and, more importantly, documents exactly
which pointers are protected by RCU and checks that the pointer is
annotated as __rcu.


====================


rcu_dereference_bh_check(p, c)
rcu_dereference_bh with debug checking

Parameters


p
The pointer to read, prior to dereferencing

c
The conditions under which the dereference will take place

Description

This is the RCU-bh counterpart to rcu_dereference_check().  However,
please note that starting in v5.0 kernels, vanilla RCU grace periods
wait for local_bh_disable() regions of code in addition to regions of
code demarked by rcu_read_lock() and rcu_read_unlock().  This means
that synchronize_rcu(), call_rcu, and friends all take not only
rcu_read_lock() but also rcu_read_lock_bh() into account.


====================


rcu_dereference_sched_check(p, c)
rcu_dereference_sched with debug checking

Parameters


p
The pointer to read, prior to dereferencing

c
The conditions under which the dereference will take place

Description

This is the RCU-sched counterpart to rcu_dereference_check().
However, please note that starting in v5.0 kernels, vanilla RCU grace
periods wait for preempt_disable() regions of code in addition to
regions of code demarked by rcu_read_lock() and rcu_read_unlock().
This means that synchronize_rcu(), call_rcu, and friends all take not
only rcu_read_lock() but also rcu_read_lock_sched() into account.


====================


rcu_dereference_protected(p, c)
fetch RCU pointer when updates prevented

Parameters


p
The pointer to read, prior to dereferencing

c
The conditions under which the dereference will take place

Description

Return the value of the specified RCU-protected pointer, but omit
the READ_ONCE().  This is useful in cases where update-side locks
prevent the value of the pointer from changing.  Please note that this
primitive does not prevent the compiler from repeating this reference
or combining it with other references, so it should not be used without
protection of appropriate locks.

This function is only for update-side use.  Using this function
when protected only by rcu_read_lock() will result in infrequent
but very ugly failures.


====================


rcu_dereference(p)
fetch RCU-protected pointer for dereferencing

Parameters


p
The pointer to read, prior to dereferencing

Description

This is a simple wrapper around rcu_dereference_check().


====================


rcu_dereference_bh(p)
fetch an RCU-bh-protected pointer for dereferencing

Parameters


p
The pointer to read, prior to dereferencing

Description

Makes rcu_dereference_check() do the dirty work.


====================


rcu_dereference_sched(p)
fetch RCU-sched-protected pointer for dereferencing

Parameters


p
The pointer to read, prior to dereferencing

Description

Makes rcu_dereference_check() do the dirty work.


====================


rcu_pointer_handoff(p)
Hand off a pointer from RCU to other mechanism

Parameters


p
The pointer to hand off

Description

This is simply an identity function, but it documents where a pointer
is handed off from RCU to some other synchronization mechanism, for
example, reference counting or locking.  In C11, it would map to
kill_dependency().  It could be used as follows:

rcu_read_lock();
p = rcu_dereference(gp);
long_lived = is_long_lived(p);
if (long_lived) {
        if (!atomic_inc_not_zero(p->refcnt))
                long_lived = false;
        else
                p = rcu_pointer_handoff(p);
}
rcu_read_unlock();



====================


void rcu_read_lock(void)
mark the beginning of an RCU read-side critical section

Parameters


void
no arguments

Description

When synchronize_rcu() is invoked on one CPU while other CPUs
are within RCU read-side critical sections, then the
synchronize_rcu() is guaranteed to block until after all the other
CPUs exit their critical sections.  Similarly, if call_rcu() is invoked
on one CPU while other CPUs are within RCU read-side critical
sections, invocation of the corresponding RCU callback is deferred
until after the all the other CPUs exit their critical sections.

In v5.0 and later kernels, synchronize_rcu() and call_rcu() also
wait for regions of code with preemption disabled, including regions of
code with interrupts or softirqs disabled.  In pre-v5.0 kernels, which
define synchronize_sched(), only code enclosed within rcu_read_lock()
and rcu_read_unlock() are guaranteed to be waited for.

Note, however, that RCU callbacks are permitted to run concurrently
with new RCU read-side critical sections.  One way that this can happen
is via the following sequence of events: (1) CPU 0 enters an RCU
read-side critical section, (2) CPU 1 invokes call_rcu() to register
an RCU callback, (3) CPU 0 exits the RCU read-side critical section,
(4) CPU 2 enters a RCU read-side critical section, (5) the RCU
callback is invoked.  This is legal, because the RCU read-side critical
section that was running concurrently with the call_rcu() (and which
therefore might be referencing something that the corresponding RCU
callback would free up) has completed before the corresponding
RCU callback is invoked.

RCU read-side critical sections may be nested.  Any deferred actions
will be deferred until the outermost RCU read-side critical section
completes.

You can avoid reading and understanding the next paragraph by
following this rule: don't put anything in an rcu_read_lock() RCU
read-side critical section that would block in a !PREEMPTION kernel.
But if you want the full story, read on!

In non-preemptible RCU implementations (pure TREE_RCU and TINY_RCU),
it is illegal to block while in an RCU read-side critical section.
In preemptible RCU implementations (PREEMPT_RCU) in CONFIG_PREEMPTION
kernel builds, RCU read-side critical sections may be preempted,
but explicit blocking is illegal.  Finally, in preemptible RCU
implementations in real-time (with -rt patchset) kernel builds, RCU
read-side critical sections may be preempted and they may also block, but
only when acquiring spinlocks that are subject to priority inheritance.


====================


void rcu_read_unlock(void)
marks the end of an RCU read-side critical section.

Parameters


void
no arguments

Description

In almost all situations, rcu_read_unlock() is immune from deadlock.
In recent kernels that have consolidated synchronize_sched() and
synchronize_rcu_bh() into synchronize_rcu(), this deadlock immunity
also extends to the scheduler's runqueue and priority-inheritance
spinlocks, courtesy of the quiescent-state deferral that is carried
out when rcu_read_unlock() is invoked with interrupts disabled.

See rcu_read_lock() for more information.


====================


void rcu_read_lock_bh(void)
mark the beginning of an RCU-bh critical section

Parameters


void
no arguments

Description

This is equivalent to rcu_read_lock(), but also disables softirqs.
Note that anything else that disables softirqs can also serve as an RCU
read-side critical section.  However, please note that this equivalence
applies only to v5.0 and later.  Before v5.0, rcu_read_lock() and
rcu_read_lock_bh() were unrelated.

Note that rcu_read_lock_bh() and the matching rcu_read_unlock_bh()
must occur in the same context, for example, it is illegal to invoke
rcu_read_unlock_bh() from one task if the matching rcu_read_lock_bh()
was invoked from some other task.


====================


void rcu_read_unlock_bh(void)
marks the end of a softirq-only RCU critical section

Parameters


void
no arguments

Description

See rcu_read_lock_bh() for more information.


====================


void rcu_read_lock_sched(void)
mark the beginning of a RCU-sched critical section

Parameters


void
no arguments

Description

This is equivalent to rcu_read_lock(), but also disables preemption.
Read-side critical sections can also be introduced by anything else that
disables preemption, including local_irq_disable() and friends.  However,
please note that the equivalence to rcu_read_lock() applies only to
v5.0 and later.  Before v5.0, rcu_read_lock() and rcu_read_lock_sched()
were unrelated.

Note that rcu_read_lock_sched() and the matching rcu_read_unlock_sched()
must occur in the same context, for example, it is illegal to invoke
rcu_read_unlock_sched() from process context if the matching
rcu_read_lock_sched() was invoked from an NMI handler.


====================


void rcu_read_unlock_sched(void)
marks the end of a RCU-classic critical section

Parameters


void
no arguments

Description

See rcu_read_lock_sched() for more information.


====================


RCU_INIT_POINTER(p, v)
initialize an RCU protected pointer

Parameters


p
The pointer to be initialized.

v
The value to initialized the pointer to.

Description

Initialize an RCU-protected pointer in special cases where readers
do not need ordering constraints on the CPU or the compiler.  These
special cases are:


This use of RCU_INIT_POINTER() is NULLing out the pointer or
The caller has taken whatever steps are required to prevent
RCU readers from concurrently accessing this pointer or
The referenced data structure has already been exposed to
readers either at compile time or via rcu_assign_pointer() and

You have not made any reader-visible changes to
this structure since then or
It is OK for readers accessing this structure from its
new location to see the old state of the structure.  (For
example, the changes were to statistical counters or to
other state where exact synchronization is not required.)



Failure to follow these rules governing use of RCU_INIT_POINTER() will
result in impossible-to-diagnose memory corruption.  As in the structures
will look OK in crash dumps, but any concurrent RCU readers might
see pre-initialized values of the referenced data structure.  So
please be very careful how you use RCU_INIT_POINTER()!!!

If you are creating an RCU-protected linked structure that is accessed
by a single external-to-structure RCU-protected pointer, then you may
use RCU_INIT_POINTER() to initialize the internal RCU-protected
pointers, but you must use rcu_assign_pointer() to initialize the
external-to-structure pointer after you have completely initialized
the reader-accessible portions of the linked structure.

Note that unlike rcu_assign_pointer(), RCU_INIT_POINTER() provides no
ordering guarantees for either the CPU or the compiler.


====================


RCU_POINTER_INITIALIZER(p, v)
statically initialize an RCU protected pointer

Parameters


p
The pointer to be initialized.

v
The value to initialized the pointer to.

Description

GCC-style initialization for an RCU-protected pointer in a structure field.


====================


kfree_rcu(ptr, rhf...¦)
kfree an object after a grace period.

Parameters


ptr
pointer to kfree for both single- and double-argument invocations.

rhf...
the name of the struct rcu_head within the type of ptr,
but only for double-argument invocations.

Description

Many rcu callbacks functions just call kfree() on the base structure.
These functions are trivial, but their size adds up, and furthermore
when they are used in a kernel module, that module must invoke the
high-latency rcu_barrier() function at module-unload time.

The kfree_rcu() function handles this issue.  Rather than encoding a
function address in the embedded rcu_head structure, kfree_rcu() instead
encodes the offset of the rcu_head structure within the base structure.
Because the functions are not allowed in the low-order 4096 bytes of
kernel virtual memory, offsets up to 4095 bytes can be accommodated.
If the offset is larger than 4095 bytes, a compile-time error will
be generated in kvfree_rcu_arg_2(). If this error is triggered, you can
either fall back to use of call_rcu() or rearrange the structure to
position the rcu_head structure into the first 4096 bytes.

Note that the allowable offset might decrease in the future, for example,
to allow something like kmem_cache_free_rcu().

The BUILD_BUG_ON check must not involve any function calls, hence the
checks are done in macros here.


====================


kvfree_rcu(...¦)
kvfree an object after a grace period.

Parameters


...
variable arguments

Description

This macro consists of one or two arguments and it is
based on whether an object is head-less or not. If it
has a head then a semantic stays the same as it used
to be before:


kvfree_rcu(ptr, rhf);

where ptr is a pointer to kvfree(), rhf is the name
of the rcu_head structure within the type of ptr.

When it comes to head-less variant, only one argument
is passed and that is just a pointer which has to be
freed after a grace period. Therefore the semantic is


kvfree_rcu(ptr);

where ptr is the pointer to be freed by kvfree().

Please note, head-less way of freeing is permitted to
use from a context that has to follow might_sleep()
annotation. Otherwise, please switch and embed the
rcu_head structure within the type of ptr.


====================


void rcu_head_init(struct rcu_head *rhp)
Initialize rcu_head for rcu_head_after_call_rcu()

Parameters


struct rcu_head *rhp
The rcu_head structure to initialize.

Description

If you intend to invoke rcu_head_after_call_rcu() to test whether a
given rcu_head structure has already been passed to call_rcu(), then
you must also invoke this rcu_head_init() function on it just after
allocating that structure.  Calls to this function must not race with
calls to call_rcu(), rcu_head_after_call_rcu(), or callback invocation.


====================


bool rcu_head_after_call_rcu(struct rcu_head *rhp, rcu_callback_t f)
Has this rcu_head been passed to call_rcu()?

Parameters


struct rcu_head *rhp
The rcu_head structure to test.

rcu_callback_t f
The function passed to call_rcu() along with rhp.

Description

Returns true if the rhp has been passed to call_rcu() with func,
and false otherwise.  Emits a warning in any other case, including
the case where rhp has already been invoked after a grace period.
Calls to this function must not race with callback invocation.  One way
to avoid such races is to enclose the call to rcu_head_after_call_rcu()
in an RCU read-side critical section that includes a read-side fetch
of the pointer to the structure containing rhp.


====================


int rcu_is_cpu_rrupt_from_idle(void)
see if 'interrupted' from idle

Parameters


void
no arguments

Description

If the current CPU is idle and running at a first-level (not nested)
interrupt, or directly, from idle, return true.

The caller must have at least disabled IRQs.


====================


void rcu_idle_enter(void)
inform RCU that current CPU is entering idle

Parameters


void
no arguments

Description

Enter idle mode, in other words, -leave- the mode in which RCU
read-side critical sections can occur.  (Though RCU read-side
critical sections can occur in irq handlers in idle, a possibility
handled by irq_enter() and irq_exit().)

If you add or remove a call to rcu_idle_enter(), be sure to test with
CONFIG_RCU_EQS_DEBUG=y.


====================


noinstr void rcu_user_enter(void)
inform RCU that we are resuming userspace.

Parameters


void
no arguments

Description

Enter RCU idle mode right before resuming userspace.  No use of RCU
is permitted between this call and rcu_user_exit(). This way the
CPU doesn't need to maintain the tick for RCU maintenance purposes
when the CPU runs in userspace.

If you add or remove a call to rcu_user_enter(), be sure to test with
CONFIG_RCU_EQS_DEBUG=y.


====================


noinstr void rcu_nmi_exit(void)
inform RCU of exit from NMI context

Parameters


void
no arguments

Description

If we are returning from the outermost NMI handler that interrupted an
RCU-idle period, update rdp->dynticks and rdp->dynticks_nmi_nesting
to let the RCU grace-period handling know that the CPU is back to
being RCU-idle.

If you add or remove a call to rcu_nmi_exit(), be sure to test
with CONFIG_RCU_EQS_DEBUG=y.


====================


void noinstr rcu_irq_exit(void)
inform RCU that current CPU is exiting irq towards idle

Parameters


void
no arguments

Description

Exit from an interrupt handler, which might possibly result in entering
idle mode, in other words, leaving the mode in which read-side critical
sections can occur.  The caller must have disabled interrupts.

This code assumes that the idle loop never does anything that might
result in unbalanced calls to irq_enter() and irq_exit().  If your
architecture's idle loop violates this assumption, RCU will give you what
you deserve, good and hard.  But very infrequently and irreproducibly.

Use things like work queues to work around this limitation.

You have been warned.

If you add or remove a call to rcu_irq_exit(), be sure to test with
CONFIG_RCU_EQS_DEBUG=y.


====================


void rcu_irq_exit_check_preempt(void)
Validate that scheduling is possible

Parameters


void
no arguments


====================


void rcu_idle_exit(void)
inform RCU that current CPU is leaving idle

Parameters


void
no arguments

Description

Exit idle mode, in other words, -enter- the mode in which RCU
read-side critical sections can occur.

If you add or remove a call to rcu_idle_exit(), be sure to test with
CONFIG_RCU_EQS_DEBUG=y.


====================


void noinstr rcu_user_exit(void)
inform RCU that we are exiting userspace.

Parameters


void
no arguments

Description

Exit RCU idle mode while entering the kernel because it can
run a RCU read side critical section anytime.

If you add or remove a call to rcu_user_exit(), be sure to test with
CONFIG_RCU_EQS_DEBUG=y.


====================


void __rcu_irq_enter_check_tick(void)
Enable scheduler tick on CPU if RCU needs it.

Parameters


void
no arguments

Description

The scheduler tick is not normally enabled when CPUs enter the kernel
from nohz_full userspace execution.  After all, nohz_full userspace
execution is an RCU quiescent state and the time executing in the kernel
is quite short.  Except of course when it isn't.  And it is not hard to
cause a large system to spend tens of seconds or even minutes looping
in the kernel, which can cause a number of problems, include RCU CPU
stall warnings.

Therefore, if a nohz_full CPU fails to report a quiescent state
in a timely manner, the RCU grace-period kthread sets that CPU's
->rcu_urgent_qs flag with the expectation that the next interrupt or
exception will invoke this function, which will turn on the scheduler
tick, which will enable RCU to detect that CPU's quiescent states,
for example, due to cond_resched() calls in CONFIG_PREEMPT=n kernels.
The tick will be disabled once a quiescent state is reported for
this CPU.

Of course, in carefully tuned systems, there might never be an
interrupt or exception.  In that case, the RCU grace-period kthread
will eventually cause one to happen.  However, in less carefully
controlled environments, this function allows RCU to get what it
needs without creating otherwise useless interruptions.


====================


noinstr void rcu_nmi_enter(void)
inform RCU of entry to NMI context

Parameters


void
no arguments

Description

If the CPU was idle from RCU's viewpoint, update rdp->dynticks and
rdp->dynticks_nmi_nesting to let the RCU grace-period handling know
that the CPU is active.  This implementation permits nested NMIs, as
long as the nesting level does not overflow an int.  (You will probably
run out of stack space first.)

If you add or remove a call to rcu_nmi_enter(), be sure to test
with CONFIG_RCU_EQS_DEBUG=y.


====================


noinstr void rcu_irq_enter(void)
inform RCU that current CPU is entering irq away from idle

Parameters


void
no arguments

Description

Enter an interrupt handler, which might possibly result in exiting
idle mode, in other words, entering the mode in which read-side critical
sections can occur.  The caller must have disabled interrupts.

Note that the Linux kernel is fully capable of entering an interrupt
handler that it never exits, for example when doing upcalls to user mode!
This code assumes that the idle loop never does upcalls to user mode.
If your architecture's idle loop does do upcalls to user mode (or does
anything else that results in unbalanced calls to the irq_enter() and
irq_exit() functions), RCU will give you what you deserve, good and hard.
But very infrequently and irreproducibly.

Use things like work queues to work around this limitation.

You have been warned.

If you add or remove a call to rcu_irq_enter(), be sure to test with
CONFIG_RCU_EQS_DEBUG=y.


====================


notrace bool rcu_is_watching(void)
see if RCU thinks that the current CPU is not idle

Parameters


void
no arguments

Description

Return true if RCU is watching the running CPU, which means that this
CPU can safely enter RCU read-side critical sections.  In other words,
if the current CPU is not in its idle loop or is in an interrupt or
NMI handler, return true.

Make notrace because it can be called by the internal functions of
ftrace, and making this notrace removes unnecessary recursion calls.


====================


void call_rcu(struct rcu_head *head, rcu_callback_t func)
Queue an RCU callback for invocation after a grace period.

Parameters


struct rcu_head *head
structure to be used for queueing the RCU updates.

rcu_callback_t func
actual callback function to be invoked after the grace period

Description

The callback function will be invoked some time after a full grace
period elapses, in other words after all pre-existing RCU read-side
critical sections have completed.  However, the callback function
might well execute concurrently with RCU read-side critical sections
that started after call_rcu() was invoked.

RCU read-side critical sections are delimited by rcu_read_lock()
and rcu_read_unlock(), and may be nested.  In addition, but only in
v5.0 and later, regions of code across which interrupts, preemption,
or softirqs have been disabled also serve as RCU read-side critical
sections.  This includes hardware interrupt handlers, softirq handlers,
and NMI handlers.

Note that all CPUs must agree that the grace period extended beyond
all pre-existing RCU read-side critical section.  On systems with more
than one CPU, this means that when 'func()' is invoked, each CPU is
guaranteed to have executed a full memory barrier since the end of its
last RCU read-side critical section whose beginning preceded the call
to call_rcu().  It also means that each CPU executing an RCU read-side
critical section that continues beyond the start of 'func()' must have
executed a memory barrier after the call_rcu() but before the beginning
of that RCU read-side critical section.  Note that these guarantees
include CPUs that are offline, idle, or executing in user mode, as
well as CPUs that are executing in the kernel.

Furthermore, if CPU A invoked call_rcu() and CPU B invoked the
resulting RCU callback function 'func()', then both CPU A and CPU B are
guaranteed to execute a full memory barrier during the time interval
between the call to call_rcu() and the invocation of 'func()' ... even
if CPU A and CPU B are the same CPU (but again only if the system has
more than one CPU).

Implementation of these memory-ordering guarantees is described here:
A Tour Through TREE_RCU's Grace-Period Memory Ordering.



struct kvfree_rcu_bulk_data
single block to store kvfree_rcu() pointers

Definition

struct kvfree_rcu_bulk_data {
  unsigned long nr_records;
  struct kvfree_rcu_bulk_data *next;
  void *records[];
};


Members


nr_records
Number of active pointers in the array

next
Next bulk object in the block chain

records
Array of the kvfree_rcu() pointers



struct kfree_rcu_cpu_work
single batch of kfree_rcu() requests

Definition

struct kfree_rcu_cpu_work {
  struct rcu_work rcu_work;
  struct rcu_head *head_free;
  struct kvfree_rcu_bulk_data *bkvhead_free[FREE_N_CHANNELS];
  struct kfree_rcu_cpu *krcp;
};


Members


rcu_work
Let queue_rcu_work() invoke workqueue handler after grace period

head_free
List of kfree_rcu() objects waiting for a grace period

bkvhead_free
Bulk-List of kvfree_rcu() objects waiting for a grace period

krcp
Pointer to kfree_rcu_cpu structure



struct kfree_rcu_cpu
batch up kfree_rcu() requests for RCU grace period

Definition

struct kfree_rcu_cpu {
  struct rcu_head *head;
  struct kvfree_rcu_bulk_data *bkvhead[FREE_N_CHANNELS];
  struct kfree_rcu_cpu_work krw_arr[KFREE_N_BATCHES];
  raw_spinlock_t lock;
  struct delayed_work monitor_work;
  bool monitor_todo;
  bool initialized;
  int count;
  struct delayed_work page_cache_work;
  atomic_t backoff_page_cache_fill;
  atomic_t work_in_progress;
  struct hrtimer hrtimer;
  struct llist_head bkvcache;
  int nr_bkv_objs;
};


Members


head
List of kfree_rcu() objects not yet waiting for a grace period

bkvhead
Bulk-List of kvfree_rcu() objects not yet waiting for a grace period

krw_arr
Array of batches of kfree_rcu() objects waiting for a grace period

lock
Synchronize access to this structure

monitor_work
Promote head to head_free after KFREE_DRAIN_JIFFIES

monitor_todo
Tracks whether a monitor_work delayed work is pending

initialized
The rcu_work fields have been initialized

count
Number of objects for which GP not started

page_cache_work
A work to refill the cache when it is empty

backoff_page_cache_fill
Delay cache refills

work_in_progress
Indicates that page_cache_work is running

hrtimer
A hrtimer for scheduling a page_cache_work

bkvcache


A simple cache list that contains objects for reuse purpose.
In order to save some per-cpu space the list is singular.
Even though it is lockless an access has to be protected by the
per-cpu lock.


nr_bkv_objs
number of allocated objects at bkvcache.

Description

This is a per-CPU structure.  The reason that it is not included in
the rcu_data structure is to permit this code to be extracted from
the RCU files.  Such extraction could allow further optimization of
the interactions with the slab allocators.


====================


void synchronize_rcu(void)
wait until a grace period has elapsed.

Parameters


void
no arguments

Description

Control will return to the caller some time after a full grace
period has elapsed, in other words after all currently executing RCU
read-side critical sections have completed.  Note, however, that
upon return from synchronize_rcu(), the caller might well be executing
concurrently with new RCU read-side critical sections that began while
synchronize_rcu() was waiting.

RCU read-side critical sections are delimited by rcu_read_lock()
and rcu_read_unlock(), and may be nested.  In addition, but only in
v5.0 and later, regions of code across which interrupts, preemption,
or softirqs have been disabled also serve as RCU read-side critical
sections.  This includes hardware interrupt handlers, softirq handlers,
and NMI handlers.

Note that this guarantee implies further memory-ordering guarantees.
On systems with more than one CPU, when synchronize_rcu() returns,
each CPU is guaranteed to have executed a full memory barrier since
the end of its last RCU read-side critical section whose beginning
preceded the call to synchronize_rcu().  In addition, each CPU having
an RCU read-side critical section that extends beyond the return from
synchronize_rcu() is guaranteed to have executed a full memory barrier
after the beginning of synchronize_rcu() and before the beginning of
that RCU read-side critical section.  Note that these guarantees include
CPUs that are offline, idle, or executing in user mode, as well as CPUs
that are executing in the kernel.

Furthermore, if CPU A invoked synchronize_rcu(), which returned
to its caller on CPU B, then both CPU A and CPU B are guaranteed
to have executed a full memory barrier during the execution of
synchronize_rcu() ... even if CPU A and CPU B are the same CPU (but
again only if the system has more than one CPU).

Implementation of these memory-ordering guarantees is described here:
A Tour Through TREE_RCU's Grace-Period Memory Ordering.


====================


unsigned long get_state_synchronize_rcu(void)
Snapshot current RCU state

Parameters


void
no arguments

Description

Returns a cookie that is used by a later call to cond_synchronize_rcu()
or poll_state_synchronize_rcu() to determine whether or not a full
grace period has elapsed in the meantime.


====================


unsigned long start_poll_synchronize_rcu(void)
Snapshot and start RCU grace period

Parameters


void
no arguments

Description

Returns a cookie that is used by a later call to cond_synchronize_rcu()
or poll_state_synchronize_rcu() to determine whether or not a full
grace period has elapsed in the meantime.  If the needed grace period
is not already slated to start, notifies RCU core of the need for that
grace period.

Interrupts must be enabled for the case where it is necessary to awaken
the grace-period kthread.


====================


bool poll_state_synchronize_rcu(unsigned long oldstate)
Conditionally wait for an RCU grace period

Parameters


unsigned long oldstate
value from get_state_synchronize_rcu() or start_poll_synchronize_rcu()

Description

If a full RCU grace period has elapsed since the earlier call from
which oldstate was obtained, return true, otherwise return false.
If false is returned, it is the caller's responsibility to invoke this
function later on until it does return true.  Alternatively, the caller
can explicitly wait for a grace period, for example, by passing oldstate
to cond_synchronize_rcu() or by directly invoking synchronize_rcu().

Yes, this function does not take counter wrap into account.
But counter wrap is harmless.  If the counter wraps, we have waited for
more than 2 billion grace periods (and way more on a 64-bit system!).
Those needing to keep oldstate values for very long time periods
(many hours even on 32-bit systems) should check them occasionally
and either refresh them or set a flag indicating that the grace period
has completed.

This function provides the same memory-ordering guarantees that
would be provided by a synchronize_rcu() that was invoked at the call
to the function that provided oldstate, and that returned at the end
of this function.


====================


void cond_synchronize_rcu(unsigned long oldstate)
Conditionally wait for an RCU grace period

Parameters


unsigned long oldstate
value from get_state_synchronize_rcu() or start_poll_synchronize_rcu()

Description

If a full RCU grace period has elapsed since the earlier call to
get_state_synchronize_rcu() or start_poll_synchronize_rcu(), just return.
Otherwise, invoke synchronize_rcu() to wait for a full grace period.

Yes, this function does not take counter wrap into account.  But
counter wrap is harmless.  If the counter wraps, we have waited for
more than 2 billion grace periods (and way more on a 64-bit system!),
so waiting for one additional grace period should be just fine.

This function provides the same memory-ordering guarantees that
would be provided by a synchronize_rcu() that was invoked at the call
to the function that provided oldstate, and that returned at the end
of this function.


====================


void rcu_barrier(void)
Wait until all in-flight call_rcu() callbacks complete.

Parameters


void
no arguments

Description

Note that this primitive does not necessarily wait for an RCU grace period
to complete.  For example, if there are no RCU callbacks queued anywhere
in the system, then rcu_barrier() is within its rights to return
immediately, without waiting for anything, much less an RCU grace period.


====================


void synchronize_rcu_expedited(void)
Brute-force RCU grace period

Parameters


void
no arguments

Description

Wait for an RCU grace period, but expedite it.  The basic idea is to
IPI all non-idle non-nohz online CPUs.  The IPI handler checks whether
the CPU is in an RCU critical section, and if so, it sets a flag that
causes the outermost rcu_read_unlock() to report the quiescent state
for RCU-preempt or asks the scheduler for help for RCU-sched.  On the
other hand, if the CPU is not in an RCU read-side critical section,
the IPI handler reports the quiescent state immediately.

Although this is a great improvement over previous expedited
implementations, it is still unfriendly to real-time workloads, so is
thus not recommended for any sort of common-case code.  In fact, if
you are using synchronize_rcu_expedited() in a loop, please restructure
your code to batch your updates, and then use a single synchronize_rcu()
instead.

This has the same semantics as (but is more brutal than) synchronize_rcu().


====================


bool rcu_read_lock_held_common(bool *ret)
might we be in RCU-sched read-side critical section?

Parameters


bool *ret
Best guess answer if lockdep cannot be relied on

Description

Returns true if lockdep must be ignored, in which case *ret contains
the best guess described below.  Otherwise returns false, in which
case *ret tells the caller nothing and the caller should instead
consult lockdep.

If CONFIG_DEBUG_LOCK_ALLOC is selected, set *ret to nonzero iff in an
RCU-sched read-side critical section.  In absence of
CONFIG_DEBUG_LOCK_ALLOC, this assumes we are in an RCU-sched read-side
critical section unless it can prove otherwise.  Note that disabling
of preemption (including disabling irqs) counts as an RCU-sched
read-side critical section.  This is useful for debug checks in functions
that required that they be called within an RCU-sched read-side
critical section.

Check debug_lockdep_rcu_enabled() to prevent false positives during boot
and while lockdep is disabled.

Note that if the CPU is in the idle loop from an RCU point of view (ie:
that we are in the section between rcu_idle_enter() and rcu_idle_exit())
then rcu_read_lock_held() sets *ret to false even if the CPU did an
rcu_read_lock().  The reason for this is that RCU ignores CPUs that are
in such a section, considering these as in extended quiescent state,
so such a CPU is effectively never in an RCU read-side critical section
regardless of what RCU primitives it invokes.  This state of affairs is
required ... we need to keep an RCU-free window in idle where the CPU may
possibly enter into low power mode. This way we can notice an extended
quiescent state to other CPUs that started a grace period. Otherwise
we would delay any grace period as long as we run in the idle task.

Similarly, we avoid claiming an RCU read lock held if the current
CPU is offline.


====================


void rcu_expedite_gp(void)
Expedite future RCU grace periods

Parameters


void
no arguments

Description

After a call to this function, future calls to synchronize_rcu() and
friends act as the corresponding synchronize_rcu_expedited() function
had instead been called.


====================


void rcu_unexpedite_gp(void)
Cancel prior rcu_expedite_gp() invocation

Parameters


void
no arguments

Description

Undo a prior call to rcu_expedite_gp().  If all prior calls to
rcu_expedite_gp() are undone by a subsequent call to rcu_unexpedite_gp(),
and if the rcu_expedited sysfs/boot parameter is not set, then all
subsequent calls to synchronize_rcu() and friends will return to
their normal non-expedited behavior.


====================


int rcu_read_lock_held(void)
might we be in RCU read-side critical section?

Parameters


void
no arguments

Description

If CONFIG_DEBUG_LOCK_ALLOC is selected, returns nonzero iff in an RCU
read-side critical section.  In absence of CONFIG_DEBUG_LOCK_ALLOC,
this assumes we are in an RCU read-side critical section unless it can
prove otherwise.  This is useful for debug checks in functions that
require that they be called within an RCU read-side critical section.

Checks debug_lockdep_rcu_enabled() to prevent false positives during boot
and while lockdep is disabled.

Note that rcu_read_lock() and the matching rcu_read_unlock() must
occur in the same context, for example, it is illegal to invoke
rcu_read_unlock() in process context if the matching rcu_read_lock()
was invoked from within an irq handler.

Note that rcu_read_lock() is disallowed if the CPU is either idle or
offline from an RCU perspective, so check for those as well.


====================


int rcu_read_lock_bh_held(void)
might we be in RCU-bh read-side critical section?

Parameters


void
no arguments

Description

Check for bottom half being disabled, which covers both the
CONFIG_PROVE_RCU and not cases.  Note that if someone uses
rcu_read_lock_bh(), but then later enables BH, lockdep (if enabled)
will show the situation.  This is useful for debug checks in functions
that require that they be called within an RCU read-side critical
section.

Check debug_lockdep_rcu_enabled() to prevent false positives during boot.

Note that rcu_read_lock_bh() is disallowed if the CPU is either idle or
offline from an RCU perspective, so check for those as well.


====================


void wakeme_after_rcu(struct rcu_head *head)
Callback function to awaken a task after grace period

Parameters


struct rcu_head *head
Pointer to rcu_head member within rcu_synchronize structure

Description

Awaken the corresponding task now that a grace period has elapsed.


====================


void init_rcu_head_on_stack(struct rcu_head *head)
initialize on-stack rcu_head for debugobjects

Parameters


struct rcu_head *head
pointer to rcu_head structure to be initialized

Description

This function informs debugobjects of a new rcu_head structure that
has been allocated as an auto variable on the stack.  This function
is not required for rcu_head structures that are statically defined or
that are dynamically allocated on the heap.  This function has no
effect for !CONFIG_DEBUG_OBJECTS_RCU_HEAD kernel builds.


====================


void destroy_rcu_head_on_stack(struct rcu_head *head)
destroy on-stack rcu_head for debugobjects

Parameters


struct rcu_head *head
pointer to rcu_head structure to be initialized

Description

This function informs debugobjects that an on-stack rcu_head structure
is about to go out of scope.  As with init_rcu_head_on_stack(), this
function is not required for rcu_head structures that are statically
defined or that are dynamically allocated on the heap.  Also as with
init_rcu_head_on_stack(), this function has no effect for
!CONFIG_DEBUG_OBJECTS_RCU_HEAD kernel builds.


====================


int srcu_read_lock_held(const struct srcu_struct *ssp)
might we be in SRCU read-side critical section?

Parameters


const struct srcu_struct *ssp
The srcu_struct structure to check

Description

If CONFIG_DEBUG_LOCK_ALLOC is selected, returns nonzero iff in an SRCU
read-side critical section.  In absence of CONFIG_DEBUG_LOCK_ALLOC,
this assumes we are in an SRCU read-side critical section unless it can
prove otherwise.

Checks debug_lockdep_rcu_enabled() to prevent false positives during boot
and while lockdep is disabled.

Note that SRCU is based on its own statemachine and it doesn't
relies on normal RCU, it can be called from the CPU which
is in the idle loop from an RCU point of view or offline.


====================


srcu_dereference_check(p, ssp, c)
fetch SRCU-protected pointer for later dereferencing

Parameters


p
the pointer to fetch and protect for later dereferencing

ssp
pointer to the srcu_struct, which is used to check that we
really are in an SRCU read-side critical section.

c
condition to check for update-side use

Description

If PROVE_RCU is enabled, invoking this outside of an RCU read-side
critical section will result in an RCU-lockdep splat, unless c evaluates
to 1.  The c argument will normally be a logical expression containing
lockdep_is_held() calls.


====================


srcu_dereference(p, ssp)
fetch SRCU-protected pointer for later dereferencing

Parameters


p
the pointer to fetch and protect for later dereferencing

ssp
pointer to the srcu_struct, which is used to check that we
really are in an SRCU read-side critical section.

Description

Makes rcu_dereference_check() do the dirty work.  If PROVE_RCU
is enabled, invoking this outside of an RCU read-side critical
section will result in an RCU-lockdep splat.


====================


srcu_dereference_notrace(p, ssp)
no tracing and no lockdep calls from here

Parameters


p
the pointer to fetch and protect for later dereferencing

ssp
pointer to the srcu_struct, which is used to check that we
really are in an SRCU read-side critical section.


====================


int srcu_read_lock(struct srcu_struct *ssp)
register a new reader for an SRCU-protected structure.

Parameters


struct srcu_struct *ssp
srcu_struct in which to register the new reader.

Description

Enter an SRCU read-side critical section.  Note that SRCU read-side
critical sections may be nested.  However, it is illegal to
call anything that waits on an SRCU grace period for the same
srcu_struct, whether directly or indirectly.  Please note that
one way to indirectly wait on an SRCU grace period is to acquire
a mutex that is held elsewhere while calling synchronize_srcu() or
synchronize_srcu_expedited().

Note that srcu_read_lock() and the matching srcu_read_unlock() must
occur in the same context, for example, it is illegal to invoke
srcu_read_unlock() in an irq handler if the matching srcu_read_lock()
was invoked in process context.


====================


void srcu_read_unlock(struct srcu_struct *ssp, int idx)
unregister a old reader from an SRCU-protected structure.

Parameters


struct srcu_struct *ssp
srcu_struct in which to unregister the old reader.

int idx
return value from corresponding srcu_read_lock().

Description

Exit an SRCU read-side critical section.


====================


void smp_mb__after_srcu_read_unlock(void)
ensure full ordering after srcu_read_unlock

Parameters


void
no arguments

Description

Converts the preceding srcu_read_unlock into a two-way memory barrier.

Call this after srcu_read_unlock, to guarantee that all memory operations
that occur after smp_mb__after_srcu_read_unlock will appear to happen after
the preceding srcu_read_unlock.


====================


int init_srcu_struct(struct srcu_struct *ssp)
initialize a sleep-RCU structure

Parameters


struct srcu_struct *ssp
structure to initialize.

Description

Must invoke this on a given srcu_struct before passing that srcu_struct
to any other function.  Each srcu_struct represents a separate domain
of SRCU protection.


====================


bool srcu_readers_active(struct srcu_struct *ssp)
returns true if there are readers. and false otherwise

Parameters


struct srcu_struct *ssp
which srcu_struct to count active readers (holding srcu_read_lock).

Description

Note that this is not an atomic primitive, and can therefore suffer
severe errors when invoked on an active srcu_struct.  That said, it
can be useful as an error check at cleanup time.


====================


void cleanup_srcu_struct(struct srcu_struct *ssp)
deconstruct a sleep-RCU structure

Parameters


struct srcu_struct *ssp
structure to clean up.

Description

Must invoke this after you are finished using a given srcu_struct that
was initialized via init_srcu_struct(), else you leak memory.


====================


void call_srcu(struct srcu_struct *ssp, struct rcu_head *rhp, rcu_callback_t func)
Queue a callback for invocation after an SRCU grace period

Parameters


struct srcu_struct *ssp
srcu_struct in queue the callback

struct rcu_head *rhp
structure to be used for queueing the SRCU callback.

rcu_callback_t func
function to be invoked after the SRCU grace period

Description

The callback function will be invoked some time after a full SRCU
grace period elapses, in other words after all pre-existing SRCU
read-side critical sections have completed.  However, the callback
function might well execute concurrently with other SRCU read-side
critical sections that started after call_srcu() was invoked.  SRCU
read-side critical sections are delimited by srcu_read_lock() and
srcu_read_unlock(), and may be nested.

The callback will be invoked from process context, but must nevertheless
be fast and must not block.


====================


void synchronize_srcu_expedited(struct srcu_struct *ssp)
Brute-force SRCU grace period

Parameters


struct srcu_struct *ssp
srcu_struct with which to synchronize.

Description

Wait for an SRCU grace period to elapse, but be more aggressive about
spinning rather than blocking when waiting.

Note that synchronize_srcu_expedited() has the same deadlock and
memory-ordering properties as does synchronize_srcu().


====================


void synchronize_srcu(struct srcu_struct *ssp)
wait for prior SRCU read-side critical-section completion

Parameters


struct srcu_struct *ssp
srcu_struct with which to synchronize.

Description

Wait for the count to drain to zero of both indexes. To avoid the
possible starvation of synchronize_srcu(), it waits for the count of
the index=((->srcu_idx & 1) ^ 1) to drain to zero at first,
and then flip the srcu_idx and wait for the count of the other index.

Can block; must be called from process context.

Note that it is illegal to call synchronize_srcu() from the corresponding
SRCU read-side critical section; doing so will result in deadlock.
However, it is perfectly legal to call synchronize_srcu() on one
srcu_struct from some other srcu_struct's read-side critical section,
as long as the resulting graph of srcu_structs is acyclic.

There are memory-ordering constraints implied by synchronize_srcu().
On systems with more than one CPU, when synchronize_srcu() returns,
each CPU is guaranteed to have executed a full memory barrier since
the end of its last corresponding SRCU read-side critical section
whose beginning preceded the call to synchronize_srcu().  In addition,
each CPU having an SRCU read-side critical section that extends beyond
the return from synchronize_srcu() is guaranteed to have executed a
full memory barrier after the beginning of synchronize_srcu() and before
the beginning of that SRCU read-side critical section.  Note that these
guarantees include CPUs that are offline, idle, or executing in user mode,
as well as CPUs that are executing in the kernel.

Furthermore, if CPU A invoked synchronize_srcu(), which returned
to its caller on CPU B, then both CPU A and CPU B are guaranteed
to have executed a full memory barrier during the execution of
synchronize_srcu().  This guarantee applies even if CPU A and CPU B
are the same CPU, but again only if the system has more than one CPU.

Of course, these memory-ordering guarantees apply only when
synchronize_srcu(), srcu_read_lock(), and srcu_read_unlock() are
passed the same srcu_struct structure.

Implementation of these memory-ordering guarantees is similar to
that of synchronize_rcu().

If SRCU is likely idle, expedite the first request.  This semantic
was provided by Classic SRCU, and is relied upon by its users, so TREE
SRCU must also provide it.  Note that detecting idleness is heuristic
and subject to both false positives and negatives.


====================


unsigned long get_state_synchronize_srcu(struct srcu_struct *ssp)
Provide an end-of-grace-period cookie

Parameters


struct srcu_struct *ssp
srcu_struct to provide cookie for.

Description

This function returns a cookie that can be passed to
poll_state_synchronize_srcu(), which will return true if a full grace
period has elapsed in the meantime.  It is the caller's responsibility
to make sure that grace period happens, for example, by invoking
call_srcu() after return from get_state_synchronize_srcu().


====================


unsigned long start_poll_synchronize_srcu(struct srcu_struct *ssp)
Provide cookie and start grace period

Parameters


struct srcu_struct *ssp
srcu_struct to provide cookie for.

Description

This function returns a cookie that can be passed to
poll_state_synchronize_srcu(), which will return true if a full grace
period has elapsed in the meantime.  Unlike get_state_synchronize_srcu(),
this function also ensures that any needed SRCU grace period will be
started.  This convenience does come at a cost in terms of CPU overhead.


====================


bool poll_state_synchronize_srcu(struct srcu_struct *ssp, unsigned long cookie)
Has cookie's grace period ended?

Parameters


struct srcu_struct *ssp
srcu_struct to provide cookie for.

unsigned long cookie
Return value from get_state_synchronize_srcu() or start_poll_synchronize_srcu().

Description

This function takes the cookie that was returned from either
get_state_synchronize_srcu() or start_poll_synchronize_srcu(), and
returns true if an SRCU grace period elapsed since the time that the
cookie was created.

Because cookies are finite in size, wrapping/overflow is possible.
This is more pronounced on 32-bit systems where cookies are 32 bits,
where in theory wrapping could happen in about 14 hours assuming
25-microsecond expedited SRCU grace periods.  However, a more likely
overflow lower bound is on the order of 24 days in the case of
one-millisecond SRCU grace periods.  Of course, wrapping in a 64-bit
system requires geologic timespans, as in more than seven million years
even for expedited SRCU grace periods.

Wrapping/overflow is much more of an issue for CONFIG_SMP=n systems
that also have CONFIG_PREEMPTION=n, which selects Tiny SRCU.  This uses
a 16-bit cookie, which rcutorture routinely wraps in a matter of a
few minutes.  If this proves to be a problem, this counter will be
expanded to the same size as for Tree SRCU.


====================


void srcu_barrier(struct srcu_struct *ssp)
Wait until all in-flight call_srcu() callbacks complete.

Parameters


struct srcu_struct *ssp
srcu_struct on which to wait for in-flight callbacks.


====================


unsigned long srcu_batches_completed(struct srcu_struct *ssp)
return batches completed.

Parameters


struct srcu_struct *ssp
srcu_struct on which to report batch completion.

Description

Report the number of batches, correlated with, but not necessarily
precisely the same as, the number of grace periods that have elapsed.


====================


void hlist_bl_del_rcu(struct hlist_bl_node *n)
deletes entry from hash list without re-initialization

Parameters


struct hlist_bl_node *n
the element to delete from the hash list.

Note

hlist_bl_unhashed() on entry does not return true after this,
the entry is in an undefined state. It is useful for RCU based
lockfree traversal.

Description

In particular, it means that we can not poison the forward
pointers that may still be used for walking the hash list.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as hlist_bl_add_head_rcu()
or hlist_bl_del_rcu(), running on this same list.
However, it is perfectly legal to run concurrently with
the _rcu list-traversal primitives, such as
hlist_bl_for_each_entry().


====================


void hlist_bl_add_head_rcu(struct hlist_bl_node *n, struct hlist_bl_head *h)

Parameters


struct hlist_bl_node *n
the element to add to the hash list.

struct hlist_bl_head *h
the list to add to.

Description

Adds the specified element to the specified hlist_bl,
while permitting racing traversals.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as hlist_bl_add_head_rcu()
or hlist_bl_del_rcu(), running on this same list.
However, it is perfectly legal to run concurrently with
the _rcu list-traversal primitives, such as
hlist_bl_for_each_entry_rcu(), used to prevent memory-consistency
problems on Alpha CPUs.  Regardless of the type of CPU, the
list-traversal primitive must be guarded by rcu_read_lock().


====================


hlist_bl_for_each_entry_rcu(tpos, pos, head, member)
iterate over rcu list of given type

Parameters


tpos
the type * to use as a loop cursor.

pos
the struct hlist_bl_node to use as a loop cursor.

head
the head for your list.

member
the name of the hlist_bl_node within the struct.


====================


list_tail_rcu(head)
returns the prev pointer of the head of the list

Parameters


head
the head of the list

Note

This should only be used with the list header, and even then
only if list_del() and similar primitives are not also used on the
list header.


====================


void list_add_rcu(struct list_head *new, struct list_head *head)
add a new entry to rcu-protected list

Parameters


struct list_head *new
new entry to be added

struct list_head *head
list head to add it after

Description

Insert a new entry after the specified head.
This is good for implementing stacks.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as list_add_rcu()
or list_del_rcu(), running on this same list.
However, it is perfectly legal to run concurrently with
the _rcu list-traversal primitives, such as
list_for_each_entry_rcu().


====================


void list_add_tail_rcu(struct list_head *new, struct list_head *head)
add a new entry to rcu-protected list

Parameters


struct list_head *new
new entry to be added

struct list_head *head
list head to add it before

Description

Insert a new entry before the specified head.
This is useful for implementing queues.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as list_add_tail_rcu()
or list_del_rcu(), running on this same list.
However, it is perfectly legal to run concurrently with
the _rcu list-traversal primitives, such as
list_for_each_entry_rcu().


====================


void list_del_rcu(struct list_head *entry)
deletes entry from list without re-initialization

Parameters


struct list_head *entry
the element to delete from the list.

Note

list_empty() on entry does not return true after this,
the entry is in an undefined state. It is useful for RCU based
lockfree traversal.

Description

In particular, it means that we can not poison the forward
pointers that may still be used for walking the list.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as list_del_rcu()
or list_add_rcu(), running on this same list.
However, it is perfectly legal to run concurrently with
the _rcu list-traversal primitives, such as
list_for_each_entry_rcu().

Note that the caller is not permitted to immediately free
the newly deleted entry.  Instead, either synchronize_rcu()
or call_rcu() must be used to defer freeing until an RCU
grace period has elapsed.


====================


void hlist_del_init_rcu(struct hlist_node *n)
deletes entry from hash list with re-initialization

Parameters


struct hlist_node *n
the element to delete from the hash list.

Note

list_unhashed() on the node return true after this. It is
useful for RCU based read lockfree traversal if the writer side
must know if the list entry is still hashed or already unhashed.

Description

In particular, it means that we can not poison the forward pointers
that may still be used for walking the hash list and we can only
zero the pprev pointer so list_unhashed() will return true after
this.

The caller must take whatever precautions are necessary (such as
holding appropriate locks) to avoid racing with another
list-mutation primitive, such as hlist_add_head_rcu() or
hlist_del_rcu(), running on this same list.  However, it is
perfectly legal to run concurrently with the _rcu list-traversal
primitives, such as hlist_for_each_entry_rcu().


====================


void list_replace_rcu(struct list_head *old, struct list_head *new)
replace old entry by new one

Parameters


struct list_head *old
the element to be replaced

struct list_head *new
the new element to insert

Description

The old entry will be replaced with the new entry atomically.

Note

old should not be empty.


====================


void __list_splice_init_rcu(struct list_head *list, struct list_head *prev, struct list_head *next, void (*sync)(void))
join an RCU-protected list into an existing list.

Parameters


struct list_head *list
the RCU-protected list to splice

struct list_head *prev
points to the last element of the existing list

struct list_head *next
points to the first element of the existing list

void (*sync)(void)
synchronize_rcu, synchronize_rcu_expedited, ...¦

Description

The list pointed to by prev and next can be RCU-read traversed
concurrently with this function.

Note that this function blocks.

Important note: the caller must take whatever action is necessary to prevent
any other updates to the existing list.  In principle, it is possible to
modify the list as soon as sync() begins execution. If this sort of thing
becomes necessary, an alternative version based on call_rcu() could be
created.  But only if -really- needed ... there is no shortage of RCU API
members.


====================


void list_splice_init_rcu(struct list_head *list, struct list_head *head, void (*sync)(void))
splice an RCU-protected list into an existing list, designed for stacks.

Parameters


struct list_head *list
the RCU-protected list to splice

struct list_head *head
the place in the existing list to splice the first list into

void (*sync)(void)
synchronize_rcu, synchronize_rcu_expedited, ...¦


====================


void list_splice_tail_init_rcu(struct list_head *list, struct list_head *head, void (*sync)(void))
splice an RCU-protected list into an existing list, designed for queues.

Parameters


struct list_head *list
the RCU-protected list to splice

struct list_head *head
the place in the existing list to splice the first list into

void (*sync)(void)
synchronize_rcu, synchronize_rcu_expedited, ...¦


====================


list_entry_rcu(ptr, type, member)
get the struct for this entry

Parameters


ptr
the struct list_head pointer.

type
the type of the struct this is embedded in.

member
the name of the list_head within the struct.

Description

This primitive may safely run concurrently with the _rcu list-mutation
primitives such as list_add_rcu() as long as it's guarded by rcu_read_lock().


====================


list_first_or_null_rcu(ptr, type, member)
get the first element from a list

Parameters


ptr
the list head to take the element from.

type
the type of the struct this is embedded in.

member
the name of the list_head within the struct.

Description

Note that if the list is empty, it returns NULL.

This primitive may safely run concurrently with the _rcu list-mutation
primitives such as list_add_rcu() as long as it's guarded by rcu_read_lock().


====================


list_next_or_null_rcu(head, ptr, type, member)
get the first element from a list

Parameters


head
the head for the list.

ptr
the list head to take the next element from.

type
the type of the struct this is embedded in.

member
the name of the list_head within the struct.

Description

Note that if the ptr is at the end of the list, NULL is returned.

This primitive may safely run concurrently with the _rcu list-mutation
primitives such as list_add_rcu() as long as it's guarded by rcu_read_lock().


====================


list_for_each_entry_rcu(pos, head, member, cond...¦)
iterate over rcu list of given type

Parameters


pos
the type * to use as a loop cursor.

head
the head for your list.

member
the name of the list_head within the struct.

cond...
optional lockdep expression if called from non-RCU protection.

Description

This list-traversal primitive may safely run concurrently with
the _rcu list-mutation primitives such as list_add_rcu()
as long as the traversal is guarded by rcu_read_lock().


====================


list_for_each_entry_srcu(pos, head, member, cond)
iterate over rcu list of given type

Parameters


pos
the type * to use as a loop cursor.

head
the head for your list.

member
the name of the list_head within the struct.

cond
lockdep expression for the lock required to traverse the list.

Description

This list-traversal primitive may safely run concurrently with
the _rcu list-mutation primitives such as list_add_rcu()
as long as the traversal is guarded by srcu_read_lock().
The lockdep expression srcu_read_lock_held() can be passed as the
cond argument from read side.


====================


list_entry_lockless(ptr, type, member)
get the struct for this entry

Parameters


ptr
the struct list_head pointer.

type
the type of the struct this is embedded in.

member
the name of the list_head within the struct.

Description

This primitive may safely run concurrently with the _rcu
list-mutation primitives such as list_add_rcu(), but requires some
implicit RCU read-side guarding.  One example is running within a special
exception-time environment where preemption is disabled and where lockdep
cannot be invoked.  Another example is when items are added to the list,
but never deleted.


====================


list_for_each_entry_lockless(pos, head, member)
iterate over rcu list of given type

Parameters


pos
the type * to use as a loop cursor.

head
the head for your list.

member
the name of the list_struct within the struct.

Description

This primitive may safely run concurrently with the _rcu
list-mutation primitives such as list_add_rcu(), but requires some
implicit RCU read-side guarding.  One example is running within a special
exception-time environment where preemption is disabled and where lockdep
cannot be invoked.  Another example is when items are added to the list,
but never deleted.


====================


list_for_each_entry_continue_rcu(pos, head, member)
continue iteration over list of given type

Parameters


pos
the type * to use as a loop cursor.

head
the head for your list.

member
the name of the list_head within the struct.

Description

Continue to iterate over list of given type, continuing after
the current position which must have been in the list when the RCU read
lock was taken.
This would typically require either that you obtained the node from a
previous walk of the list in the same RCU read-side critical section, or
that you held some sort of non-RCU reference (such as a reference count)
to keep the node alive and in the list.

This iterator is similar to list_for_each_entry_from_rcu() except
this starts after the given position and that one starts at the given
position.


====================


list_for_each_entry_from_rcu(pos, head, member)
iterate over a list from current point

Parameters


pos
the type * to use as a loop cursor.

head
the head for your list.

member
the name of the list_node within the struct.

Description

Iterate over the tail of a list starting from a given position,
which must have been in the list when the RCU read lock was taken.
This would typically require either that you obtained the node from a
previous walk of the list in the same RCU read-side critical section, or
that you held some sort of non-RCU reference (such as a reference count)
to keep the node alive and in the list.

This iterator is similar to list_for_each_entry_continue_rcu() except
this starts from the given position and that one starts from the position
after the given position.


====================


void hlist_del_rcu(struct hlist_node *n)
deletes entry from hash list without re-initialization

Parameters


struct hlist_node *n
the element to delete from the hash list.

Note

list_unhashed() on entry does not return true after this,
the entry is in an undefined state. It is useful for RCU based
lockfree traversal.

Description

In particular, it means that we can not poison the forward
pointers that may still be used for walking the hash list.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as hlist_add_head_rcu()
or hlist_del_rcu(), running on this same list.
However, it is perfectly legal to run concurrently with
the _rcu list-traversal primitives, such as
hlist_for_each_entry().


====================


void hlist_replace_rcu(struct hlist_node *old, struct hlist_node *new)
replace old entry by new one

Parameters


struct hlist_node *old
the element to be replaced

struct hlist_node *new
the new element to insert

Description

The old entry will be replaced with the new entry atomically.


====================


void hlists_swap_heads_rcu(struct hlist_head *left, struct hlist_head *right)
swap the lists the hlist heads point to

Parameters


struct hlist_head *left
The hlist head on the left

struct hlist_head *right
The hlist head on the right

Description


The lists start out as [left  ][node1 ...¦ ] and
[right ][node2 ...¦ ]

The lists end up as    [left  ][node2 ...¦ ]
[right ][node1 ...¦ ]


====================


void hlist_add_head_rcu(struct hlist_node *n, struct hlist_head *h)

Parameters


struct hlist_node *n
the element to add to the hash list.

struct hlist_head *h
the list to add to.

Description

Adds the specified element to the specified hlist,
while permitting racing traversals.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as hlist_add_head_rcu()
or hlist_del_rcu(), running on this same list.
However, it is perfectly legal to run concurrently with
the _rcu list-traversal primitives, such as
hlist_for_each_entry_rcu(), used to prevent memory-consistency
problems on Alpha CPUs.  Regardless of the type of CPU, the
list-traversal primitive must be guarded by rcu_read_lock().


====================


void hlist_add_tail_rcu(struct hlist_node *n, struct hlist_head *h)

Parameters


struct hlist_node *n
the element to add to the hash list.

struct hlist_head *h
the list to add to.

Description

Adds the specified element to the specified hlist,
while permitting racing traversals.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as hlist_add_head_rcu()
or hlist_del_rcu(), running on this same list.
However, it is perfectly legal to run concurrently with
the _rcu list-traversal primitives, such as
hlist_for_each_entry_rcu(), used to prevent memory-consistency
problems on Alpha CPUs.  Regardless of the type of CPU, the
list-traversal primitive must be guarded by rcu_read_lock().


====================


void hlist_add_before_rcu(struct hlist_node *n, struct hlist_node *next)

Parameters


struct hlist_node *n
the new element to add to the hash list.

struct hlist_node *next
the existing element to add the new element before.

Description

Adds the specified element to the specified hlist
before the specified node while permitting racing traversals.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as hlist_add_head_rcu()
or hlist_del_rcu(), running on this same list.
However, it is perfectly legal to run concurrently with
the _rcu list-traversal primitives, such as
hlist_for_each_entry_rcu(), used to prevent memory-consistency
problems on Alpha CPUs.


====================


void hlist_add_behind_rcu(struct hlist_node *n, struct hlist_node *prev)

Parameters


struct hlist_node *n
the new element to add to the hash list.

struct hlist_node *prev
the existing element to add the new element after.

Description

Adds the specified element to the specified hlist
after the specified node while permitting racing traversals.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as hlist_add_head_rcu()
or hlist_del_rcu(), running on this same list.
However, it is perfectly legal to run concurrently with
the _rcu list-traversal primitives, such as
hlist_for_each_entry_rcu(), used to prevent memory-consistency
problems on Alpha CPUs.


====================


hlist_for_each_entry_rcu(pos, head, member, cond...¦)
iterate over rcu list of given type

Parameters


pos
the type * to use as a loop cursor.

head
the head for your list.

member
the name of the hlist_node within the struct.

cond...
optional lockdep expression if called from non-RCU protection.

Description

This list-traversal primitive may safely run concurrently with
the _rcu list-mutation primitives such as hlist_add_head_rcu()
as long as the traversal is guarded by rcu_read_lock().


====================


hlist_for_each_entry_srcu(pos, head, member, cond)
iterate over rcu list of given type

Parameters


pos
the type * to use as a loop cursor.

head
the head for your list.

member
the name of the hlist_node within the struct.

cond
lockdep expression for the lock required to traverse the list.

Description

This list-traversal primitive may safely run concurrently with
the _rcu list-mutation primitives such as hlist_add_head_rcu()
as long as the traversal is guarded by srcu_read_lock().
The lockdep expression srcu_read_lock_held() can be passed as the
cond argument from read side.


====================


hlist_for_each_entry_rcu_notrace(pos, head, member)
iterate over rcu list of given type (for tracing)

Parameters


pos
the type * to use as a loop cursor.

head
the head for your list.

member
the name of the hlist_node within the struct.

Description

This list-traversal primitive may safely run concurrently with
the _rcu list-mutation primitives such as hlist_add_head_rcu()
as long as the traversal is guarded by rcu_read_lock().

This is the same as hlist_for_each_entry_rcu() except that it does
not do any RCU debugging or tracing.


====================


hlist_for_each_entry_rcu_bh(pos, head, member)
iterate over rcu list of given type

Parameters


pos
the type * to use as a loop cursor.

head
the head for your list.

member
the name of the hlist_node within the struct.

Description

This list-traversal primitive may safely run concurrently with
the _rcu list-mutation primitives such as hlist_add_head_rcu()
as long as the traversal is guarded by rcu_read_lock().


====================


hlist_for_each_entry_continue_rcu(pos, member)
iterate over a hlist continuing after current point

Parameters


pos
the type * to use as a loop cursor.

member
the name of the hlist_node within the struct.


====================


hlist_for_each_entry_continue_rcu_bh(pos, member)
iterate over a hlist continuing after current point

Parameters


pos
the type * to use as a loop cursor.

member
the name of the hlist_node within the struct.


====================


hlist_for_each_entry_from_rcu(pos, member)
iterate over a hlist continuing from current point

Parameters


pos
the type * to use as a loop cursor.

member
the name of the hlist_node within the struct.


====================


void hlist_nulls_del_init_rcu(struct hlist_nulls_node *n)
deletes entry from hash list with re-initialization

Parameters


struct hlist_nulls_node *n
the element to delete from the hash list.

Note

hlist_nulls_unhashed() on the node return true after this. It is
useful for RCU based read lockfree traversal if the writer side
must know if the list entry is still hashed or already unhashed.

Description

In particular, it means that we can not poison the forward pointers
that may still be used for walking the hash list and we can only
zero the pprev pointer so list_unhashed() will return true after
this.

The caller must take whatever precautions are necessary (such as
holding appropriate locks) to avoid racing with another
list-mutation primitive, such as hlist_nulls_add_head_rcu() or
hlist_nulls_del_rcu(), running on this same list.  However, it is
perfectly legal to run concurrently with the _rcu list-traversal
primitives, such as hlist_nulls_for_each_entry_rcu().


====================


hlist_nulls_first_rcu(head)
returns the first element of the hash list.

Parameters


head
the head of the list.


====================


hlist_nulls_next_rcu(node)
returns the element of the list after node.

Parameters


node
element of the list.


====================


void hlist_nulls_del_rcu(struct hlist_nulls_node *n)
deletes entry from hash list without re-initialization

Parameters


struct hlist_nulls_node *n
the element to delete from the hash list.

Note

hlist_nulls_unhashed() on entry does not return true after this,
the entry is in an undefined state. It is useful for RCU based
lockfree traversal.

Description

In particular, it means that we can not poison the forward
pointers that may still be used for walking the hash list.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as hlist_nulls_add_head_rcu()
or hlist_nulls_del_rcu(), running on this same list.
However, it is perfectly legal to run concurrently with
the _rcu list-traversal primitives, such as
hlist_nulls_for_each_entry().


====================


void hlist_nulls_add_head_rcu(struct hlist_nulls_node *n, struct hlist_nulls_head *h)

Parameters


struct hlist_nulls_node *n
the element to add to the hash list.

struct hlist_nulls_head *h
the list to add to.

Description

Adds the specified element to the specified hlist_nulls,
while permitting racing traversals.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as hlist_nulls_add_head_rcu()
or hlist_nulls_del_rcu(), running on this same list.
However, it is perfectly legal to run concurrently with
the _rcu list-traversal primitives, such as
hlist_nulls_for_each_entry_rcu(), used to prevent memory-consistency
problems on Alpha CPUs.  Regardless of the type of CPU, the
list-traversal primitive must be guarded by rcu_read_lock().


====================


void hlist_nulls_add_tail_rcu(struct hlist_nulls_node *n, struct hlist_nulls_head *h)

Parameters


struct hlist_nulls_node *n
the element to add to the hash list.

struct hlist_nulls_head *h
the list to add to.

Description

Adds the specified element to the specified hlist_nulls,
while permitting racing traversals.

The caller must take whatever precautions are necessary
(such as holding appropriate locks) to avoid racing
with another list-mutation primitive, such as hlist_nulls_add_head_rcu()
or hlist_nulls_del_rcu(), running on this same list.
However, it is perfectly legal to run concurrently with
the _rcu list-traversal primitives, such as
hlist_nulls_for_each_entry_rcu(), used to prevent memory-consistency
problems on Alpha CPUs.  Regardless of the type of CPU, the
list-traversal primitive must be guarded by rcu_read_lock().


====================


hlist_nulls_for_each_entry_rcu(tpos, pos, head, member)
iterate over rcu list of given type

Parameters


tpos
the type * to use as a loop cursor.

pos
the struct hlist_nulls_node to use as a loop cursor.

head
the head of the list.

member
the name of the hlist_nulls_node within the struct.

Description

The barrier() is needed to make sure compiler doesn't cache first element [1],
as this loop can be restarted [2]
[1] Documentation/memory-barriers.txt around line 1533
[2] Using RCU hlist_nulls to protect list and objects around line 146


====================


hlist_nulls_for_each_entry_safe(tpos, pos, head, member)
iterate over list of given type safe against removal of list entry

Parameters


tpos
the type * to use as a loop cursor.

pos
the struct hlist_nulls_node to use as a loop cursor.

head
the head of the list.

member
the name of the hlist_nulls_node within the struct.


====================


bool rcu_sync_is_idle(struct rcu_sync *rsp)
Are readers permitted to use their fastpaths?

Parameters


struct rcu_sync *rsp
Pointer to rcu_sync structure to use for synchronization

Description

Returns true if readers are permitted to use their fastpaths.  Must be
invoked within some flavor of RCU read-side critical section.


====================


void rcu_sync_init(struct rcu_sync *rsp)
Initialize an rcu_sync structure

Parameters


struct rcu_sync *rsp
Pointer to rcu_sync structure to be initialized


====================


void rcu_sync_enter_start(struct rcu_sync *rsp)
Force readers onto slow path for multiple updates

Parameters


struct rcu_sync *rsp
Pointer to rcu_sync structure to use for synchronization

Description

Must be called after rcu_sync_init() and before first use.

Ensures rcu_sync_is_idle() returns false and rcu_sync_{enter,exit}()
pairs turn into NO-OPs.


====================


void rcu_sync_func(struct rcu_head *rhp)
Callback function managing reader access to fastpath

Parameters


struct rcu_head *rhp
Pointer to rcu_head in rcu_sync structure to use for synchronization

Description

This function is passed to call_rcu() function by rcu_sync_enter() and
rcu_sync_exit(), so that it is invoked after a grace period following the
that invocation of enter/exit.

If it is called by rcu_sync_enter() it signals that all the readers were
switched onto slow path.

If it is called by rcu_sync_exit() it takes action based on events that
have taken place in the meantime, so that closely spaced rcu_sync_enter()
and rcu_sync_exit() pairs need not wait for a grace period.

If another rcu_sync_enter() is invoked before the grace period
ended, reset state to allow the next rcu_sync_exit() to let the
readers back onto their fastpaths (after a grace period).  If both
another rcu_sync_enter() and its matching rcu_sync_exit() are invoked
before the grace period ended, re-invoke call_rcu() on behalf of that
rcu_sync_exit().  Otherwise, set all state back to idle so that readers
can again use their fastpaths.


====================


void rcu_sync_enter(struct rcu_sync *rsp)
Force readers onto slowpath

Parameters


struct rcu_sync *rsp
Pointer to rcu_sync structure to use for synchronization

Description

This function is used by updaters who need readers to make use of
a slowpath during the update.  After this function returns, all
subsequent calls to rcu_sync_is_idle() will return false, which
tells readers to stay off their fastpaths.  A later call to
rcu_sync_exit() re-enables reader slowpaths.

When called in isolation, rcu_sync_enter() must wait for a grace
period, however, closely spaced calls to rcu_sync_enter() can
optimize away the grace-period wait via a state machine implemented
by rcu_sync_enter(), rcu_sync_exit(), and rcu_sync_func().


====================


void rcu_sync_exit(struct rcu_sync *rsp)
Allow readers back onto fast path after grace period

Parameters


struct rcu_sync *rsp
Pointer to rcu_sync structure to use for synchronization

Description

This function is used by updaters who have completed, and can therefore
now allow readers to make use of their fastpaths after a grace period
has elapsed.  After this grace period has completed, all subsequent
calls to rcu_sync_is_idle() will return true, which tells readers that
they can once again use their fastpaths.


====================


void rcu_sync_dtor(struct rcu_sync *rsp)
Clean up an rcu_sync structure

Parameters


struct rcu_sync *rsp
Pointer to rcu_sync structure to be cleaned up


====================


access_ok(addr, size)
Checks if a user space pointer is valid

Parameters


addr
User space pointer to start of block to check

size
Size of block to check

Context

User context only. This function may sleep if pagefaults are
enabled.

Description

Checks if a pointer to a block of memory in user space is valid.

Note that, depending on architecture, this function probably just
checks that the pointer is in the user space range - after calling
this function, memory access functions may still return -EFAULT.

Return

true (nonzero) if the memory block may be valid, false (zero)
if it is definitely invalid.


====================


get_user(x, ptr)
Get a simple variable from user space.

Parameters


x
Variable to store result.

ptr
Source address, in user space.

Context

User context only. This function may sleep if pagefaults are
enabled.

Description

This macro copies a single simple variable from user space to kernel
space.  It supports simple types like char and int, but not larger
data types like structures or arrays.

ptr must have pointer-to-simple-variable type, and the result of
dereferencing ptr must be assignable to x without a cast.

Return

zero on success, or -EFAULT on error.
On error, the variable x is set to zero.


====================


__get_user(x, ptr)
Get a simple variable from user space, with less checking.

Parameters


x
Variable to store result.

ptr
Source address, in user space.

Context

User context only. This function may sleep if pagefaults are
enabled.

Description

This macro copies a single simple variable from user space to kernel
space.  It supports simple types like char and int, but not larger
data types like structures or arrays.

ptr must have pointer-to-simple-variable type, and the result of
dereferencing ptr must be assignable to x without a cast.

Caller must check the pointer with access_ok() before calling this
function.

Return

zero on success, or -EFAULT on error.
On error, the variable x is set to zero.


====================


put_user(x, ptr)
Write a simple value into user space.

Parameters


x
Value to copy to user space.

ptr
Destination address, in user space.

Context

User context only. This function may sleep if pagefaults are
enabled.

Description

This macro copies a single simple value from kernel space to user
space.  It supports simple types like char and int, but not larger
data types like structures or arrays.

ptr must have pointer-to-simple-variable type, and x must be assignable
to the result of dereferencing ptr.

Return

zero on success, or -EFAULT on error.


====================


__put_user(x, ptr)
Write a simple value into user space, with less checking.

Parameters


x
Value to copy to user space.

ptr
Destination address, in user space.

Context

User context only. This function may sleep if pagefaults are
enabled.

Description

This macro copies a single simple value from kernel space to user
space.  It supports simple types like char and int, but not larger
data types like structures or arrays.

ptr must have pointer-to-simple-variable type, and x must be assignable
to the result of dereferencing ptr.

Caller must check the pointer with access_ok() before calling this
function.

Return

zero on success, or -EFAULT on error.


====================


unsigned long clear_user(void __user *to, unsigned long n)
Zero a block of memory in user space.

Parameters


void __user *to
Destination address, in user space.

unsigned long n
Number of bytes to zero.

Description

Zero a block of memory in user space.

Return

number of bytes that could not be cleared.
On success, this will be zero.


====================


unsigned long __clear_user(void __user *to, unsigned long n)
Zero a block of memory in user space, with less checking.

Parameters


void __user *to
Destination address, in user space.

unsigned long n
Number of bytes to zero.

Description

Zero a block of memory in user space.  Caller must check
the specified block with access_ok() before calling this function.

Return

number of bytes that could not be cleared.
On success, this will be zero.


====================


int get_user_pages_fast(unsigned long start, int nr_pages, unsigned int gup_flags, struct page **pages)
pin user pages in memory

Parameters


unsigned long start
starting user address

int nr_pages
number of pages from start to pin

unsigned int gup_flags
flags modifying pin behaviour

struct page **pages
array that receives pointers to the pages pinned.
Should be at least nr_pages long.

Description

Attempt to pin user pages in memory without taking mm->mmap_lock.
If not successful, it will fall back to taking the lock and
calling get_user_pages().

Returns number of pages pinned. This may be fewer than the number requested.
If nr_pages is 0 or negative, returns 0. If no pages were pinned, returns
-errno.

These flags provide hints about how mobile the page is. Pages with similar
mobility are placed within the same pageblocks to minimise problems due
to external fragmentation.

__GFP_MOVABLE (also a zone modifier) indicates that the page can be
moved by page migration during memory compaction or can be reclaimed.

__GFP_RECLAIMABLE is used for slab allocations that specify
SLAB_RECLAIM_ACCOUNT and whose pages can be freed via shrinkers.

__GFP_WRITE indicates the caller intends to dirty the page. Where possible,
these pages will be spread between local zones to avoid all the dirty
pages being in one zone (fair zone allocation policy).

__GFP_HARDWALL enforces the cpuset memory allocation policy.

__GFP_THISNODE forces the allocation to be satisfied from the requested
node with no fallbacks or placement policy enforcements.

__GFP_ACCOUNT causes the allocation to be accounted to kmemcg.

__GFP_HIGH indicates that the caller is high-priority and that granting
the request is necessary before the system can make forward progress.
For example, creating an IO context to clean pages.

__GFP_ATOMIC indicates that the caller cannot reclaim or sleep and is
high priority. Users are typically interrupt handlers. This may be
used in conjunction with __GFP_HIGH

__GFP_MEMALLOC allows access to all memory. This should only be used when
the caller guarantees the allocation will allow more memory to be freed
very shortly e.g. process exiting or swapping. Users either should
be the MM or co-ordinating closely with the VM (e.g. swap over NFS).
Users of this flag have to be extremely careful to not deplete the reserve
completely and implement a throttling mechanism which controls the
consumption of the reserve based on the amount of freed memory.
Usage of a pre-allocated pool (e.g. mempool) should be always considered
before using this flag.

__GFP_NOMEMALLOC is used to explicitly forbid access to emergency reserves.
This takes precedence over the __GFP_MEMALLOC flag if both are set.

Please note that all the following flags are only applicable to sleepable
allocations (e.g. GFP_NOWAIT and GFP_ATOMIC will ignore them).

__GFP_IO can start physical IO.

__GFP_FS can call down to the low-level FS. Clearing the flag avoids the
allocator recursing into the filesystem which might already be holding
locks.

__GFP_DIRECT_RECLAIM indicates that the caller may enter direct reclaim.
This flag can be cleared to avoid unnecessary delays when a fallback
option is available.

__GFP_KSWAPD_RECLAIM indicates that the caller wants to wake kswapd when
the low watermark is reached and have it reclaim pages until the high
watermark is reached. A caller may wish to clear this flag when fallback
options are available and the reclaim is likely to disrupt the system. The
canonical example is THP allocation where a fallback is cheap but
reclaim/compaction may cause indirect stalls.

__GFP_RECLAIM is shorthand to allow/forbid both direct and kswapd reclaim.

The default allocator behavior depends on the request size. We have a concept
of so called costly allocations (with order > PAGE_ALLOC_COSTLY_ORDER).
!costly allocations are too essential to fail so they are implicitly
non-failing by default (with some exceptions like OOM victims might fail so
the caller still has to check for failures) while costly requests try to be
not disruptive and back off even without invoking the OOM killer.
The following three modifiers might be used to override some of these
implicit rules

__GFP_NORETRY: The VM implementation will try only very lightweight
memory direct reclaim to get some memory under memory pressure (thus
it can sleep). It will avoid disruptive actions like OOM killer. The
caller must handle the failure which is quite likely to happen under
heavy memory pressure. The flag is suitable when failure can easily be
handled at small cost, such as reduced throughput

__GFP_RETRY_MAYFAIL: The VM implementation will retry memory reclaim
procedures that have previously failed if there is some indication
that progress has been made else where.  It can wait for other
tasks to attempt high level approaches to freeing memory such as
compaction (which removes fragmentation) and page-out.
There is still a definite limit to the number of retries, but it is
a larger limit than with __GFP_NORETRY.
Allocations with this flag may fail, but only when there is
genuinely little unused memory. While these allocations do not
directly trigger the OOM killer, their failure indicates that
the system is likely to need to use the OOM killer soon.  The
caller must handle failure, but can reasonably do so by failing
a higher-level request, or completing it only in a much less
efficient manner.
If the allocation does fail, and the caller is in a position to
free some non-essential memory, doing so could benefit the system
as a whole.

__GFP_NOFAIL: The VM implementation _must_ retry infinitely: the caller
cannot handle allocation failures. The allocation could block
indefinitely but will never return with failure. Testing for
failure is pointless.
New users should be evaluated carefully (and the flag should be
used only when there is no reasonable failure policy) but it is
definitely preferable to use the flag rather than opencode endless
loop around allocator.
Using this flag for costly allocations is _highly_ discouraged.

Useful GFP flag combinations that are commonly used. It is recommended
that subsystems start with one of these combinations and then set/clear
__GFP_FOO flags as necessary.

GFP_ATOMIC users can not sleep and need the allocation to succeed. A lower
watermark is applied to allow access to 'atomic reserves'.
The current implementation doesn't support NMI and few other strict
non-preemptive contexts (e.g. raw_spin_lock). The same applies to GFP_NOWAIT.

GFP_KERNEL is typical for kernel-internal allocations. The caller requires
ZONE_NORMAL or a lower zone for direct access but can direct reclaim.

GFP_KERNEL_ACCOUNT is the same as GFP_KERNEL, except the allocation is
accounted to kmemcg.

GFP_NOWAIT is for kernel allocations that should not stall for direct
reclaim, start physical IO or use any filesystem callback.

GFP_NOIO will use direct reclaim to discard clean pages or slab pages
that do not require the starting of any physical IO.
Please try to avoid using this flag directly and instead use
memalloc_noio_{save,restore} to mark the whole scope which cannot
perform any IO with a short explanation why. All allocation requests
will inherit GFP_NOIO implicitly.

GFP_NOFS will use direct reclaim but will not use any filesystem interfaces.
Please try to avoid using this flag directly and instead use
memalloc_nofs_{save,restore} to mark the whole scope which cannot/shouldn't
recurse into the FS layer with a short explanation why. All allocation
requests will inherit GFP_NOFS implicitly.

GFP_USER is for userspace allocations that also need to be directly
accessibly by the kernel or hardware. It is typically used by hardware
for buffers that are mapped to userspace (e.g. graphics) that hardware
still must DMA to. cpuset limits are enforced for these allocations.

GFP_DMA exists for historical reasons and should be avoided where possible.
The flags indicates that the caller requires that the lowest zone be
used (ZONE_DMA or 16M on x86-64). Ideally, this would be removed but
it would require careful auditing as some users really require it and
others use the flag to avoid lowmem reserves in ZONE_DMA and treat the
lowest zone as a type of emergency reserve.

GFP_DMA32 is similar to GFP_DMA except that the caller requires a 32-bit
address. Note that kmalloc(...¦, GFP_DMA32) does not return DMA32 memory
because the DMA32 kmalloc cache array is not implemented.
(Reason: there is no such user in kernel).

GFP_HIGHUSER is for userspace allocations that may be mapped to userspace,
do not need to be directly accessible by the kernel but that cannot
move once in use. An example may be a hardware allocation that maps
data directly into userspace but has no addressing limitations.

GFP_HIGHUSER_MOVABLE is for userspace allocations that the kernel does not
need direct access to but can use kmap() when access is required. They
are expected to be movable via page reclaim or page migration. Typically,
pages on the LRU would also be allocated with GFP_HIGHUSER_MOVABLE.

GFP_TRANSHUGE and GFP_TRANSHUGE_LIGHT are used for THP allocations. They
are compound allocations that will generally fail quickly if memory is not
available and will not wake kswapd/kcompactd on failure. The _LIGHT
version does not attempt reclaim/compaction at all and is by default used
in page fault path, while the non-light is used by khugepaged.


====================


void * kmalloc(size_t size, gfp_t flags)
allocate memory

Parameters


size_t size
how many bytes of memory are required.

gfp_t flags
the type of memory to allocate.

Description

kmalloc is the normal method of allocating memory
for objects smaller than page size in the kernel.

The allocated object address is aligned to at least ARCH_KMALLOC_MINALIGN
bytes. For size of power of two bytes, the alignment is also guaranteed
to be at least to the size.

The flags argument may be one of the GFP flags defined at
include/linux/gfp.h and described at
Documentation/core-api/mm-api.rst

The recommended usage of the flags is described at
Documentation/core-api/memory-allocation.rst

Below is a brief outline of the most useful GFP flags


GFP_KERNEL
Allocate normal kernel ram. May sleep.

GFP_NOWAIT
Allocation will not sleep.

GFP_ATOMIC
Allocation will not sleep.  May use emergency pools.

GFP_HIGHUSER
Allocate memory from high memory on behalf of user.

Also it is possible to set different flags by OR'ing
in one or more of the following additional flags:


__GFP_HIGH
This allocation has high priority and may use emergency pools.

__GFP_NOFAIL
Indicate that this allocation is in no way allowed to fail
(think twice before using).

__GFP_NORETRY
If memory is not immediately available,
then give up at once.

__GFP_NOWARN
If allocation fails, don't issue any warnings.

__GFP_RETRY_MAYFAIL
Try really hard to succeed the allocation but fail
eventually.


====================


void * kmalloc_array(size_t n, size_t size, gfp_t flags)
allocate memory for an array.

Parameters


size_t n
number of elements.

size_t size
element size.

gfp_t flags
the type of memory to allocate (see kmalloc).


====================


void * krealloc_array(void *p, size_t new_n, size_t new_size, gfp_t flags)
reallocate memory for an array.

Parameters


void *p
pointer to the memory chunk to reallocate

size_t new_n
new number of elements to alloc

size_t new_size
new size of a single member of the array

gfp_t flags
the type of memory to allocate (see kmalloc)


====================


void * kcalloc(size_t n, size_t size, gfp_t flags)
allocate memory for an array. The memory is set to zero.

Parameters


size_t n
number of elements.

size_t size
element size.

gfp_t flags
the type of memory to allocate (see kmalloc).


====================


void * kzalloc(size_t size, gfp_t flags)
allocate memory. The memory is set to zero.

Parameters


size_t size
how many bytes of memory are required.

gfp_t flags
the type of memory to allocate (see kmalloc).


====================


void * kzalloc_node(size_t size, gfp_t flags, int node)
allocate zeroed memory from a particular memory node.

Parameters


size_t size
how many bytes of memory are required.

gfp_t flags
the type of memory to allocate (see kmalloc).

int node
memory node from which to allocate


====================


void * kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags)
Allocate an object

Parameters


struct kmem_cache *cachep
The cache to allocate from.

gfp_t flags
See kmalloc().

Description

Allocate an object from this cache.  The flags are only relevant
if the cache has no available objects.

Return

pointer to the new object or NULL in case of error


====================


void * kmem_cache_alloc_node(struct kmem_cache *cachep, gfp_t flags, int nodeid)
Allocate an object on the specified node

Parameters


struct kmem_cache *cachep
The cache to allocate from.

gfp_t flags
See kmalloc().

int nodeid
node number of the target node.

Description

Identical to kmem_cache_alloc but it will allocate memory on the given
node, which can improve the performance for cpu bound structures.

Fallback to other node is possible if __GFP_THISNODE is not set.

Return

pointer to the new object or NULL in case of error


====================


void kmem_cache_free(struct kmem_cache *cachep, void *objp)
Deallocate an object

Parameters


struct kmem_cache *cachep
The cache the allocation was from.

void *objp
The previously allocated object.

Description

Free an object which was previously allocated from this
cache.


====================


void kfree(const void *objp)
free previously allocated memory

Parameters


const void *objp
pointer returned by kmalloc.

Description

If objp is NULL, no operation is performed.

Don't free memory not originally allocated by kmalloc()
or you will run into trouble.


====================


size_t __ksize(const void *objp)

Uninstrumented ksize.


Parameters


const void *objp
pointer to the object

Description

Unlike ksize(), __ksize() is uninstrumented, and does not provide the same
safety checks as ksize() with KASAN instrumentation enabled.

Return

size of the actual memory used by objp in bytes


====================


struct kmem_cache * kmem_cache_create_usercopy(const char *name, unsigned int size, unsigned int align, slab_flags_t flags, unsigned int useroffset, unsigned int usersize, void (*ctor)(void *))
Create a cache with a region suitable for copying to userspace

Parameters


const char *name
A string which is used in /proc/slabinfo to identify this cache.

unsigned int size
The size of objects to be created in this cache.

unsigned int align
The required alignment for the objects.

slab_flags_t flags
SLAB flags

unsigned int useroffset
Usercopy region offset

unsigned int usersize
Usercopy region size

void (*ctor)(void *)
A constructor for the objects.

Description

Cannot be called within a interrupt, but can be interrupted.
The ctor is run when new pages are allocated by the cache.

The flags are

SLAB_POISON - Poison the slab with a known test pattern (a5a5a5a5)
to catch references to uninitialised memory.

SLAB_RED_ZONE - Insert Red zones around the allocated memory to check
for buffer overruns.

SLAB_HWCACHE_ALIGN - Align the objects in this cache to a hardware
cacheline.  This can be beneficial if you're counting cycles as closely
as davem.

Return

a pointer to the cache on success, NULL on failure.


====================


struct kmem_cache * kmem_cache_create(const char *name, unsigned int size, unsigned int align, slab_flags_t flags, void (*ctor)(void *))
Create a cache.

Parameters


const char *name
A string which is used in /proc/slabinfo to identify this cache.

unsigned int size
The size of objects to be created in this cache.

unsigned int align
The required alignment for the objects.

slab_flags_t flags
SLAB flags

void (*ctor)(void *)
A constructor for the objects.

Description

Cannot be called within a interrupt, but can be interrupted.
The ctor is run when new pages are allocated by the cache.

The flags are

SLAB_POISON - Poison the slab with a known test pattern (a5a5a5a5)
to catch references to uninitialised memory.

SLAB_RED_ZONE - Insert Red zones around the allocated memory to check
for buffer overruns.

SLAB_HWCACHE_ALIGN - Align the objects in this cache to a hardware
cacheline.  This can be beneficial if you're counting cycles as closely
as davem.

Return

a pointer to the cache on success, NULL on failure.


====================


int kmem_cache_shrink(struct kmem_cache *cachep)
Shrink a cache.

Parameters


struct kmem_cache *cachep
The cache to shrink.

Description

Releases as many slabs as possible for a cache.
To help debugging, a zero exit status indicates all slabs were released.

Return

0 if all slabs were released, non-zero otherwise


====================


bool kmem_valid_obj(void *object)
does the pointer reference a valid slab object?

Parameters


void *object
pointer to query.

Return

true if the pointer is to a not-yet-freed object from
kmalloc() or kmem_cache_alloc(), either true or false if the pointer
is to an already-freed object, and false otherwise.


====================


void kmem_dump_obj(void *object)
Print available slab provenance information

Parameters


void *object
slab object for which to find provenance information.

Description

This function uses pr_cont(), so that the caller is expected to have
printed out whatever preamble is appropriate.  The provenance information
depends on the type of object and on how much debugging is enabled.
For a slab-cache object, the fact that it is a slab object is printed,
and, if available, the slab name, return address, and stack trace from
the allocation and last free path of that object.

This function will splat if passed a pointer to a non-slab object.
If you are not sure what type of object you have, you should instead
use mem_dump_obj().


====================


void * krealloc(const void *p, size_t new_size, gfp_t flags)
reallocate memory. The contents will remain unchanged.

Parameters


const void *p
object to reallocate memory for.

size_t new_size
how many bytes of memory are required.

gfp_t flags
the type of memory to allocate.

Description

The contents of the object pointed to are preserved up to the
lesser of the new and old sizes (__GFP_ZERO flag is effectively ignored).
If p is NULL, krealloc() behaves exactly like kmalloc().  If new_size
is 0 and p is not a NULL pointer, the object pointed to is freed.

Return

pointer to the allocated memory or NULL in case of error


====================


void kfree_sensitive(const void *p)
Clear sensitive information in memory before freeing

Parameters


const void *p
object to free memory of

Description

The memory of the object p points to is zeroed before freed.
If p is NULL, kfree_sensitive() does nothing.

Note

this function zeroes the whole allocated buffer which can be a good
deal bigger than the requested buffer size passed to kmalloc(). So be
careful when using this function in performance sensitive code.


====================


size_t ksize(const void *objp)
get the actual amount of memory allocated for a given object

Parameters


const void *objp
Pointer to the object

Description

kmalloc may internally round up allocations and return more memory
than requested. ksize() can be used to determine the actual amount of
memory allocated. The caller may use this additional memory, even though
a smaller amount of memory was initially specified with the kmalloc call.
The caller must guarantee that objp points to a valid object previously
allocated with either kmalloc() or kmem_cache_alloc(). The object
must not be freed during the duration of the call.

Return

size of the actual memory used by objp in bytes


====================


void kfree_const(const void *x)
conditionally free memory

Parameters


const void *x
pointer to the memory

Description

Function calls kfree only if x is not in .rodata section.


====================


void * kvmalloc_node(size_t size, gfp_t flags, int node)
attempt to allocate physically contiguous memory, but upon failure, fall back to non-contiguous (vmalloc) allocation.

Parameters


size_t size
size of the request.

gfp_t flags
gfp mask for the allocation - must be compatible (superset) with GFP_KERNEL.

int node
numa node to allocate from

Description

Uses kmalloc to get the memory but if the allocation fails then falls back
to the vmalloc allocator. Use kvfree for freeing the memory.

GFP_NOWAIT and GFP_ATOMIC are not supported, neither is the __GFP_NORETRY modifier.
__GFP_RETRY_MAYFAIL is supported, and it should be used only if kmalloc is
preferable to the vmalloc fallback, due to visible performance drawbacks.

Return

pointer to the allocated memory of NULL in case of failure


====================


void kvfree(const void *addr)
Free memory.

Parameters


const void *addr
Pointer to allocated memory.

Description

kvfree frees memory allocated by any of vmalloc(), kmalloc() or kvmalloc().
It is slightly more efficient to use kfree() or vfree() if you are certain
that you know which one to use.

Context

Either preemptible task context or not-NMI interrupt.


====================


void vm_unmap_aliases(void)
unmap outstanding lazy aliases in the vmap layer

Parameters


void
no arguments

Description

The vmap/vmalloc layer lazily flushes kernel virtual mappings primarily
to amortize TLB flushing overheads. What this means is that any page you
have now, may, in a former life, have been mapped into kernel virtual
address by the vmap layer and so there might be some CPUs with TLB entries
still referencing that page (additional to the regular 1:1 kernel mapping).

vm_unmap_aliases flushes all such lazy mappings. After it returns, we can
be sure that none of the pages we have control over will have any aliases
from the vmap layer.


====================


void vm_unmap_ram(const void *mem, unsigned int count)
unmap linear kernel address space set up by vm_map_ram

Parameters


const void *mem
the pointer returned by vm_map_ram

unsigned int count
the count passed to that vm_map_ram call (cannot unmap partial)


====================


void * vm_map_ram(struct page **pages, unsigned int count, int node)
map pages linearly into kernel virtual address (vmalloc space)

Parameters


struct page **pages
an array of pointers to the pages to be mapped

unsigned int count
number of pages

int node
prefer to allocate data structures on this node

Description

If you use this function for less than VMAP_MAX_ALLOC pages, it could be
faster than vmap so it's good.  But if you mix long-life and short-life
objects with vm_map_ram(), it could consume lots of address space through
fragmentation (especially on a 32bit machine).  You could see failures in
the end.  Please use this function for short-lived objects.

Return

a pointer to the address that has been mapped, or NULL on failure


====================


void vfree(const void *addr)
Release memory allocated by vmalloc()

Parameters


const void *addr
Memory base address

Description

Free the virtually continuous memory area starting at addr, as obtained
from one of the vmalloc() family of APIs.  This will usually also free the
physical memory underlying the virtual allocation, but that memory is
reference counted, so it will not be freed until the last user goes away.

If addr is NULL, no operation is performed.

Context

May sleep if called not from interrupt context.
Must not be called in NMI context (strictly speaking, it could be
if we have CONFIG_ARCH_HAVE_NMI_SAFE_CMPXCHG, but making the calling
conventions for vfree() arch-dependent would be a really bad idea).


====================


void vunmap(const void *addr)
release virtual mapping obtained by vmap()

Parameters


const void *addr
memory base address

Description

Free the virtually contiguous memory area starting at addr,
which was created from the page array passed to vmap().

Must not be called in interrupt context.


====================


void * vmap(struct page **pages, unsigned int count, unsigned long flags, pgprot_t prot)
map an array of pages into virtually contiguous space

Parameters


struct page **pages
array of page pointers

unsigned int count
number of pages to map

unsigned long flags
vm_area->flags

pgprot_t prot
page protection for the mapping

Description

Maps count pages from pages into contiguous kernel virtual space.
If flags contains VM_MAP_PUT_PAGES the ownership of the pages array itself
(which must be kmalloc or vmalloc memory) and one reference per pages in it
are transferred from the caller to vmap(), and will be freed / dropped when
vfree() is called on the return value.

Return

the address of the area or NULL on failure


====================


void * vmap_pfn(unsigned long *pfns, unsigned int count, pgprot_t prot)
map an array of PFNs into virtually contiguous space

Parameters


unsigned long *pfns
array of PFNs

unsigned int count
number of pages to map

pgprot_t prot
page protection for the mapping

Description

Maps count PFNs from pfns into contiguous kernel virtual space and returns
the start address of the mapping.


====================


void * __vmalloc_node(unsigned long size, unsigned long align, gfp_t gfp_mask, int node, const void *caller)
allocate virtually contiguous memory

Parameters


unsigned long size
allocation size

unsigned long align
desired alignment

gfp_t gfp_mask
flags for the page level allocator

int node
node to use for allocation or NUMA_NO_NODE

const void *caller
caller's return address

Description

Allocate enough pages to cover size from the page level allocator with
gfp_mask flags.  Map them into contiguous kernel virtual space.

Reclaim modifiers in gfp_mask - __GFP_NORETRY, __GFP_RETRY_MAYFAIL
and __GFP_NOFAIL are not supported

Any use of gfp flags outside of GFP_KERNEL should be consulted
with mm people.

Return

pointer to the allocated memory or NULL on error


====================


void * vmalloc(unsigned long size)
allocate virtually contiguous memory

Parameters


unsigned long size
allocation size

Description

Allocate enough pages to cover size from the page level
allocator and map them into contiguous kernel virtual space.

For tight control over page level allocator and protection flags
use __vmalloc() instead.

Return

pointer to the allocated memory or NULL on error


====================


void * vmalloc_huge(unsigned long size, gfp_t gfp_mask)
allocate virtually contiguous memory, allow huge pages

Parameters


unsigned long size
allocation size

gfp_t gfp_mask
flags for the page level allocator

Description

Allocate enough pages to cover size from the page level
allocator and map them into contiguous kernel virtual space.
If size is greater than or equal to PMD_SIZE, allow using
huge pages for the memory

Return

pointer to the allocated memory or NULL on error


====================


void * vzalloc(unsigned long size)
allocate virtually contiguous memory with zero fill

Parameters


unsigned long size
allocation size

Description

Allocate enough pages to cover size from the page level
allocator and map them into contiguous kernel virtual space.
The memory allocated is set to zero.

For tight control over page level allocator and protection flags
use __vmalloc() instead.

Return

pointer to the allocated memory or NULL on error


====================


void * vmalloc_user(unsigned long size)
allocate zeroed virtually contiguous memory for userspace

Parameters


unsigned long size
allocation size

Description

The resulting memory area is zeroed so it can be mapped to userspace
without leaking data.

Return

pointer to the allocated memory or NULL on error


====================


void * vmalloc_node(unsigned long size, int node)
allocate memory on a specific node

Parameters


unsigned long size
allocation size

int node
numa node

Description

Allocate enough pages to cover size from the page level
allocator and map them into contiguous kernel virtual space.

For tight control over page level allocator and protection flags
use __vmalloc() instead.

Return

pointer to the allocated memory or NULL on error


====================


void * vzalloc_node(unsigned long size, int node)
allocate memory on a specific node with zero fill

Parameters


unsigned long size
allocation size

int node
numa node

Description

Allocate enough pages to cover size from the page level
allocator and map them into contiguous kernel virtual space.
The memory allocated is set to zero.

Return

pointer to the allocated memory or NULL on error


====================


void * vmalloc_32(unsigned long size)
allocate virtually contiguous memory (32bit addressable)

Parameters


unsigned long size
allocation size

Description

Allocate enough 32bit PA addressable pages to cover size from the
page level allocator and map them into contiguous kernel virtual space.

Return

pointer to the allocated memory or NULL on error


====================


void * vmalloc_32_user(unsigned long size)
allocate zeroed virtually contiguous 32bit memory

Parameters


unsigned long size
allocation size

Description

The resulting memory area is 32bit addressable and zeroed so it can be
mapped to userspace without leaking data.

Return

pointer to the allocated memory or NULL on error


====================


int remap_vmalloc_range(struct vm_area_struct *vma, void *addr, unsigned long pgoff)
map vmalloc pages to userspace

Parameters


struct vm_area_struct *vma
vma to cover (map full range of vma)

void *addr
vmalloc memory

unsigned long pgoff
number of pages into addr before first page to map

Return

0 for success, -Exxx on failure

Description

This function checks that addr is a valid vmalloc'ed area, and
that it is big enough to cover the vma. Will return failure if
that criteria isn't met.

Similar to remap_pfn_range() (see mm/memory.c)


====================


int filemap_fdatawrite_wbc(struct address_space *mapping, struct writeback_control *wbc)
start writeback on mapping dirty pages in range

Parameters


struct address_space *mapping
address space structure to write

struct writeback_control *wbc
the writeback_control controlling the writeout

Description

Call writepages on the mapping using the provided wbc to control the
writeout.

Return

0 on success, negative error code otherwise.


====================


int filemap_flush(struct address_space *mapping)
mostly a non-blocking flush

Parameters


struct address_space *mapping
target address_space

Description

This is a mostly non-blocking flush.  Not suitable for data-integrity
purposes - I/O may not be started against all dirty pages.

Return

0 on success, negative error code otherwise.


====================


bool filemap_range_has_page(struct address_space *mapping, loff_t start_byte, loff_t end_byte)
check if a page exists in range.

Parameters


struct address_space *mapping
address space within which to check

loff_t start_byte
offset in bytes where the range starts

loff_t end_byte
offset in bytes where the range ends (inclusive)

Description

Find at least one page in the range supplied, usually used to check if
direct writing in this range will trigger a writeback.

Return

true if at least one page exists in the specified range,
false otherwise.


====================


int filemap_fdatawait_range(struct address_space *mapping, loff_t start_byte, loff_t end_byte)
wait for writeback to complete

Parameters


struct address_space *mapping
address space structure to wait for

loff_t start_byte
offset in bytes where the range starts

loff_t end_byte
offset in bytes where the range ends (inclusive)

Description

Walk the list of under-writeback pages of the given address space
in the given range and wait for all of them.  Check error status of
the address space and return it.

Since the error status of the address space is cleared by this function,
callers are responsible for checking the return value and handling and/or
reporting the error.

Return

error status of the address space.


====================


int filemap_fdatawait_range_keep_errors(struct address_space *mapping, loff_t start_byte, loff_t end_byte)
wait for writeback to complete

Parameters


struct address_space *mapping
address space structure to wait for

loff_t start_byte
offset in bytes where the range starts

loff_t end_byte
offset in bytes where the range ends (inclusive)

Description

Walk the list of under-writeback pages of the given address space in the
given range and wait for all of them.  Unlike filemap_fdatawait_range(),
this function does not clear error status of the address space.

Use this function if callers don't handle errors themselves.  Expected
call sites are system-wide / filesystem-wide data flushers: e.g. sync(2),
fsfreeze(8)


====================


int file_fdatawait_range(struct file *file, loff_t start_byte, loff_t end_byte)
wait for writeback to complete

Parameters


struct file *file
file pointing to address space structure to wait for

loff_t start_byte
offset in bytes where the range starts

loff_t end_byte
offset in bytes where the range ends (inclusive)

Description

Walk the list of under-writeback pages of the address space that file
refers to, in the given range and wait for all of them.  Check error
status of the address space vs. the file->f_wb_err cursor and return it.

Since the error status of the file is advanced by this function,
callers are responsible for checking the return value and handling and/or
reporting the error.

Return

error status of the address space vs. the file->f_wb_err cursor.


====================


int filemap_fdatawait_keep_errors(struct address_space *mapping)
wait for writeback without clearing errors

Parameters


struct address_space *mapping
address space structure to wait for

Description

Walk the list of under-writeback pages of the given address space
and wait for all of them.  Unlike filemap_fdatawait(), this function
does not clear error status of the address space.

Use this function if callers don't handle errors themselves.  Expected
call sites are system-wide / filesystem-wide data flushers: e.g. sync(2),
fsfreeze(8)

Return

error status of the address space.


====================


int filemap_write_and_wait_range(struct address_space *mapping, loff_t lstart, loff_t lend)
write out & wait on a file range

Parameters


struct address_space *mapping
the address_space for the pages

loff_t lstart
offset in bytes where the range starts

loff_t lend
offset in bytes where the range ends (inclusive)

Description

Write out and wait upon file offsets lstart->lend, inclusive.

Note that lend is inclusive (describes the last byte to be written) so
that this function can be used to write to the very end-of-file (end = -1).

Return

error status of the address space.


====================


int file_check_and_advance_wb_err(struct file *file)
report wb error (if any) that was previously and advance wb_err to current one

Parameters


struct file *file
struct file on which the error is being reported

Description

When userland calls fsync (or something like nfsd does the equivalent), we
want to report any writeback errors that occurred since the last fsync (or
since the file was opened if there haven't been any).

Grab the wb_err from the mapping. If it matches what we have in the file,
then just quickly return 0. The file is all caught up.

If it doesn't match, then take the mapping value, set the 'seen' flag in
it and try to swap it into place. If it works, or another task beat us
to it with the new value, then update the f_wb_err and return the error
portion. The error at this point must be reported via proper channels
(a'la fsync, or NFS COMMIT operation, etc.).

While we handle mapping->wb_err with atomic operations, the f_wb_err
value is protected by the f_lock since we must ensure that it reflects
the latest value swapped in for this file descriptor.

Return

0 on success, negative error code otherwise.


====================


int file_write_and_wait_range(struct file *file, loff_t lstart, loff_t lend)
write out & wait on a file range

Parameters


struct file *file
file pointing to address_space with pages

loff_t lstart
offset in bytes where the range starts

loff_t lend
offset in bytes where the range ends (inclusive)

Description

Write out and wait upon file offsets lstart->lend, inclusive.

Note that lend is inclusive (describes the last byte to be written) so
that this function can be used to write to the very end-of-file (end = -1).

After writing out and waiting on the data, we check and advance the
f_wb_err cursor to the latest value, and return any errors detected there.

Return

0 on success, negative error code otherwise.


====================


void replace_page_cache_page(struct page *old, struct page *new)
replace a pagecache page with a new one

Parameters


struct page *old
page to be replaced

struct page *new
page to replace with

Description

This function replaces a page in the pagecache with a new one.  On
success it acquires the pagecache reference for the new page and
drops it for the old page.  Both the old and new pages must be
locked.  This function does not add the new page to the LRU, the
caller must do that.

The remove + add is atomic.  This function cannot fail.


====================


int add_to_page_cache_locked(struct page *page, struct address_space *mapping, pgoff_t offset, gfp_t gfp_mask)
add a locked page to the pagecache

Parameters


struct page *page
page to add

struct address_space *mapping
the page's address_space

pgoff_t offset
page index

gfp_t gfp_mask
page allocation mode

Description

This function is used to add a page to the pagecache. It must be locked.
This function does not add the page to the LRU.  The caller must do that.

Return

0 on success, negative error code otherwise.


====================


void folio_add_wait_queue(struct folio *folio, wait_queue_entry_t *waiter)
Add an arbitrary waiter to a folio's wait queue

Parameters


struct folio *folio
Folio defining the wait queue of interest

wait_queue_entry_t *waiter
Waiter to add to the queue

Description

Add an arbitrary waiter to the wait queue for the nominated folio.


====================


void folio_unlock(struct folio *folio)
Unlock a locked folio.

Parameters


struct folio *folio
The folio.

Description

Unlocks the folio and wakes up any thread sleeping on the page lock.

Context

May be called from interrupt or process context.  May not be
called from NMI context.


====================


void folio_end_private_2(struct folio *folio)
Clear PG_private_2 and wake any waiters.

Parameters


struct folio *folio
The folio.

Description

Clear the PG_private_2 bit on a folio and wake up any sleepers waiting for
it.  The folio reference held for PG_private_2 being set is released.

This is, for example, used when a netfs folio is being written to a local
disk cache, thereby allowing writes to the cache for the same folio to be
serialised.


====================


void folio_wait_private_2(struct folio *folio)
Wait for PG_private_2 to be cleared on a folio.

Parameters


struct folio *folio
The folio to wait on.

Description

Wait for PG_private_2 (aka PG_fscache) to be cleared on a folio.


====================


int folio_wait_private_2_killable(struct folio *folio)
Wait for PG_private_2 to be cleared on a folio.

Parameters


struct folio *folio
The folio to wait on.

Description

Wait for PG_private_2 (aka PG_fscache) to be cleared on a folio or until a
fatal signal is received by the calling task.

Return


0 if successful.
-EINTR if a fatal signal was encountered.

====================


void folio_end_writeback(struct folio *folio)
End writeback against a folio.

Parameters


struct folio *folio
The folio.


====================


void __folio_lock(struct folio *folio)
Get a lock on the folio, assuming we need to sleep to get it.

Parameters


struct folio *folio
The folio to lock


====================


pgoff_t page_cache_next_miss(struct address_space *mapping, pgoff_t index, unsigned long max_scan)
Find the next gap in the page cache.

Parameters


struct address_space *mapping
Mapping.

pgoff_t index
Index.

unsigned long max_scan
Maximum range to search.

Description

Search the range [index, min(index + max_scan - 1, ULONG_MAX)] for the
gap with the lowest index.

This function may be called under the rcu_read_lock.  However, this will
not atomically search a snapshot of the cache at a single point in time.
For example, if a gap is created at index 5, then subsequently a gap is
created at index 10, page_cache_next_miss covering both indices may
return 10 if called under the rcu_read_lock.

Return

The index of the gap if found, otherwise an index outside the
range specified (in which case 'return - index >= max_scan' will be true).
In the rare case of index wrap-around, 0 will be returned.


====================


pgoff_t page_cache_prev_miss(struct address_space *mapping, pgoff_t index, unsigned long max_scan)
Find the previous gap in the page cache.

Parameters


struct address_space *mapping
Mapping.

pgoff_t index
Index.

unsigned long max_scan
Maximum range to search.

Description

Search the range [max(index - max_scan + 1, 0), index] for the
gap with the highest index.

This function may be called under the rcu_read_lock.  However, this will
not atomically search a snapshot of the cache at a single point in time.
For example, if a gap is created at index 10, then subsequently a gap is
created at index 5, page_cache_prev_miss() covering both indices may
return 5 if called under the rcu_read_lock.

Return

The index of the gap if found, otherwise an index outside the
range specified (in which case 'index - return >= max_scan' will be true).
In the rare case of wrap-around, ULONG_MAX will be returned.


====================


struct folio * __filemap_get_folio(struct address_space *mapping, pgoff_t index, int fgp_flags, gfp_t gfp)
Find and get a reference to a folio.

Parameters


struct address_space *mapping
The address_space to search.

pgoff_t index
The page index.

int fgp_flags
FGP flags modify how the folio is returned.

gfp_t gfp
Memory allocation flags to use if FGP_CREAT is specified.

Description

Looks up the page cache entry at mapping & index.

fgp_flags can be zero or more of these flags:


FGP_ACCESSED - The folio will be marked accessed.
FGP_LOCK - The folio is returned locked.
FGP_ENTRY - If there is a shadow / swap / DAX entry, return it
instead of allocating a new folio to replace it.
FGP_CREAT - If no page is present then a new page is allocated using
gfp and added to the page cache and the VM's LRU list.
The page is returned locked and with an increased refcount.
FGP_FOR_MMAP - The caller wants to do its own locking dance if the
page is already in cache.  If the page was allocated, unlock it before
returning so the caller can do the same dance.
FGP_WRITE - The page will be written to by the caller.
FGP_NOFS - __GFP_FS will get cleared in gfp.
FGP_NOWAIT - Don't get blocked by page lock.
FGP_STABLE - Wait for the folio to be stable (finished writeback)
If FGP_LOCK or FGP_CREAT are specified then the function may sleep even
if the GFP flags specified for FGP_CREAT are atomic.

If there is a page cache page, it is returned with an increased refcount.

Return

The found folio or NULL otherwise.


====================


unsigned find_get_pages_contig(struct address_space *mapping, pgoff_t index, unsigned int nr_pages, struct page **pages)
gang contiguous pagecache lookup

Parameters


struct address_space *mapping
The address_space to search

pgoff_t index
The starting page index

unsigned int nr_pages
The maximum number of pages

struct page **pages
Where the resulting pages are placed

Description

find_get_pages_contig() works exactly like find_get_pages_range(),
except that the returned number of pages are guaranteed to be
contiguous.

Return

the number of pages which were found.


====================


unsigned find_get_pages_range_tag(struct address_space *mapping, pgoff_t *index, pgoff_t end, xa_mark_t tag, unsigned int nr_pages, struct page **pages)
Find and return head pages matching tag.

Parameters


struct address_space *mapping
the address_space to search

pgoff_t *index
the starting page index

pgoff_t end
The final page index (inclusive)

xa_mark_t tag
the tag index

unsigned int nr_pages
the maximum number of pages

struct page **pages
where the resulting pages are placed

Description

Like find_get_pages_range(), except we only return head pages which are
tagged with tag.  index is updated to the index immediately after the
last page we return, ready for the next iteration.

Return

the number of pages which were found.


====================


ssize_t filemap_read(struct kiocb *iocb, struct iov_iter *iter, ssize_t already_read)
Read data from the page cache.

Parameters


struct kiocb *iocb
The iocb to read.

struct iov_iter *iter
Destination for the data.

ssize_t already_read
Number of bytes already read by the caller.

Description

Copies data from the page cache.  If the data is not currently present,
uses the readahead and readpage address_space operations to fetch it.

Return

Total number of bytes copied, including those already read by
the caller.  If an error happens before any bytes are copied, returns
a negative error number.


====================


ssize_t generic_file_read_iter(struct kiocb *iocb, struct iov_iter *iter)
generic filesystem read routine

Parameters


struct kiocb *iocb
kernel I/O control block

struct iov_iter *iter
destination for the data read

Description

This is the 'read_iter()' routine for all filesystems
that can use the page cache directly.

The IOCB_NOWAIT flag in iocb->ki_flags indicates that -EAGAIN shall
be returned when no data can be read without waiting for I/O requests
to complete; it doesn't prevent readahead.

The IOCB_NOIO flag in iocb->ki_flags indicates that no new I/O
requests shall be made for the read or for readahead.  When no data
can be read, -EAGAIN shall be returned.  When readahead would be
triggered, a partial, possibly empty read shall be returned.

Return


number of bytes copied, even for partial reads
negative error code (or 0 if IOCB_NOIO) if nothing was read

====================


vm_fault_t filemap_fault(struct vm_fault *vmf)
read in file data for page fault handling

Parameters


struct vm_fault *vmf
struct vm_fault containing details of the fault

Description

filemap_fault() is invoked via the vma operations vector for a
mapped memory region to read in file data during a page fault.

The goto's are kind of ugly, but this streamlines the normal case of having
it in the page cache, and handles the special cases reasonably without
having a lot of duplicated code.

vma->vm_mm->mmap_lock must be held on entry.

If our return value has VM_FAULT_RETRY set, it's because the mmap_lock
may be dropped before doing I/O or by lock_folio_maybe_drop_mmap().

If our return value does not have VM_FAULT_RETRY set, the mmap_lock
has not been released.

We never return with VM_FAULT_RETRY and a bit from VM_FAULT_ERROR set.

Return

bitwise-OR of VM_FAULT_ codes.


====================


struct folio * read_cache_folio(struct address_space *mapping, pgoff_t index, filler_t filler, void *data)
read into page cache, fill it if needed

Parameters


struct address_space *mapping
the page's address_space

pgoff_t index
the page index

filler_t filler
function to perform the read

void *data
first arg to filler(data, page) function, often left as NULL

Description

Read into the page cache. If a page already exists, and PageUptodate() is
not set, try to fill the page and wait for it to become unlocked.

If the page does not get brought uptodate, return -EIO.

The function expects mapping->invalidate_lock to be already held.

Return

up to date page on success, ERR_PTR() on failure.


====================


struct page * read_cache_page_gfp(struct address_space *mapping, pgoff_t index, gfp_t gfp)
read into page cache, using specified page allocation flags.

Parameters


struct address_space *mapping
the page's address_space

pgoff_t index
the page index

gfp_t gfp
the page allocator flags to use if allocating

Description

This is the same as 'read_mapping_page(mapping, index, NULL)', but with
any new page allocations done using the specified allocation flags.

If the page does not get brought uptodate, return -EIO.

The function expects mapping->invalidate_lock to be already held.

Return

up to date page on success, ERR_PTR() on failure.


====================


ssize_t __generic_file_write_iter(struct kiocb *iocb, struct iov_iter *from)
write data to a file

Parameters


struct kiocb *iocb
IO state structure (file, offset, etc.)

struct iov_iter *from
iov_iter with data to write

Description

This function does all the work needed for actually writing data to a
file. It does all basic checks, removes SUID from the file, updates
modification times and calls proper subroutines depending on whether we
do direct IO or a standard buffered write.

It expects i_rwsem to be grabbed unless we work on a block device or similar
object which does not need locking at all.

This function does not take care of syncing data in case of O_SYNC write.
A caller has to handle it. This is mainly due to the fact that we want to
avoid syncing under i_rwsem.

Return


number of bytes written, even for truncated writes
negative error code if no data has been written at all

====================


ssize_t generic_file_write_iter(struct kiocb *iocb, struct iov_iter *from)
write data to a file

Parameters


struct kiocb *iocb
IO state structure

struct iov_iter *from
iov_iter with data to write

Description

This is a wrapper around __generic_file_write_iter() to be used by most
filesystems. It takes care of syncing the file in case of O_SYNC file
and acquires i_rwsem as needed.

Return


negative error code if no data has been written at all of
vfs_fsync_range() failed for a synchronous write
number of bytes written, even for truncated writes

====================


bool filemap_release_folio(struct folio *folio, gfp_t gfp)
Release fs-specific metadata on a folio.

Parameters


struct folio *folio
The folio which the kernel is trying to free.

gfp_t gfp
Memory allocation flags (and I/O mode).

Description

The address_space is trying to release any data attached to a folio
(presumably at folio->private).

This will also be called if the private_2 flag is set on a page,
indicating that the folio has other metadata associated with it.

The gfp argument specifies whether I/O may be performed to release
this page (__GFP_IO), and whether the call may block
(__GFP_RECLAIM & __GFP_FS).

Return

true if the release was successful, otherwise false.

Readahead is used to read content into the page cache before it is
explicitly requested by the application.  Readahead only ever
attempts to read folios that are not yet in the page cache.  If a
folio is present but not up-to-date, readahead will not try to read
it. In that case a simple ->readpage() will be requested.

Readahead is triggered when an application read request (whether a
system call or a page fault) finds that the requested folio is not in
the page cache, or that it is in the page cache and has the
readahead flag set.  This flag indicates that the folio was read
as part of a previous readahead request and now that it has been
accessed, it is time for the next readahead.

Each readahead request is partly synchronous read, and partly async
readahead.  This is reflected in the struct file_ra_state which
contains ->size being the total number of pages, and ->async_size
which is the number of pages in the async section.  The readahead
flag will be set on the first folio in this async section to trigger
a subsequent readahead.  Once a series of sequential reads has been
established, there should be no need for a synchronous component and
all readahead request will be fully asynchronous.

When either of the triggers causes a readahead, three numbers need
to be determined: the start of the region to read, the size of the
region, and the size of the async tail.

The start of the region is simply the first page address at or after
the accessed address, which is not currently populated in the page
cache.  This is found with a simple search in the page cache.

The size of the async tail is determined by subtracting the size that
was explicitly requested from the determined request size, unless
this would be less than zero - then zero is used.  NOTE THIS
CALCULATION IS WRONG WHEN THE START OF THE REGION IS NOT THE ACCESSED
PAGE.  ALSO THIS CALCULATION IS NOT USED CONSISTENTLY.

The size of the region is normally determined from the size of the
previous readahead which loaded the preceding pages.  This may be
discovered from the struct file_ra_state for simple sequential reads,
or from examining the state of the page cache when multiple
sequential reads are interleaved.  Specifically: where the readahead
was triggered by the readahead flag, the size of the previous
readahead is assumed to be the number of pages from the triggering
page to the start of the new readahead.  In these cases, the size of
the previous readahead is scaled, often doubled, for the new
readahead, though see get_next_ra_size() for details.

If the size of the previous read cannot be determined, the number of
preceding pages in the page cache is used to estimate the size of
a previous read.  This estimate could easily be misled by random
reads being coincidentally adjacent, so it is ignored unless it is
larger than the current request, and it is not scaled up, unless it
is at the start of file.

In general readahead is accelerated at the start of the file, as
reads from there are often sequential.  There are other minor
adjustments to the readahead size in various special cases and these
are best discovered by reading the code.

The above calculation, based on the previous readahead size,
determines the size of the readahead, to which any requested read
size may be added.

Readahead requests are sent to the filesystem using the ->readahead()
address space operation, for which mpage_readahead() is a canonical
implementation.  ->readahead() should normally initiate reads on all
folios, but may fail to read any or all folios without causing an I/O
error.  The page cache reading code will issue a ->readpage() request
for any folio which ->readahead() did not read, and only an error
from this will be final.

->readahead() will generally call readahead_folio() repeatedly to get
each folio from those prepared for readahead.  It may fail to read a
folio by:


not calling readahead_folio() sufficiently many times, effectively
ignoring some folios, as might be appropriate if the path to
storage is congested.
failing to actually submit a read request for a given folio,
possibly due to insufficient resources, or
getting an error during subsequent processing of a request.
In the last two cases, the folio should be unlocked by the filesystem
to indicate that the read attempt has failed.  In the first case the
folio will be unlocked by the VFS.

Those folios not in the final async_size of the request should be
considered to be important and ->readahead() should not fail them due
to congestion or temporary resource unavailability, but should wait
for necessary resources (e.g.  memory or indexing information) to
become available.  Folios in the final async_size may be
considered less urgent and failure to read them is more acceptable.
In this case it is best to use filemap_remove_folio() to remove the
folios from the page cache as is automatically done for folios that
were not fetched with readahead_folio().  This will allow a
subsequent synchronous readahead request to try them again.  If they
are left in the page cache, then they will be read individually using
->readpage() which may be less efficient.


====================


void page_cache_ra_unbounded(struct readahead_control *ractl, unsigned long nr_to_read, unsigned long lookahead_size)
Start unchecked readahead.

Parameters


struct readahead_control *ractl
Readahead control.

unsigned long nr_to_read
The number of pages to read.

unsigned long lookahead_size
Where to start the next readahead.

Description

This function is for filesystems to call when they want to start
readahead beyond a file's stated i_size.  This is almost certainly
not the function you want to call.  Use page_cache_async_readahead()
or page_cache_sync_readahead() instead.

Context

File is referenced by caller.  Mutexes may be held by caller.
May sleep, but will not reenter filesystem to reclaim memory.


====================


void readahead_expand(struct readahead_control *ractl, loff_t new_start, size_t new_len)
Expand a readahead request

Parameters


struct readahead_control *ractl
The request to be expanded

loff_t new_start
The revised start

size_t new_len
The revised size of the request

Description

Attempt to expand a readahead request outwards from the current size to the
specified size by inserting locked pages before and after the current window
to increase the size to the new window.  This may involve the insertion of
THPs, in which case the window may get expanded even beyond what was
requested.

The algorithm will stop if it encounters a conflicting page already in the
pagecache and leave a smaller expansion than requested.

The caller must check for this by examining the revised ractl object for a
different expansion than was requested.


====================


void balance_dirty_pages_ratelimited(struct address_space *mapping)
balance dirty memory state

Parameters


struct address_space *mapping
address_space which was dirtied

Description

Processes which are dirtying memory should call in here once for each page
which was newly dirtied.  The function will periodically check the system's
dirty state and will initiate writeback if needed.

Once we're over the dirty memory limit we decrease the ratelimiting
by a lot, to prevent individual processes from overshooting the limit
by (ratelimit_pages) each.


====================


void tag_pages_for_writeback(struct address_space *mapping, pgoff_t start, pgoff_t end)
tag pages to be written by write_cache_pages

Parameters


struct address_space *mapping
address space structure to write

pgoff_t start
starting page index

pgoff_t end
ending page index (inclusive)

Description

This function scans the page range from start to end (inclusive) and tags
all pages that have DIRTY tag set with a special TOWRITE tag. The idea is
that write_cache_pages (or whoever calls this function) will then use
TOWRITE tag to identify pages eligible for writeback.  This mechanism is
used to avoid livelocking of writeback by a process steadily creating new
dirty pages in the file (thus it is important for this function to be quick
so that it can tag pages faster than a dirtying process can create them).


====================


int write_cache_pages(struct address_space *mapping, struct writeback_control *wbc, writepage_t writepage, void *data)
walk the list of dirty pages of the given address space and write all of them.

Parameters


struct address_space *mapping
address space structure to write

struct writeback_control *wbc
subtract the number of written pages from *wbc->nr_to_write

writepage_t writepage
function called for each page

void *data
data passed to writepage function

Description

If a page is already under I/O, write_cache_pages() skips it, even
if it's dirty.  This is desirable behaviour for memory-cleaning writeback,
but it is INCORRECT for data-integrity system calls such as fsync().  fsync()
and msync() need to guarantee that all the data which was dirty at the time
the call was made get new I/O started against them.  If wbc->sync_mode is
WB_SYNC_ALL then we were called for data integrity and we must wait for
existing IO to complete.

To avoid livelocks (when other process dirties new pages), we first tag
pages which should be written back with TOWRITE tag and only then start
writing them. For data-integrity sync we have to be careful so that we do
not miss some pages (e.g., because some other process has cleared TOWRITE
tag we set). The rule we follow is that TOWRITE tag can be cleared only
by the process clearing the DIRTY tag (and submitting the page for IO).

To avoid deadlocks between range_cyclic writeback and callers that hold
pages in PageWriteback to aggregate IO until write_cache_pages() returns,
we do not loop back to the start of the file. Doing so causes a page
lock/page writeback access order inversion - we should only ever lock
multiple pages in ascending page->index order, and looping back to the start
of the file violates that rule and causes deadlocks.

Return

0 on success, negative error code otherwise


====================


int generic_writepages(struct address_space *mapping, struct writeback_control *wbc)
walk the list of dirty pages of the given address space and writepage() all of them.

Parameters


struct address_space *mapping
address space structure to write

struct writeback_control *wbc
subtract the number of written pages from *wbc->nr_to_write

Description

This is a library function, which implements the writepages()
address_space_operation.

Return

0 on success, negative error code otherwise


====================


int folio_write_one(struct folio *folio)
write out a single folio and wait on I/O.

Parameters


struct folio *folio
The folio to write.

Description

The folio must be locked by the caller and will be unlocked upon return.

Note that the mapping's AS_EIO/AS_ENOSPC flags will be cleared when this
function returns.

Return

0 on success, negative error code otherwise


====================


bool filemap_dirty_folio(struct address_space *mapping, struct folio *folio)
Mark a folio dirty for filesystems which do not use buffer_heads.

Parameters


struct address_space *mapping
Address space this folio belongs to.

struct folio *folio
Folio to be marked as dirty.

Description

Filesystems which do not use buffer heads should call this function
from their set_page_dirty address space operation.  It ignores the
contents of folio_get_private(), so if the filesystem marks individual
blocks as dirty, the filesystem should handle that itself.

This is also sometimes used by filesystems which use buffer_heads when
a single buffer is being dirtied: we want to set the folio dirty in
that case, but not all the buffers.  This is a 'bottom-up' dirtying,
whereas block_dirty_folio() is a 'top-down' dirtying.

The caller must ensure this doesn't race with truncation.  Most will
simply hold the folio lock, but e.g. zap_pte_range() calls with the
folio mapped and the pte lock held, which also locks out truncation.


====================


void folio_account_redirty(struct folio *folio)
Manually account for redirtying a page.

Parameters


struct folio *folio
The folio which is being redirtied.

Description

Most filesystems should call folio_redirty_for_writepage() instead
of this fuction.  If your filesystem is doing writeback outside the
context of a writeback_control(), it can call this when redirtying
a folio, to de-account the dirty counters (NR_DIRTIED, WB_DIRTIED,
tsk->nr_dirtied), so that they match the written counters (NR_WRITTEN,
WB_WRITTEN) in long term. The mismatches will lead to systematic errors
in balanced_dirty_ratelimit and the dirty pages position control.


====================


bool folio_redirty_for_writepage(struct writeback_control *wbc, struct folio *folio)
Decline to write a dirty folio.

Parameters


struct writeback_control *wbc
The writeback control.

struct folio *folio
The folio.

Description

When a writepage implementation decides that it doesn't want to write
folio for some reason, it should call this function, unlock folio and
return 0.

Return

True if we redirtied the folio.  False if someone else dirtied
it first.


====================


bool folio_mark_dirty(struct folio *folio)
Mark a folio as being modified.

Parameters


struct folio *folio
The folio.

Description

For folios with a mapping this should be done with the folio lock held
for the benefit of asynchronous memory errors who prefer a consistent
dirty state. This rule can be broken in some special cases,
but should be better not to.

Return

True if the folio was newly dirtied, false if it was already dirty.


====================


void folio_wait_writeback(struct folio *folio)
Wait for a folio to finish writeback.

Parameters


struct folio *folio
The folio to wait for.

Description

If the folio is currently being written back to storage, wait for the
I/O to complete.

Context

Sleeps.  Must be called in process context and with
no spinlocks held.  Caller should hold a reference on the folio.
If the folio is not locked, writeback may start again after writeback
has finished.


====================


int folio_wait_writeback_killable(struct folio *folio)
Wait for a folio to finish writeback.

Parameters


struct folio *folio
The folio to wait for.

Description

If the folio is currently being written back to storage, wait for the
I/O to complete or a fatal signal to arrive.

Context

Sleeps.  Must be called in process context and with
no spinlocks held.  Caller should hold a reference on the folio.
If the folio is not locked, writeback may start again after writeback
has finished.

Return

0 on success, -EINTR if we get a fatal signal while waiting.


====================


void folio_wait_stable(struct folio *folio)
wait for writeback to finish, if necessary.

Parameters


struct folio *folio
The folio to wait on.

Description

This function determines if the given folio is related to a backing
device that requires folio contents to be held stable during writeback.
If so, then it will wait for any pending writeback to complete.

Context

Sleeps.  Must be called in process context and with
no spinlocks held.  Caller should hold a reference on the folio.
If the folio is not locked, writeback may start again after writeback
has finished.


====================


void folio_invalidate(struct folio *folio, size_t offset, size_t length)
Invalidate part or all of a folio.

Parameters


struct folio *folio
The folio which is affected.

size_t offset
start of the range to invalidate

size_t length
length of the range to invalidate

Description

folio_invalidate() is called when all or part of the folio has become
invalidated by a truncate operation.

folio_invalidate() does not have to release all buffers, but it must
ensure that no dirty buffer is left outside offset and that no I/O
is underway against any of the blocks which are outside the truncation
point.  Because the caller is about to free (and possibly reuse) those
blocks on-disk.


====================


void truncate_inode_pages_range(struct address_space *mapping, loff_t lstart, loff_t lend)
truncate range of pages specified by start & end byte offsets

Parameters


struct address_space *mapping
mapping to truncate

loff_t lstart
offset from which to truncate

loff_t lend
offset to which to truncate (inclusive)

Description

Truncate the page cache, removing the pages that are between
specified offsets (and zeroing out partial pages
if lstart or lend + 1 is not page aligned).

Truncate takes two passes - the first pass is nonblocking.  It will not
block on page locks and it will not block on writeback.  The second pass
will wait.  This is to prevent as much IO as possible in the affected region.
The first pass will remove most pages, so the search cost of the second pass
is low.

We pass down the cache-hot hint to the page freeing code.  Even if the
mapping is large, it is probably the case that the final pages are the most
recently touched, and freeing happens in ascending file offset order.

Note that since ->invalidate_folio() accepts range to invalidate
truncate_inode_pages_range is able to handle cases where lend + 1 is not
page aligned properly.


====================


void truncate_inode_pages(struct address_space *mapping, loff_t lstart)
truncate all the pages from an offset

Parameters


struct address_space *mapping
mapping to truncate

loff_t lstart
offset from which to truncate

Description

Called under (and serialised by) inode->i_rwsem and
mapping->invalidate_lock.

Note

When this function returns, there can be a page in the process of
deletion (inside __delete_from_page_cache()) in the specified range.  Thus
mapping->nrpages can be non-zero when this function returns even after
truncation of the whole mapping.


====================


void truncate_inode_pages_final(struct address_space *mapping)
truncate all pages before inode dies

Parameters


struct address_space *mapping
mapping to truncate

Description

Called under (and serialized by) inode->i_rwsem.

Filesystems have to use this in the .evict_inode path to inform the
VM that this is the final truncate and the inode is going away.


====================


unsigned long invalidate_mapping_pages(struct address_space *mapping, pgoff_t start, pgoff_t end)
Invalidate all clean, unlocked cache of one inode

Parameters


struct address_space *mapping
the address_space which holds the cache to invalidate

pgoff_t start
the offset 'from' which to invalidate

pgoff_t end
the offset 'to' which to invalidate (inclusive)

Description

This function removes pages that are clean, unmapped and unlocked,
as well as shadow entries. It will not block on IO activity.

If you want to remove all the pages of one inode, regardless of
their use and writeback state, use truncate_inode_pages().

Return

the number of the cache entries that were invalidated


====================


int invalidate_inode_pages2_range(struct address_space *mapping, pgoff_t start, pgoff_t end)
remove range of pages from an address_space

Parameters


struct address_space *mapping
the address_space

pgoff_t start
the page offset 'from' which to invalidate

pgoff_t end
the page offset 'to' which to invalidate (inclusive)

Description

Any pages which are found to be mapped into pagetables are unmapped prior to
invalidation.

Return

-EBUSY if any pages could not be invalidated.


====================


int invalidate_inode_pages2(struct address_space *mapping)
remove all pages from an address_space

Parameters


struct address_space *mapping
the address_space

Description

Any pages which are found to be mapped into pagetables are unmapped prior to
invalidation.

Return

-EBUSY if any pages could not be invalidated.


====================


void truncate_pagecache(struct inode *inode, loff_t newsize)
unmap and remove pagecache that has been truncated

Parameters


struct inode *inode
inode

loff_t newsize
new file size

Description

inode's new i_size must already be written before truncate_pagecache
is called.

This function should typically be called before the filesystem
releases resources associated with the freed range (eg. deallocates
blocks). This way, pagecache will always stay logically coherent
with on-disk format, and the filesystem would not have to deal with
situations such as writepage being called for a page that has already
had its underlying blocks deallocated.


====================


void truncate_setsize(struct inode *inode, loff_t newsize)
update inode and pagecache for a new file size

Parameters


struct inode *inode
inode

loff_t newsize
new file size

Description

truncate_setsize updates i_size and performs pagecache truncation (if
necessary) to newsize. It will be typically be called from the filesystem's
setattr function when ATTR_SIZE is passed in.

Must be called with a lock serializing truncates and writes (generally
i_rwsem but e.g. xfs uses a different lock) and before all filesystem
specific block truncation has been performed.


====================


void pagecache_isize_extended(struct inode *inode, loff_t from, loff_t to)
update pagecache after extension of i_size

Parameters


struct inode *inode
inode for which i_size was extended

loff_t from
original inode size

loff_t to
new inode size

Description

Handle extension of inode size either caused by extending truncate or by
write starting after current i_size. We mark the page straddling current
i_size RO so that page_mkwrite() is called on the nearest write access to
the page.  This way filesystem can be sure that page_mkwrite() is called on
the page before user writes to the page via mmap after the i_size has been
changed.

The function must be called after i_size is updated so that page fault
coming after we unlock the page will already see the new i_size.
The function must be called while we still hold i_rwsem - this not only
makes sure i_size is stable but also that userspace cannot observe new
i_size value before we are prepared to store mmap writes at new inode size.


====================


void truncate_pagecache_range(struct inode *inode, loff_t lstart, loff_t lend)
unmap and remove pagecache that is hole-punched

Parameters


struct inode *inode
inode

loff_t lstart
offset of beginning of hole

loff_t lend
offset of last byte of hole

Description

This function should typically be called before the filesystem
releases resources associated with the freed range (eg. deallocates
blocks). This way, pagecache will always stay logically coherent
with on-disk format, and the filesystem would not have to deal with
situations such as writepage being called for a page that has already
had its underlying blocks deallocated.


====================


void filemap_set_wb_err(struct address_space *mapping, int err)
set a writeback error on an address_space

Parameters


struct address_space *mapping
mapping in which to set writeback error

int err
error to be set in mapping

Description

When writeback fails in some way, we must record that error so that
userspace can be informed when fsync and the like are called.  We endeavor
to report errors on any file that was open at the time of the error.  Some
internal callers also need to know when writeback errors have occurred.

When a writeback error occurs, most filesystems will want to call
filemap_set_wb_err to record the error in the mapping so that it will be
automatically reported whenever fsync is called on the file.


====================


int filemap_check_wb_err(struct address_space *mapping, errseq_t since)
has an error occurred since the mark was sampled?

Parameters


struct address_space *mapping
mapping to check for writeback errors

errseq_t since
previously-sampled errseq_t

Description

Grab the errseq_t value from the mapping, and see if it has changed 'since'
the given value was sampled.

If it has then report the latest error set, otherwise return 0.


====================


errseq_t filemap_sample_wb_err(struct address_space *mapping)
sample the current errseq_t to test for later errors

Parameters


struct address_space *mapping
mapping to be sampled

Description

Writeback errors are always reported relative to a particular sample point
in the past. This function provides those sample points.


====================


errseq_t file_sample_sb_err(struct file *file)
sample the current errseq_t to test for later errors

Parameters


struct file *file
file pointer to be sampled

Description

Grab the most current superblock-level errseq_t value for the given
struct file.


====================


void mapping_set_error(struct address_space *mapping, int error)
record a writeback error in the address_space

Parameters


struct address_space *mapping
the mapping in which an error should be set

int error
the error to set in the mapping

Description

When writeback fails in some way, we must record that error so that
userspace can be informed when fsync and the like are called.  We endeavor
to report errors on any file that was open at the time of the error.  Some
internal callers also need to know when writeback errors have occurred.

When a writeback error occurs, most filesystems will want to call
mapping_set_error to record the error in the mapping so that it can be
reported when the application calls fsync(2).


====================


void mapping_set_large_folios(struct address_space *mapping)
Indicate the file supports large folios.

Parameters


struct address_space *mapping
The file.

Description

The filesystem should call this function in its inode constructor to
indicate that the VFS can use large folios to cache the contents of
the file.

Context

This should not be called while the inode is active as it
is non-atomic.


====================


struct address_space * folio_file_mapping(struct folio *folio)
Find the mapping this folio belongs to.

Parameters


struct folio *folio
The folio.

Description

For folios which are in the page cache, return the mapping that this
page belongs to.  Folios in the swap cache return the mapping of the
swap file or swap device where the data is stored.  This is different
from the mapping returned by folio_mapping().  The only reason to
use it is if, like NFS, you return 0 from ->activate_swapfile.

Do not call this for folios which aren't in the page cache or swap cache.


====================


struct inode * folio_inode(struct folio *folio)
Get the host inode for this folio.

Parameters


struct folio *folio
The folio.

Description

For folios which are in the page cache, return the inode that this folio
belongs to.

Do not call this for folios which aren't in the page cache.


====================


void folio_attach_private(struct folio *folio, void *data)
Attach private data to a folio.

Parameters


struct folio *folio
Folio to attach data to.

void *data
Data to attach to folio.

Description

Attaching private data to a folio increments the page's reference count.
The data must be detached before the folio will be freed.


====================


void * folio_change_private(struct folio *folio, void *data)
Change private data on a folio.

Parameters


struct folio *folio
Folio to change the data on.

void *data
Data to set on the folio.

Description

Change the private data attached to a folio and return the old
data.  The page must previously have had data attached and the data
must be detached before the folio will be freed.

Return

Data that was previously attached to the folio.


====================


void * folio_detach_private(struct folio *folio)
Detach private data from a folio.

Parameters


struct folio *folio
Folio to detach data from.

Description

Removes the data that was previously attached to the folio and decrements
the refcount on the page.

Return

Data that was attached to the folio.


====================


struct folio * filemap_get_folio(struct address_space *mapping, pgoff_t index)
Find and get a folio.

Parameters


struct address_space *mapping
The address_space to search.

pgoff_t index
The page index.

Description

Looks up the page cache entry at mapping & index.  If a folio is
present, it is returned with an increased refcount.

Otherwise, NULL is returned.


====================


struct folio * filemap_lock_folio(struct address_space *mapping, pgoff_t index)
Find and lock a folio.

Parameters


struct address_space *mapping
The address_space to search.

pgoff_t index
The page index.

Description

Looks up the page cache entry at mapping & index.  If a folio is
present, it is returned locked with an increased refcount.

Context

May sleep.

Return

A folio or NULL if there is no folio in the cache for this
index.  Will not return a shadow, swap or DAX entry.


====================


struct page * find_get_page(struct address_space *mapping, pgoff_t offset)
find and get a page reference

Parameters


struct address_space *mapping
the address_space to search

pgoff_t offset
the page index

Description

Looks up the page cache slot at mapping & offset.  If there is a
page cache page, it is returned with an increased refcount.

Otherwise, NULL is returned.


====================


struct page * find_lock_page(struct address_space *mapping, pgoff_t index)
locate, pin and lock a pagecache page

Parameters


struct address_space *mapping
the address_space to search

pgoff_t index
the page index

Description

Looks up the page cache entry at mapping & index.  If there is a
page cache page, it is returned locked and with an increased
refcount.

Context

May sleep.

Return

A struct page or NULL if there is no page in the cache for this
index.


====================


struct page * find_or_create_page(struct address_space *mapping, pgoff_t index, gfp_t gfp_mask)
locate or add a pagecache page

Parameters


struct address_space *mapping
the page's address_space

pgoff_t index
the page's index into the mapping

gfp_t gfp_mask
page allocation mode

Description

Looks up the page cache slot at mapping & offset.  If there is a
page cache page, it is returned locked and with an increased
refcount.

If the page is not present, a new page is allocated using gfp_mask
and added to the page cache and the VM's LRU list.  The page is
returned locked and with an increased refcount.

On memory exhaustion, NULL is returned.

find_or_create_page() may sleep, even if gfp_flags specifies an
atomic allocation!


====================


struct page * grab_cache_page_nowait(struct address_space *mapping, pgoff_t index)
returns locked page at given index in given cache

Parameters


struct address_space *mapping
target address_space

pgoff_t index
the page index

Description

Same as grab_cache_page(), but do not wait if the page is unavailable.
This is intended for speculative data generators, where the data can
be regenerated if the page couldn't be grabbed.  This routine should
be safe to call while holding the lock for another page.

Clear __GFP_FS when allocating the page to avoid recursion into the fs
and deadlock against the caller's locked page.


====================


pgoff_t folio_index(struct folio *folio)
File index of a folio.

Parameters


struct folio *folio
The folio.

Description

For a folio which is either in the page cache or the swap cache,
return its index within the address_space it belongs to.  If you know
the page is definitely in the page cache, you can look at the folio's
index directly.

Return

The index (offset in units of pages) of a folio in its file.


====================


pgoff_t folio_next_index(struct folio *folio)
Get the index of the next folio.

Parameters


struct folio *folio
The current folio.

Return

The index of the folio which follows this folio in the file.


====================


struct page * folio_file_page(struct folio *folio, pgoff_t index)
The page for a particular index.

Parameters


struct folio *folio
The folio which contains this index.

pgoff_t index
The index we want to look up.

Description

Sometimes after looking up a folio in the page cache, we need to
obtain the specific page for an index (eg a page fault).

Return

The page containing the file data for this index.


====================


bool folio_contains(struct folio *folio, pgoff_t index)
Does this folio contain this index?

Parameters


struct folio *folio
The folio.

pgoff_t index
The page index within the file.

Context

The caller should have the page locked in order to prevent
(eg) shmem from moving the page between the page cache and swap cache
and changing its index in the middle of the operation.

Return

true or false.


====================


loff_t folio_pos(struct folio *folio)
Returns the byte position of this folio in its file.

Parameters


struct folio *folio
The folio.


====================


loff_t folio_file_pos(struct folio *folio)
Returns the byte position of this folio in its file.

Parameters


struct folio *folio
The folio.

Description

This differs from folio_pos() for folios which belong to a swap file.
NFS is the only filesystem today which needs to use folio_file_pos().


====================


bool filemap_range_needs_writeback(struct address_space *mapping, loff_t start_byte, loff_t end_byte)
check if range potentially needs writeback

Parameters


struct address_space *mapping
address space within which to check

loff_t start_byte
offset in bytes where the range starts

loff_t end_byte
offset in bytes where the range ends (inclusive)

Description

Find at least one page in the range supplied, usually used to check if
direct writing in this range will trigger a writeback. Used by O_DIRECT
read/write with IOCB_NOWAIT, to see if the caller needs to do
filemap_write_and_wait_range() before proceeding.

Return

true if the caller should do filemap_write_and_wait_range() before
doing O_DIRECT to a page in this range, false otherwise.



struct readahead_control
Describes a readahead request.

Definition

struct readahead_control {
  struct file *file;
  struct address_space *mapping;
  struct file_ra_state *ra;
};


Members


file
The file, used primarily by network filesystems for authentication.
May be NULL if invoked internally by the filesystem.

mapping
Readahead this filesystem object.

ra
File readahead state.  May be NULL.

Description

A readahead request is for consecutive pages.  Filesystems which
implement the ->readahead method should call readahead_page() or
readahead_page_batch() in a loop and attempt to start I/O against
each page in the request.

Most of the fields in this struct are private and should be accessed
by the functions below.


====================


void page_cache_sync_readahead(struct address_space *mapping, struct file_ra_state *ra, struct file *file, pgoff_t index, unsigned long req_count)
generic file readahead

Parameters


struct address_space *mapping
address_space which holds the pagecache and I/O vectors

struct file_ra_state *ra
file_ra_state which holds the readahead state

struct file *file
Used by the filesystem for authentication.

pgoff_t index
Index of first page to be read.

unsigned long req_count
Total number of pages being read by the caller.

Description

page_cache_sync_readahead() should be called when a cache miss happened:
it will submit the read.  The readahead logic may decide to piggyback more
pages onto the read request if access patterns suggest it will improve
performance.


====================


void page_cache_async_readahead(struct address_space *mapping, struct file_ra_state *ra, struct file *file, struct page *page, pgoff_t index, unsigned long req_count)
file readahead for marked pages

Parameters


struct address_space *mapping
address_space which holds the pagecache and I/O vectors

struct file_ra_state *ra
file_ra_state which holds the readahead state

struct file *file
Used by the filesystem for authentication.

struct page *page
The page at index which triggered the readahead call.

pgoff_t index
Index of first page to be read.

unsigned long req_count
Total number of pages being read by the caller.

Description

page_cache_async_readahead() should be called when a page is used which
is marked as PageReadahead; this is a marker to suggest that the application
has used up enough of the readahead window that we should start pulling in
more pages.


====================


struct page * readahead_page(struct readahead_control *ractl)
Get the next page to read.

Parameters


struct readahead_control *ractl
The current readahead request.

Context

The page is locked and has an elevated refcount.  The caller
should decreases the refcount once the page has been submitted for I/O
and unlock the page once all I/O to that page has completed.

Return

A pointer to the next page, or NULL if we are done.


====================


struct folio * readahead_folio(struct readahead_control *ractl)
Get the next folio to read.

Parameters


struct readahead_control *ractl
The current readahead request.

Context

The folio is locked.  The caller should unlock the folio once
all I/O to that folio has completed.

Return

A pointer to the next folio, or NULL if we are done.


====================


readahead_page_batch(rac, array)
Get a batch of pages to read.

Parameters


rac
The current readahead request.

array
An array of pointers to struct page.

Context

The pages are locked and have an elevated refcount.  The caller
should decreases the refcount once the page has been submitted for I/O
and unlock the page once all I/O to that page has completed.

Return

The number of pages placed in the array.  0 indicates the request
is complete.


====================


loff_t readahead_pos(struct readahead_control *rac)
The byte offset into the file of this readahead request.

Parameters


struct readahead_control *rac
The readahead request.


====================


size_t readahead_length(struct readahead_control *rac)
The number of bytes in this readahead request.

Parameters


struct readahead_control *rac
The readahead request.


====================


pgoff_t readahead_index(struct readahead_control *rac)
The index of the first page in this readahead request.

Parameters


struct readahead_control *rac
The readahead request.


====================


unsigned int readahead_count(struct readahead_control *rac)
The number of pages in this readahead request.

Parameters


struct readahead_control *rac
The readahead request.


====================


size_t readahead_batch_length(struct readahead_control *rac)
The number of bytes in the current batch.

Parameters


struct readahead_control *rac
The readahead request.


====================


ssize_t folio_mkwrite_check_truncate(struct folio *folio, struct inode *inode)
check if folio was truncated

Parameters


struct folio *folio
the folio to check

struct inode *inode
the inode to check the folio against

Return

the number of bytes in the folio up to EOF,
or -EFAULT if the folio was truncated.


====================


int page_mkwrite_check_truncate(struct page *page, struct inode *inode)
check if page was truncated

Parameters


struct page *page
the page to check

struct inode *inode
the inode to check the page against

Description

Returns the number of bytes in the page up to EOF,
or -EFAULT if the page was truncated.


====================


unsigned int i_blocks_per_folio(struct inode *inode, struct folio *folio)
How many blocks fit in this folio.

Parameters


struct inode *inode
The inode which contains the blocks.

struct folio *folio
The folio.

Description

If the block size is larger than the size of this folio, return zero.

Context

The caller should hold a refcount on the folio to prevent it
from being split.

Return

The number of filesystem blocks covered by this folio.


====================


void mempool_exit(mempool_t *pool)
exit a mempool initialized with mempool_init()

Parameters


mempool_t *pool
pointer to the memory pool which was initialized with
mempool_init().

Description

Free all reserved elements in pool and pool itself.  This function
only sleeps if the free_fn() function sleeps.

May be called on a zeroed but uninitialized mempool (i.e. allocated with
kzalloc()).


====================


void mempool_destroy(mempool_t *pool)
deallocate a memory pool

Parameters


mempool_t *pool
pointer to the memory pool which was allocated via
mempool_create().

Description

Free all reserved elements in pool and pool itself.  This function
only sleeps if the free_fn() function sleeps.


====================


int mempool_init(mempool_t *pool, int min_nr, mempool_alloc_t *alloc_fn, mempool_free_t *free_fn, void *pool_data)
initialize a memory pool

Parameters


mempool_t *pool
pointer to the memory pool that should be initialized

int min_nr
the minimum number of elements guaranteed to be
allocated for this pool.

mempool_alloc_t *alloc_fn
user-defined element-allocation function.

mempool_free_t *free_fn
user-defined element-freeing function.

void *pool_data
optional private data available to the user-defined functions.

Description

Like mempool_create(), but initializes the pool in (i.e. embedded in another
structure).

Return

0 on success, negative error code otherwise.


====================


mempool_t * mempool_create(int min_nr, mempool_alloc_t *alloc_fn, mempool_free_t *free_fn, void *pool_data)
create a memory pool

Parameters


int min_nr
the minimum number of elements guaranteed to be
allocated for this pool.

mempool_alloc_t *alloc_fn
user-defined element-allocation function.

mempool_free_t *free_fn
user-defined element-freeing function.

void *pool_data
optional private data available to the user-defined functions.

Description

this function creates and allocates a guaranteed size, preallocated
memory pool. The pool can be used from the mempool_alloc() and mempool_free()
functions. This function might sleep. Both the alloc_fn() and the free_fn()
functions might sleep - as long as the mempool_alloc() function is not called
from IRQ contexts.

Return

pointer to the created memory pool object or NULL on error.


====================


int mempool_resize(mempool_t *pool, int new_min_nr)
resize an existing memory pool

Parameters


mempool_t *pool
pointer to the memory pool which was allocated via
mempool_create().

int new_min_nr
the new minimum number of elements guaranteed to be
allocated for this pool.

Description

This function shrinks/grows the pool. In the case of growing,
it cannot be guaranteed that the pool will be grown to the new
size immediately, but new mempool_free() calls will refill it.
This function may sleep.

Note, the caller must guarantee that no mempool_destroy is called
while this function is running. mempool_alloc() & mempool_free()
might be called (eg. from IRQ contexts) while this function executes.

Return

0 on success, negative error code otherwise.


====================


void * mempool_alloc(mempool_t *pool, gfp_t gfp_mask)
allocate an element from a specific memory pool

Parameters


mempool_t *pool
pointer to the memory pool which was allocated via
mempool_create().

gfp_t gfp_mask
the usual allocation bitmask.

Description

this function only sleeps if the alloc_fn() function sleeps or
returns NULL. Note that due to preallocation, this function
never fails when called from process contexts. (it might
fail if called from an IRQ context.)

Note

using __GFP_ZERO is not supported.

Return

pointer to the allocated element or NULL on error.


====================


void mempool_free(void *element, mempool_t *pool)
return an element to the pool.

Parameters


void *element
pool element pointer.

mempool_t *pool
pointer to the memory pool which was allocated via
mempool_create().

Description

this function only sleeps if the free_fn() function sleeps.


====================


struct dma_pool * dma_pool_create(const char *name, struct device *dev, size_t size, size_t align, size_t boundary)
Creates a pool of consistent memory blocks, for dma.

Parameters


const char *name
name of pool, for diagnostics

struct device *dev
device that will be doing the DMA

size_t size
size of the blocks in this pool.

size_t align
alignment requirement for blocks; must be a power of two

size_t boundary
returned blocks won't cross this power of two boundary

Context

not in_interrupt()

Description

Given one of these pools, dma_pool_alloc()
may be used to allocate memory.  Such memory will all have 'consistent'
DMA mappings, accessible by the device and its driver without using
cache flushing primitives.  The actual size of blocks allocated may be
larger than requested because of alignment.

If boundary is nonzero, objects returned from dma_pool_alloc() won't
cross that size boundary.  This is useful for devices which have
addressing restrictions on individual DMA transfers, such as not crossing
boundaries of 4KBytes.

Return

a dma allocation pool with the requested characteristics, or
NULL if one can't be created.


====================


void dma_pool_destroy(struct dma_pool *pool)
destroys a pool of dma memory blocks.

Parameters


struct dma_pool *pool
dma pool that will be destroyed

Context

!in_interrupt()

Description

Caller guarantees that no more memory from the pool is in use,
and that nothing will try to use the pool after this call.


====================


void * dma_pool_alloc(struct dma_pool *pool, gfp_t mem_flags, dma_addr_t *handle)
get a block of consistent memory

Parameters


struct dma_pool *pool
dma pool that will produce the block

gfp_t mem_flags
GFP_* bitmask

dma_addr_t *handle
pointer to dma address of block

Return

the kernel virtual address of a currently unused block,
and reports its dma address through the handle.
If such a memory block can't be allocated, NULL is returned.


====================


void dma_pool_free(struct dma_pool *pool, void *vaddr, dma_addr_t dma)
put block back into dma pool

Parameters


struct dma_pool *pool
the dma pool holding the block

void *vaddr
virtual address of block

dma_addr_t dma
dma address of block

Description

Caller promises neither device nor driver will again touch this block
unless it is first re-allocated.


====================


struct dma_pool * dmam_pool_create(const char *name, struct device *dev, size_t size, size_t align, size_t allocation)
Managed dma_pool_create()

Parameters


const char *name
name of pool, for diagnostics

struct device *dev
device that will be doing the DMA

size_t size
size of the blocks in this pool.

size_t align
alignment requirement for blocks; must be a power of two

size_t allocation
returned blocks won't cross this boundary (or zero)

Description

Managed dma_pool_create().  DMA pool created with this function is
automatically destroyed on driver detach.

Return

a managed dma allocation pool with the requested
characteristics, or NULL if one can't be created.


====================


void dmam_pool_destroy(struct dma_pool *pool)
Managed dma_pool_destroy()

Parameters


struct dma_pool *pool
dma pool that will be destroyed

Description

Managed dma_pool_destroy().


====================


void zap_vma_ptes(struct vm_area_struct *vma, unsigned long address, unsigned long size)
remove ptes mapping the vma

Parameters


struct vm_area_struct *vma
vm_area_struct holding ptes to be zapped

unsigned long address
starting address of pages to zap

unsigned long size
number of bytes to zap

Description

This function only unmaps ptes assigned to VM_PFNMAP vmas.

The entire address range must be fully contained within the vma.


====================


int vm_insert_pages(struct vm_area_struct *vma, unsigned long addr, struct page **pages, unsigned long *num)
insert multiple pages into user vma, batching the pmd lock.

Parameters


struct vm_area_struct *vma
user vma to map to

unsigned long addr
target start user address of these pages

struct page **pages
source kernel pages

unsigned long *num
in: number of pages to map. out: number of pages that were not
mapped. (0 means all pages were successfully mapped).

Description

Preferred over vm_insert_page() when inserting multiple pages.

In case of error, we may have mapped a subset of the provided
pages. It is the caller's responsibility to account for this case.

The same restrictions apply as in vm_insert_page().


====================


int vm_insert_page(struct vm_area_struct *vma, unsigned long addr, struct page *page)
insert single page into user vma

Parameters


struct vm_area_struct *vma
user vma to map to

unsigned long addr
target user address of this page

struct page *page
source kernel page

Description

This allows drivers to insert individual pages they've allocated
into a user vma.

The page has to be a nice clean _individual_ kernel allocation.
If you allocate a compound page, you need to have marked it as
such (__GFP_COMP), or manually just split the page up yourself
(see split_page()).

NOTE! Traditionally this was done with 'remap_pfn_range()' which
took an arbitrary page protection parameter. This doesn't allow
that. Your vma protection will have to be set up correctly, which
means that if you want a shared writable mapping, you'd better
ask for a shared writable mapping!

The page does not need to be reserved.

Usually this function is called from f_op->mmap() handler
under mm->mmap_lock write-lock, so it can change vma->vm_flags.
Caller must set VM_MIXEDMAP on vma if it wants to call this
function from other places, for example from page-fault handler.

Return

0 on success, negative error code otherwise.


====================


int vm_map_pages(struct vm_area_struct *vma, struct page **pages, unsigned long num)
maps range of kernel pages starts with non zero offset

Parameters


struct vm_area_struct *vma
user vma to map to

struct page **pages
pointer to array of source kernel pages

unsigned long num
number of pages in page array

Description

Maps an object consisting of num pages, catering for the user's
requested vm_pgoff

If we fail to insert any page into the vma, the function will return
immediately leaving any previously inserted pages present.  Callers
from the mmap handler may immediately return the error as their caller
will destroy the vma, removing any successfully inserted pages. Other
callers should make their own arrangements for calling unmap_region().

Context

Process context. Called by mmap handlers.

Return

0 on success and error code otherwise.


====================


int vm_map_pages_zero(struct vm_area_struct *vma, struct page **pages, unsigned long num)
map range of kernel pages starts with zero offset

Parameters


struct vm_area_struct *vma
user vma to map to

struct page **pages
pointer to array of source kernel pages

unsigned long num
number of pages in page array

Description

Similar to vm_map_pages(), except that it explicitly sets the offset
to 0. This function is intended for the drivers that did not consider
vm_pgoff.

Context

Process context. Called by mmap handlers.

Return

0 on success and error code otherwise.


====================


vm_fault_t vmf_insert_pfn_prot(struct vm_area_struct *vma, unsigned long addr, unsigned long pfn, pgprot_t pgprot)
insert single pfn into user vma with specified pgprot

Parameters


struct vm_area_struct *vma
user vma to map to

unsigned long addr
target user address of this page

unsigned long pfn
source kernel pfn

pgprot_t pgprot
pgprot flags for the inserted page

Description

This is exactly like vmf_insert_pfn(), except that it allows drivers
to override pgprot on a per-page basis.

This only makes sense for IO mappings, and it makes no sense for
COW mappings.  In general, using multiple vmas is preferable;
vmf_insert_pfn_prot should only be used if using multiple VMAs is
impractical.

See vmf_insert_mixed_prot() for a discussion of the implication of using
a value of pgprot different from that of vma->vm_page_prot.

Context

Process context.  May allocate using GFP_KERNEL.

Return

vm_fault_t value.


====================


vm_fault_t vmf_insert_pfn(struct vm_area_struct *vma, unsigned long addr, unsigned long pfn)
insert single pfn into user vma

Parameters


struct vm_area_struct *vma
user vma to map to

unsigned long addr
target user address of this page

unsigned long pfn
source kernel pfn

Description

Similar to vm_insert_page, this allows drivers to insert individual pages
they've allocated into a user vma. Same comments apply.

This function should only be called from a vm_ops->fault handler, and
in that case the handler should return the result of this function.

vma cannot be a COW mapping.

As this is called only for pages that do not currently exist, we
do not need to flush old virtual caches or the TLB.

Context

Process context.  May allocate using GFP_KERNEL.

Return

vm_fault_t value.


====================


vm_fault_t vmf_insert_mixed_prot(struct vm_area_struct *vma, unsigned long addr, pfn_t pfn, pgprot_t pgprot)
insert single pfn into user vma with specified pgprot

Parameters


struct vm_area_struct *vma
user vma to map to

unsigned long addr
target user address of this page

pfn_t pfn
source kernel pfn

pgprot_t pgprot
pgprot flags for the inserted page

Description

This is exactly like vmf_insert_mixed(), except that it allows drivers
to override pgprot on a per-page basis.

Typically this function should be used by drivers to set caching- and
encryption bits different than those of vma->vm_page_prot, because
the caching- or encryption mode may not be known at mmap() time.
This is ok as long as vma->vm_page_prot is not used by the core vm
to set caching and encryption bits for those vmas (except for COW pages).
This is ensured by core vm only modifying these page table entries using
functions that don't touch caching- or encryption bits, using pte_modify()
if needed. (See for example mprotect()).
Also when new page-table entries are created, this is only done using the
fault() callback, and never using the value of vma->vm_page_prot,
except for page-table entries that point to anonymous pages as the result
of COW.

Context

Process context.  May allocate using GFP_KERNEL.

Return

vm_fault_t value.


====================


int remap_pfn_range(struct vm_area_struct *vma, unsigned long addr, unsigned long pfn, unsigned long size, pgprot_t prot)
remap kernel memory to userspace

Parameters


struct vm_area_struct *vma
user vma to map to

unsigned long addr
target page aligned user address to start at

unsigned long pfn
page frame number of kernel physical memory address

unsigned long size
size of mapping area

pgprot_t prot
page protection flags for this mapping

Note

this is only safe if the mm semaphore is held when called.

Return

0 on success, negative error code otherwise.


====================


int vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len)
remap memory to userspace

Parameters


struct vm_area_struct *vma
user vma to map to

phys_addr_t start
start of the physical memory to be mapped

unsigned long len
size of area

Description

This is a simplified io_remap_pfn_range() for common driver use. The
driver just needs to give us the physical memory range to be mapped,
we'll figure out the rest from the vma information.

NOTE! Some drivers might want to tweak vma->vm_page_prot first to get
whatever write-combining details or similar.

Return

0 on success, negative error code otherwise.


====================


void unmap_mapping_pages(struct address_space *mapping, pgoff_t start, pgoff_t nr, bool even_cows)
Unmap pages from processes.

Parameters


struct address_space *mapping
The address space containing pages to be unmapped.

pgoff_t start
Index of first page to be unmapped.

pgoff_t nr
Number of pages to be unmapped.  0 to unmap to end of file.

bool even_cows
Whether to unmap even private COWed pages.

Description

Unmap the pages in this address space from any userspace process which
has them mmaped.  Generally, you want to remove COWed pages as well when
a file is being truncated, but not when invalidating pages from the page
cache.


====================


void unmap_mapping_range(struct address_space *mapping, loff_t const holebegin, loff_t const holelen, int even_cows)
unmap the portion of all mmaps in the specified address_space corresponding to the specified byte range in the underlying file.

Parameters


struct address_space *mapping
the address space containing mmaps to be unmapped.

loff_t const holebegin
byte in first page to unmap, relative to the start of
the underlying file.  This will be rounded down to a PAGE_SIZE
boundary.  Note that this is different from truncate_pagecache(), which
must keep the partial page.  In contrast, we must get rid of
partial pages.

loff_t const holelen
size of prospective hole in bytes.  This will be rounded
up to a PAGE_SIZE boundary.  A holelen of zero truncates to the
end of the file.

int even_cows
1 when truncating a file, unmap even private COWed pages;
but 0 when invalidating pagecache, don't throw away private data.


====================


int follow_pte(struct mm_struct *mm, unsigned long address, pte_t **ptepp, spinlock_t **ptlp)
look up PTE at a user virtual address

Parameters


struct mm_struct *mm
the mm_struct of the target address space

unsigned long address
user virtual address

pte_t **ptepp
location to store found PTE

spinlock_t **ptlp
location to store the lock for the PTE

Description

On a successful return, the pointer to the PTE is stored in ptepp;
the corresponding lock is taken and its location is stored in ptlp.
The contents of the PTE are only stable until ptlp is released;
any further use, if any, must be protected against invalidation
with MMU notifiers.

Only IO mappings and raw PFN mappings are allowed.  The mmap semaphore
should be taken for read.

KVM uses this function.  While it is arguably less bad than follow_pfn,
it is not a good general-purpose API.

Return

zero on success, -ve otherwise.


====================


int follow_pfn(struct vm_area_struct *vma, unsigned long address, unsigned long *pfn)
look up PFN at a user virtual address

Parameters


struct vm_area_struct *vma
memory mapping

unsigned long address
user virtual address

unsigned long *pfn
location to store found PFN

Description

Only IO mappings and raw PFN mappings are allowed.

This function does not allow the caller to read the permissions
of the PTE.  Do not use it.

Return

zero and the pfn at pfn on success, -ve otherwise.


====================


int generic_access_phys(struct vm_area_struct *vma, unsigned long addr, void *buf, int len, int write)
generic implementation for iomem mmap access

Parameters


struct vm_area_struct *vma
the vma to access

unsigned long addr
userspace address, not relative offset within vma

void *buf
buffer to read/write

int len
length of transfer

int write
set to FOLL_WRITE when writing, otherwise reading

Description

This is a generic implementation for vm_operations_struct.access for an
iomem mapping. This callback is used by access_process_vm() when the vma is
not page based.


====================


unsigned long get_pfnblock_flags_mask(const struct page *page, unsigned long pfn, unsigned long mask)
Return the requested group of flags for the pageblock_nr_pages block of pages

Parameters


const struct page *page
The page within the block of interest

unsigned long pfn
The target page frame number

unsigned long mask
mask of bits that the caller is interested in

Return

pageblock_bits flags


====================


void set_pfnblock_flags_mask(struct page *page, unsigned long flags, unsigned long pfn, unsigned long mask)
Set the requested group of flags for a pageblock_nr_pages block of pages

Parameters


struct page *page
The page within the block of interest

unsigned long flags
The flags to set

unsigned long pfn
The target page frame number

unsigned long mask
mask of bits that the caller is interested in


====================


void __putback_isolated_page(struct page *page, unsigned int order, int mt)
Return a now-isolated page back where we got it

Parameters


struct page *page
Page that was isolated

unsigned int order
Order of the isolated page

int mt
The page's pageblock's migratetype

Description

This function is meant to return a page pulled from the free lists via
__isolate_free_page back to the free lists they were pulled from.


====================


void __free_pages(struct page *page, unsigned int order)
Free pages allocated with alloc_pages().

Parameters


struct page *page
The page pointer returned from alloc_pages().

unsigned int order
The order of the allocation.

Description

This function can free multi-page allocations that are not compound
pages.  It does not check that the order passed in matches that of
the allocation, so it is easy to leak memory.  Freeing more memory
than was allocated will probably emit a warning.

If the last reference to this page is speculative, it will be released
by put_page() which only frees the first page of a non-compound
allocation.  To prevent the remaining pages from being leaked, we free
the subsequent pages here.  If you want to use the page's reference
count to decide when to free the allocation, you should allocate a
compound page, and use put_page() instead of __free_pages().

Context

May be called in interrupt context or while holding a normal
spinlock, but not in NMI context or while holding a raw spinlock.


====================


void * alloc_pages_exact(size_t size, gfp_t gfp_mask)
allocate an exact number physically-contiguous pages.

Parameters


size_t size
the number of bytes to allocate

gfp_t gfp_mask
GFP flags for the allocation, must not contain __GFP_COMP

Description

This function is similar to alloc_pages(), except that it allocates the
minimum number of pages to satisfy the request.  alloc_pages() can only
allocate memory in power-of-two pages.

This function is also limited by MAX_ORDER.

Memory allocated by this function must be released by free_pages_exact().

Return

pointer to the allocated area or NULL in case of error.


====================


void * alloc_pages_exact_nid(int nid, size_t size, gfp_t gfp_mask)
allocate an exact number of physically-contiguous pages on a node.

Parameters


int nid
the preferred node ID where memory should be allocated

size_t size
the number of bytes to allocate

gfp_t gfp_mask
GFP flags for the allocation, must not contain __GFP_COMP

Description

Like alloc_pages_exact(), but try to allocate on node nid first before falling
back.

Return

pointer to the allocated area or NULL in case of error.


====================


void free_pages_exact(void *virt, size_t size)
release memory allocated via alloc_pages_exact()

Parameters


void *virt
the value returned by alloc_pages_exact.

size_t size
size of allocation, same value as passed to alloc_pages_exact().

Description

Release the memory allocated by a previous call to alloc_pages_exact.


====================


unsigned long nr_free_zone_pages(int offset)
count number of pages beyond high watermark

Parameters


int offset
The zone index of the highest zone

Description

nr_free_zone_pages() counts the number of pages which are beyond the
high watermark within all zones at or below a given zone index.  For each
zone, the number of pages is calculated as:


nr_free_zone_pages = managed_pages - high_pages

Return

number of pages beyond high watermark.


====================


unsigned long nr_free_buffer_pages(void)
count number of pages beyond high watermark

Parameters


void
no arguments

Description

nr_free_buffer_pages() counts the number of pages which are beyond the high
watermark within ZONE_DMA and ZONE_NORMAL.

Return

number of pages beyond high watermark within ZONE_DMA and
ZONE_NORMAL.


====================


int find_next_best_node(int node, nodemask_t *used_node_mask)
find the next node that should appear in a given node's fallback list

Parameters


int node
node whose fallback list we're appending

nodemask_t *used_node_mask
nodemask_t of already used nodes

Description

We use a number of factors to determine which is the next node that should
appear on a given node's fallback list.  The node should not have appeared
already in node's fallback list, and it should be the next closest node
according to the distance array (which contains arbitrary distance values
from each node to each node in the system), and should also prefer nodes
with no CPUs, since presumably they'll have very little allocation pressure
on them otherwise.

Return

node id of the found node or NUMA_NO_NODE if no node is found.


====================


void get_pfn_range_for_nid(unsigned int nid, unsigned long *start_pfn, unsigned long *end_pfn)
Return the start and end page frames for a node

Parameters


unsigned int nid
The nid to return the range for. If MAX_NUMNODES, the min and max PFN are returned.

unsigned long *start_pfn
Passed by reference. On return, it will have the node start_pfn.

unsigned long *end_pfn
Passed by reference. On return, it will have the node end_pfn.

Description

It returns the start and end page frame of a node based on information
provided by memblock_set_node(). If called for a node
with no available memory, a warning is printed and the start and end
PFNs will be 0.


====================


unsigned long absent_pages_in_range(unsigned long start_pfn, unsigned long end_pfn)
Return number of page frames in holes within a range

Parameters


unsigned long start_pfn
The start PFN to start searching for holes

unsigned long end_pfn
The end PFN to stop searching for holes

Return

the number of pages frames in memory holes within a range.


====================


unsigned long node_map_pfn_alignment(void)
determine the maximum internode alignment

Parameters


void
no arguments

Description

This function should be called after node map is populated and sorted.
It calculates the maximum power of two alignment which can distinguish
all the nodes.

For example, if all nodes are 1GiB and aligned to 1GiB, the return value
would indicate 1GiB alignment with (1 << (30 - PAGE_SHIFT)).  If the
nodes are shifted by 256MiB, 256MiB.  Note that if only the last node is
shifted, 1GiB is enough and this function will indicate so.

This is used to test whether pfn -> nid mapping of the chosen memory
model has fine enough granularity to avoid incorrect mapping for the
populated node map.

Return

the determined alignment in pfn's.  0 if there is no alignment
requirement (single node).


====================


unsigned long find_min_pfn_with_active_regions(void)
Find the minimum PFN registered

Parameters


void
no arguments

Return

the minimum PFN based on information provided via
memblock_set_node().


====================


void free_area_init(unsigned long *max_zone_pfn)
Initialise all pg_data_t and zone data

Parameters


unsigned long *max_zone_pfn
an array of max PFNs for each zone

Description

This will call free_area_init_node() for each active node in the system.
Using the page ranges provided by memblock_set_node(), the size of each
zone in each node and their holes is calculated. If the maximum PFN
between two adjacent zones match, it is assumed that the zone is empty.
For example, if arch_max_dma_pfn == arch_max_dma32_pfn, it is assumed
that arch_max_dma32_pfn has no pages. It is also assumed that a zone
starts where the previous one ended. For example, ZONE_DMA32 starts
at arch_max_dma_pfn.


====================


void set_dma_reserve(unsigned long new_dma_reserve)
set the specified number of pages reserved in the first zone

Parameters


unsigned long new_dma_reserve
The number of pages to mark reserved

Description

The per-cpu batchsize and zone watermarks are determined by managed_pages.
In the DMA zone, a significant percentage may be consumed by kernel image
and other unfreeable allocations which can skew the watermarks badly. This
function may optionally be used to account for unfreeable pages in the
first zone (e.g., ZONE_DMA). The effect will be lower watermarks and
smaller per-cpu batchsize.


====================


void setup_per_zone_wmarks(void)
called when min_free_kbytes changes or when memory is hot-{added|removed}

Parameters


void
no arguments

Description

Ensures that the watermark[min,low,high] values for each zone are set
correctly with respect to min_free_kbytes.


====================


int alloc_contig_range(unsigned long start, unsigned long end, unsigned migratetype, gfp_t gfp_mask)

tries to allocate given range of pages


Parameters


unsigned long start
start PFN to allocate

unsigned long end
one-past-the-last PFN to allocate

unsigned migratetype
migratetype of the underlying pageblocks (either
#MIGRATE_MOVABLE or #MIGRATE_CMA).  All pageblocks
in range must have the same migratetype and it must
be either of the two.

gfp_t gfp_mask
GFP mask to use during compaction

Description

The PFN range does not have to be pageblock or MAX_ORDER_NR_PAGES
aligned.  The PFN range must belong to a single zone.

The first thing this routine does is attempt to MIGRATE_ISOLATE all
pageblocks in the range.  Once isolated, the pageblocks should not
be modified by others.

Return

zero on success or negative error code.  On success all
pages which PFN is in [start, end) are allocated for the caller and
need to be freed with free_contig_range().


====================


struct page * alloc_contig_pages(unsigned long nr_pages, gfp_t gfp_mask, int nid, nodemask_t *nodemask)

tries to find and allocate contiguous range of pages


Parameters


unsigned long nr_pages
Number of contiguous pages to allocate

gfp_t gfp_mask
GFP mask to limit search and used during compaction

int nid
Target node

nodemask_t *nodemask
Mask for other possible nodes

Description

This routine is a wrapper around alloc_contig_range(). It scans over zones
on an applicable zonelist to find a contiguous pfn range which can then be
tried for allocation with alloc_contig_range(). This routine is intended
for allocation requests which can not be fulfilled with the buddy allocator.

The allocated memory is always aligned to a page boundary. If nr_pages is a
power of two, then allocated range is also guaranteed to be aligned to same
nr_pages (e.g. 1GB request would be aligned to 1GB).

Allocated pages can be freed with free_contig_range() or by manually calling
__free_page() on each allocated page.

Return

pointer to contiguous pages on success, or NULL if not successful.


====================


int numa_map_to_online_node(int node)
Find closest online node

Parameters


int node
Node id to start the search

Description

Lookup the next closest node by distance if nid is not online.

Return

this node if it is online, otherwise the closest node by distance


====================


struct page * alloc_pages_vma(gfp_t gfp, int order, struct vm_area_struct *vma, unsigned long addr, bool hugepage)
Allocate a page for a VMA.

Parameters


gfp_t gfp
GFP flags.

int order
Order of the GFP allocation.

struct vm_area_struct *vma
Pointer to VMA or NULL if not available.

unsigned long addr
Virtual address of the allocation.  Must be inside vma.

bool hugepage
For hugepages try only the preferred node if possible.

Description

Allocate a page for a specific address in vma, using the appropriate
NUMA policy.  When vma is not NULL the caller must hold the mmap_lock
of the mm_struct of the VMA to prevent it from going away.  Should be
used for all allocations for pages that will be mapped into user space.

Return

The page on success or NULL if allocation fails.


====================


struct page * alloc_pages(gfp_t gfp, unsigned order)
Allocate pages.

Parameters


gfp_t gfp
GFP flags.

unsigned order
Power of two of number of pages to allocate.

Description

Allocate 1 << order contiguous pages.  The physical address of the
first page is naturally aligned (eg an order-3 allocation will be aligned
to a multiple of 8 * PAGE_SIZE bytes).  The NUMA policy of the current
process is honoured when in process context.

Context

Can be called from any context, providing the appropriate GFP
flags are used.

Return

The page on success or NULL if allocation fails.


====================


int mpol_misplaced(struct page *page, struct vm_area_struct *vma, unsigned long addr)
check whether current page node is valid in policy

Parameters


struct page *page
page to be checked

struct vm_area_struct *vma
vm area where page mapped

unsigned long addr
virtual address where page mapped

Description

Lookup current policy node id for vma,addr and 'compare to' page's
node id.  Policy determination 'mimics' alloc_page_vma().
Called from fault path where we know the vma and faulting address.

Return

NUMA_NO_NODE if the page is in a node that is valid for this
policy, or a suitable node ID to allocate a replacement page from.


====================


void mpol_shared_policy_init(struct shared_policy *sp, struct mempolicy *mpol)
initialize shared policy for inode

Parameters


struct shared_policy *sp
pointer to inode shared policy

struct mempolicy *mpol
struct mempolicy to install

Description

Install non-NULL mpol in inode's shared policy rb-tree.
On entry, the current task has a reference on a non-NULL mpol.
This must be released on exit.
This is called at get_inode() calls and we can use GFP_KERNEL.


====================


int mpol_parse_str(char *str, struct mempolicy **mpol)
parse string to mempolicy, for tmpfs mpol mount option.

Parameters


char *str
string containing mempolicy to parse

struct mempolicy **mpol
pointer to struct mempolicy pointer, returned on success.

Description


Format of input:
<mode>[=<flags>][:<nodelist>]

Return

0 on success, else 1


====================


void mpol_to_str(char *buffer, int maxlen, struct mempolicy *pol)
format a mempolicy structure for printing

Parameters


char *buffer
to contain formatted mempolicy string

int maxlen
length of buffer

struct mempolicy *pol
pointer to mempolicy to be formatted

Description

Convert pol into a string.  If buffer is too short, truncate the string.
Recommend a maxlen of at least 32 for the longest mode, 'interleave', the
longest flag, 'relative', and to display at least a few node ids.



struct folio
Represents a contiguous set of bytes.

Definition

struct folio {
  unsigned long flags;
  union {
    struct list_head lru;
    struct {
      void *__filler;
      unsigned int mlock_count;
    };
  };
  struct address_space *mapping;
  pgoff_t index;
  void *private;
  atomic_t _mapcount;
  atomic_t _refcount;
#ifdef CONFIG_MEMCG;
  unsigned long memcg_data;
#endif;
};


Members


flags
Identical to the page flags.

{unnamed_union}
anonymous

lru
Least Recently Used list; tracks how recently this folio was used.

{unnamed_struct}
anonymous

mapping
The file this page belongs to, or refers to the anon_vma for
anonymous memory.

index
Offset within the file, in units of pages.  For anonymous memory,
this is the index from the beginning of the mmap.

private
Filesystem per-folio data (see folio_attach_private()).
Used for swp_entry_t if folio_test_swapcache().

_mapcount
Do not access this member directly.  Use folio_mapcount() to
find out how many times this folio is mapped by userspace.

_refcount
Do not access this member directly.  Use folio_ref_count()
to find how many references there are to this folio.

memcg_data
Memory Control Group data.

Description

A folio is a physically, virtually and logically contiguous set
of bytes.  It is a power-of-two in size, and it is aligned to that
same power-of-two.  It is at least as large as PAGE_SIZE.  If it is
in the page cache, it is at a file offset which is a multiple of that
power-of-two.  It may be mapped into userspace at an address which is
at an arbitrary page offset, but its kernel virtual address is aligned
to its size.



typedef vm_fault_t
Return type for page fault handlers.

Description

Page fault handlers return a bitmask of VM_FAULT values.



enum vm_fault_reason
Page fault handlers return a bitmask of these values to tell the core VM what happened when handling the fault. Used to decide whether a process gets delivered SIGBUS or just gets major/minor fault counters bumped up.

Constants


VM_FAULT_OOM
Out Of Memory

VM_FAULT_SIGBUS
Bad access

VM_FAULT_MAJOR
Page read from storage

VM_FAULT_WRITE
Special case for get_user_pages

VM_FAULT_HWPOISON
Hit poisoned small page

VM_FAULT_HWPOISON_LARGE
Hit poisoned large page. Index encoded
in upper bits

VM_FAULT_SIGSEGV
segmentation fault

VM_FAULT_NOPAGE
->fault installed the pte, not return page

VM_FAULT_LOCKED
->fault locked the returned page

VM_FAULT_RETRY
->fault blocked, must retry

VM_FAULT_FALLBACK
huge page fault failed, fall back to small

VM_FAULT_DONE_COW
->fault has fully handled COW

VM_FAULT_NEEDDSYNC
->fault did not modify page tables and needs
fsync() to complete (for synchronous page faults
in DAX)

VM_FAULT_HINDEX_MASK
mask HINDEX value



enum fault_flag
Fault flag definitions.

Constants


FAULT_FLAG_WRITE
Fault was a write fault.

FAULT_FLAG_MKWRITE
Fault was mkwrite of existing PTE.

FAULT_FLAG_ALLOW_RETRY
Allow to retry the fault if blocked.

FAULT_FLAG_RETRY_NOWAIT
Don't drop mmap_lock and wait when retrying.

FAULT_FLAG_KILLABLE
The fault task is in SIGKILL killable region.

FAULT_FLAG_TRIED
The fault has been tried once.

FAULT_FLAG_USER
The fault originated in userspace.

FAULT_FLAG_REMOTE
The fault is not for current task/mm.

FAULT_FLAG_INSTRUCTION
The fault was during an instruction fetch.

FAULT_FLAG_INTERRUPTIBLE
The fault can be interrupted by non-fatal signals.

Description

About FAULT_FLAG_ALLOW_RETRY and FAULT_FLAG_TRIED: we can specify
whether we would allow page faults to retry by specifying these two
fault flags correctly.  Currently there can be three legal combinations:



ALLOW_RETRY and !TRIED:  this means the page fault allows retry, andthis is the first try




ALLOW_RETRY and TRIED:   this means the page fault allows retry, andwe've already tried at least once



!ALLOW_RETRY and !TRIED: this means the page fault does not allow retry

The unlisted combination (!ALLOW_RETRY && TRIED) is illegal and should never
be used.  Note that page faults can be allowed to retry for multiple times,
in which case we'll have an initial fault with flags (a) then later on
continuous faults with flags (b).  We should always try to detect pending
signals before a retry to make sure the continuous page faults can still be
interrupted if necessary.


====================


int folio_is_file_lru(struct folio *folio)
Should the folio be on a file LRU or anon LRU?

Parameters


struct folio *folio
The folio to test.

Description

We would like to get this info without a page flag, but the state
needs to survive until the folio is last deleted from the LRU, which
could be as far down as __page_cache_release.

Return

An integer (not a boolean!) used to sort a folio onto the
right LRU list and to account folios correctly.
1 if folio is a regular filesystem backed page cache folio
or a lazily freed anonymous folio (e.g. via MADV_FREE).
0 if folio is a normal anonymous folio, a tmpfs folio or otherwise
ram or swap backed folio.


====================


void __folio_clear_lru_flags(struct folio *folio)
Clear page lru flags before releasing a page.

Parameters


struct folio *folio
The folio that was on lru and now has a zero reference.


====================


enum lru_list folio_lru_list(struct folio *folio)
Which LRU list should a folio be on?

Parameters


struct folio *folio
The folio to test.

Return

The LRU list a folio should be on, as an index
into the array of LRU lists.


====================


page_folio(p)
Converts from page to folio.

Parameters


p
The page.

Description

Every page is part of a folio.  This function cannot be called on a
NULL pointer.

Context

No reference, nor lock is required on page.  If the caller
does not hold a reference, this call may race with a folio split, so
it should re-check the folio still contains this page after gaining
a reference on the folio.

Return

The folio which contains this page.


====================


folio_page(folio, n)
Return a page from a folio.

Parameters


folio
The folio.

n
The page number to return.

Description

n is relative to the start of the folio.  This function does not
check that the page number lies within folio; the caller is presumed
to have a reference to the page.


====================


bool folio_test_uptodate(struct folio *folio)
Is this folio up to date?

Parameters


struct folio *folio
The folio.

Description

The uptodate flag is set on a folio when every byte in the folio is
at least as new as the corresponding bytes on storage.  Anonymous
and CoW folios are always uptodate.  If the folio is not uptodate,
some of the bytes in it may be; see the is_partially_uptodate()
address_space operation.


====================


bool folio_test_large(struct folio *folio)
Does this folio contain more than one page?

Parameters


struct folio *folio
The folio to test.

Return

True if the folio is larger than one page.


====================


int page_has_private(struct page *page)
Determine if page has private stuff

Parameters


struct page *page
The page to be checked

Description

Determine if a page has private stuff, indicating that release routines
should be invoked upon it.


====================


bool fault_flag_allow_retry_first(enum fault_flag flags)
check ALLOW_RETRY the first time

Parameters


enum fault_flag flags
Fault flags.

Description

This is mostly used for places where we want to try to avoid taking
the mmap_lock for too long a time when waiting for another condition
to change, in which case we can try to be polite to release the
mmap_lock in the first round to avoid potential starvation of other
processes that would also want the mmap_lock.

Return

true if the page fault allows retry and this is the first
attempt of the fault handling; false otherwise.


====================


unsigned int folio_order(struct folio *folio)
The allocation order of a folio.

Parameters


struct folio *folio
The folio.

Description

A folio is composed of 2^order pages.  See get_order() for the definition
of order.

Return

The order of the folio.


====================


unsigned int thp_order(struct page *page)
Order of a transparent huge page.

Parameters


struct page *page
Head page of a transparent huge page.


====================


int thp_nr_pages(struct page *page)
The number of regular pages in this huge page.

Parameters


struct page *page
The head page of a huge page.


====================


unsigned long thp_size(struct page *page)
Size of a transparent huge page.

Parameters


struct page *page
Head page of a transparent huge page.

Return

Number of bytes in this page.


====================


void folio_get(struct folio *folio)
Increment the reference count on a folio.

Parameters


struct folio *folio
The folio.

Context

May be called in any context, as long as you know that
you have a refcount on the folio.  If you do not already have one,
folio_try_get() may be the right interface for you to use.


====================


void folio_put(struct folio *folio)
Decrement the reference count on a folio.

Parameters


struct folio *folio
The folio.

Description

If the folio's reference count reaches zero, the memory will be
released back to the page allocator and may be used by another
allocation immediately.  Do not access the memory or the struct folio
after calling folio_put() unless you can be sure that it wasn't the
last reference.

Context

May be called in process or interrupt context, but not in NMI
context.  May be called while holding a spinlock.


====================


void folio_put_refs(struct folio *folio, int refs)
Reduce the reference count on a folio.

Parameters


struct folio *folio
The folio.

int refs
The amount to subtract from the folio's reference count.

Description

If the folio's reference count reaches zero, the memory will be
released back to the page allocator and may be used by another
allocation immediately.  Do not access the memory or the struct folio
after calling folio_put_refs() unless you can be sure that these weren't
the last references.

Context

May be called in process or interrupt context, but not in NMI
context.  May be called while holding a spinlock.


====================


unsigned long folio_pfn(struct folio *folio)
Return the Page Frame Number of a folio.

Parameters


struct folio *folio
The folio.

Description

A folio may contain multiple pages.  The pages have consecutive
Page Frame Numbers.

Return

The Page Frame Number of the first page in the folio.


====================


bool folio_maybe_dma_pinned(struct folio *folio)
Report if a folio may be pinned for DMA.

Parameters


struct folio *folio
The folio.

Description

This function checks if a folio has been pinned via a call to
a function in the pin_user_pages() family.

For small folios, the return value is partially fuzzy: false is not fuzzy,
because it means 'definitely not pinned for DMA', but true means 'probably
pinned for DMA, but possibly a false positive due to having at least
GUP_PIN_COUNTING_BIAS worth of normal folio references'.

False positives are OK, because: a) it's unlikely for a folio to
get that many refcounts, and b) all the callers of this routine are
expected to be able to deal gracefully with a false positive.

For large folios, the result will be exactly correct. That's because
we have more tracking data available: the compound_pincount is used
instead of the GUP_PIN_COUNTING_BIAS scheme.

For more information, please see pin_user_pages() and related calls.

Return

True, if it is likely that the page has been 'dma-pinned'.
False, if the page is definitely not dma-pinned.


====================


long folio_nr_pages(struct folio *folio)
The number of pages in the folio.

Parameters


struct folio *folio
The folio.

Return

A positive power of two.


====================


struct folio * folio_next(struct folio *folio)
Move to the next physical folio.

Parameters


struct folio *folio
The folio we're currently operating on.

Description

If you have physically contiguous memory which may span more than
one folio (eg a struct bio_vec), use this function to move from one
folio to the next.  Do not use it if the memory is only virtually
contiguous as the folios are almost certainly not adjacent to each
other.  This is the folio equivalent to writing page++.

Context

We assume that the folios are refcounted and/or locked at a
higher level and do not adjust the reference counts.

Return

The next struct folio.


====================


unsigned int folio_shift(struct folio *folio)
The size of the memory described by this folio.

Parameters


struct folio *folio
The folio.

Description

A folio represents a number of bytes which is a power-of-two in size.
This function tells you which power-of-two the folio is.  See also
folio_size() and folio_order().

Context

The caller should have a reference on the folio to prevent
it from being split.  It is not necessary for the folio to be locked.

Return

The base-2 logarithm of the size of this folio.


====================


size_t folio_size(struct folio *folio)
The number of bytes in a folio.

Parameters


struct folio *folio
The folio.

Context

The caller should have a reference on the folio to prevent
it from being split.  It is not necessary for the folio to be locked.

Return

The number of bytes in this folio.


====================


struct vm_area_struct * find_vma_intersection(struct mm_struct *mm, unsigned long start_addr, unsigned long end_addr)
Look up the first VMA which intersects the interval

Parameters


struct mm_struct *mm
The process address space.

unsigned long start_addr
The inclusive start user address.

unsigned long end_addr
The exclusive end user address.

Return

The first VMA within the provided range, NULL otherwise.  Assumes
start_addr < end_addr.


====================


struct vm_area_struct * vma_lookup(struct mm_struct *mm, unsigned long addr)
Find a VMA at a specific address

Parameters


struct mm_struct *mm
The process address space.

unsigned long addr
The user address.

Return

The vm_area_struct at the given address, NULL otherwise.


====================


bool vma_is_special_huge(const struct vm_area_struct *vma)
Are transhuge page-table entries considered special?

Parameters


const struct vm_area_struct *vma
Pointer to the struct vm_area_struct to consider

Description

Whether transhuge page-table entries are considered 'special' following
the definition in vm_normal_page().

Return

true if transhuge page-table entries should be considered special,
false otherwise.


====================


int seal_check_future_write(int seals, struct vm_area_struct *vma)
Check for F_SEAL_FUTURE_WRITE flag and handle it

Parameters


int seals
the seals to check

struct vm_area_struct *vma
the vma to operate on

Description

Check whether F_SEAL_FUTURE_WRITE is set; if so, do proper check/handling on
the vma flags.  Return 0 if check pass, or <0 for errors.


====================


int folio_ref_count(const struct folio *folio)
The reference count on this folio.

Parameters


const struct folio *folio
The folio.

Description

The refcount is usually incremented by calls to folio_get() and
decremented by calls to folio_put().  Some typical users of the
folio refcount:


Each reference from a page table
The page cache
Filesystem private data
The LRU list
Pipes
Direct IO which references this page in the process address space
Return

The number of references to this folio.


====================


bool folio_try_get(struct folio *folio)
Attempt to increase the refcount on a folio.

Parameters


struct folio *folio
The folio.

Description

If you do not already have a reference to a folio, you can attempt to
get one using this function.  It may fail if, for example, the folio
has been freed since you found a pointer to it, or it is frozen for
the purposes of splitting or migration.

Return

True if the reference count was successfully incremented.


====================


bool folio_try_get_rcu(struct folio *folio)
Attempt to increase the refcount on a folio.

Parameters


struct folio *folio
The folio.

Description

This is a version of folio_try_get() optimised for non-SMP kernels.
If you are still holding the rcu_read_lock() after looking up the
page and know that the page cannot have its refcount decreased to
zero in interrupt context, you can use this instead of folio_try_get().

Example users include get_user_pages_fast() (as pages are not unmapped
from interrupt context) and the page cache lookups (as pages are not
truncated from interrupt context).  We also know that pages are not
frozen in interrupt context for the purposes of splitting or migration.

You can also use this function if you're holding a lock that prevents
pages being frozen & removed; eg the i_pages lock for the page cache
or the mmap_sem or page table lock for page tables.  In this case,
it will always succeed, and you could have used a plain folio_get(),
but it's sometimes more convenient to have a common function called
from both locked and RCU-protected contexts.

Return

True if the reference count was successfully incremented.


====================


int is_highmem(struct zone *zone)
helper function to quickly check if a struct zone is a highmem zone or not. This is an attempt to keep references to ZONE_{DMA/NORMAL/HIGHMEM/etc} in general code to a minimum.

Parameters


struct zone *zone
pointer to struct zone variable

Return

1 for a highmem zone, 0 otherwise


====================


for_each_online_pgdat(pgdat)
helper macro to iterate over all online nodes

Parameters


pgdat
pointer to a pg_data_t variable


====================


for_each_zone(zone)
helper macro to iterate over all memory zones

Parameters


zone
pointer to struct zone variable

Description

The user only needs to declare the zone variable, for_each_zone
fills it in.


====================


struct zoneref * next_zones_zonelist(struct zoneref *z, enum zone_type highest_zoneidx, nodemask_t *nodes)
Returns the next zone at or below highest_zoneidx within the allowed nodemask using a cursor within a zonelist as a starting point

Parameters


struct zoneref *z
The cursor used as a starting point for the search

enum zone_type highest_zoneidx
The zone index of the highest zone to return

nodemask_t *nodes
An optional nodemask to filter the zonelist with

Description

This function returns the next zone at or below a given zone index that is
within the allowed nodemask using a cursor as the starting point for the
search. The zoneref returned is a cursor that represents the current zone
being examined. It should be advanced by one before calling
next_zones_zonelist again.

Return

the next zone at or below highest_zoneidx within the allowed
nodemask using a cursor within a zonelist as a starting point


====================


struct zoneref * first_zones_zonelist(struct zonelist *zonelist, enum zone_type highest_zoneidx, nodemask_t *nodes)
Returns the first zone at or below highest_zoneidx within the allowed nodemask in a zonelist

Parameters


struct zonelist *zonelist
The zonelist to search for a suitable zone

enum zone_type highest_zoneidx
The zone index of the highest zone to return

nodemask_t *nodes
An optional nodemask to filter the zonelist with

Description

This function returns the first zone at or below a given zone index that is
within the allowed nodemask. The zoneref returned is a cursor that can be
used to iterate the zonelist with next_zones_zonelist by advancing it by
one before calling.

When no eligible zone is found, zoneref->zone is NULL (zoneref itself is
never NULL). This may happen either genuinely, or due to concurrent nodemask
update due to cpuset modification.

Return

Zoneref pointer for the first suitable zone found


====================


for_each_zone_zonelist_nodemask(zone, z, zlist, highidx, nodemask)
helper macro to iterate over valid zones in a zonelist at or below a given zone index and within a nodemask

Parameters


zone
The current zone in the iterator

z
The current pointer within zonelist->_zonerefs being iterated

zlist
The zonelist being iterated

highidx
The zone index of the highest zone to return

nodemask
Nodemask allowed by the allocator

Description

This iterator iterates though all zones at or below a given zone index and
within a given nodemask


====================


for_each_zone_zonelist(zone, z, zlist, highidx)
helper macro to iterate over valid zones in a zonelist at or below a given zone index

Parameters


zone
The current zone in the iterator

z
The current pointer within zonelist->zones being iterated

zlist
The zonelist being iterated

highidx
The zone index of the highest zone to return

Description

This iterator iterates though all zones at or below a given zone index.


====================


int pfn_valid(unsigned long pfn)
check if there is a valid memory map entry for a PFN

Parameters


unsigned long pfn
the page frame number to check

Description

Check if there is a valid memory map entry aka struct page for the pfn.
Note, that availability of the memory map entry does not imply that
there is actual usable memory at that pfn. The struct page may
represent a hole or an unusable page frame.

Return

1 for PFNs that have memory map entries and 0 otherwise


====================


struct address_space * folio_mapping(struct folio *folio)
Find the mapping where this folio is stored.

Parameters


struct folio *folio
The folio.

Description

For folios which are in the page cache, return the mapping that this
page belongs to.  Folios in the swap cache return the swap mapping
this page is stored in (which is different from the mapping for the
swap file or swap device where the data is stored).

You can call this for folios which aren't in the swap cache or page
cache and it will return NULL.

